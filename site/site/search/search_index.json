{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Rivusio","text":"<p>A lightweight, type-safe data transformation framework for Python.</p>"},{"location":"#features","title":"Features","text":"<ul> <li>Type Safety: Built with Python's type hints for better code quality and IDE support</li> <li>Async-First: Native support for asynchronous processing</li> <li>Composable: Easy pipeline composition with the <code>&gt;&gt;</code> operator</li> <li>Stream Processing: Built-in support for processing data streams</li> <li>Monitoring: Comprehensive metrics collection and monitoring</li> <li>Error Handling: Robust error handling with retries and recovery</li> </ul>"},{"location":"#quick-example","title":"Quick Example","text":"<pre><code>from rivusio import AsyncBasePipe\nfrom typing import Dict, List\n\nclass FilterPipe(AsyncBasePipe[Dict, Dict]):\n    async def process(self, data: Dict) -&gt; Dict:\n        return {k: v for k, v in data.items() if v &gt; 100}\n\nclass TransformPipe(AsyncBasePipe[Dict, List]):\n    async def process(self, data: Dict) -&gt; List:\n        return list(data.values())\n\nasync def main():\n    # Create pipeline using &gt;&gt; operator\n    pipeline = FilterPipe() &gt;&gt; TransformPipe()\n\n    # Process data\n    data = {\"a\": 150, \"b\": 50, \"c\": 200}\n    result = await pipeline(data)  # [150, 200]\n    print(result)\n\nif __name__ == \"__main__\":\n    import asyncio\n    asyncio.run(main())\n</code></pre>"},{"location":"#installation","title":"Installation","text":"<pre><code>pip install rivusio\n</code></pre>"},{"location":"#why-rivusio","title":"Why Rivusio?","text":"<ul> <li>Simple but Powerful: Clean, intuitive API that doesn't get in your way</li> <li>Type-Safe: Catch errors before they happen with static type checking</li> <li>Production Ready: Built for reliability with monitoring and error handling</li> <li>Extensible: Easy to integrate with your existing tools and workflows</li> <li>Well Documented: Comprehensive documentation and examples</li> </ul>"},{"location":"#next-steps","title":"Next Steps","text":"<ul> <li>Check out the Quick Start guide</li> <li>Learn about Basic Concepts</li> <li>Browse the Examples</li> <li>Read the API Reference</li> </ul>"},{"location":"changelog/","title":"Changelog","text":"<p>All notable changes to this project will be documented in this file.</p> <p>The format is based on Keep a Changelog, and this project adheres to Semantic Versioning.</p>"},{"location":"changelog/#unreleased","title":"Unreleased","text":""},{"location":"changelog/#added","title":"Added","text":"<ul> <li>Support for Pydantic v2</li> <li>New stream processing capabilities</li> <li>Improved error handling and monitoring</li> </ul>"},{"location":"changelog/#changed","title":"Changed","text":"<ul> <li>Updated configuration classes to use ConfigDict</li> <li>Refactored test classes into fixtures</li> <li>Simplified Makefile by removing hardcoded paths</li> </ul>"},{"location":"changelog/#fixed","title":"Fixed","text":"<ul> <li>Documentation warnings and missing references</li> <li>Pytest collection warnings</li> </ul>"},{"location":"changelog/#010-2023-12-01","title":"0.1.0 - 2023-12-01","text":""},{"location":"changelog/#added_1","title":"Added","text":"<ul> <li>Initial release of Rivusio</li> <li>Core pipeline functionality</li> <li>Basic pipe types (Filter, Transform, etc.)</li> <li>Integration with Kafka and SQS</li> <li>Basic documentation and examples</li> </ul>"},{"location":"contributing/","title":"Contributing to Rivusio","text":"<p>We love your input! We want to make contributing to Rivusio as easy and transparent as possible, whether it's:</p> <ul> <li>Reporting a bug</li> <li>Discussing the current state of the code</li> <li>Submitting a fix</li> <li>Proposing new features</li> <li>Becoming a maintainer</li> </ul>"},{"location":"contributing/#development-process","title":"Development Process","text":"<p>We use GitHub to host code, to track issues and feature requests, as well as accept pull requests.</p> <ol> <li>Fork the repo and create your branch from <code>main</code></li> <li>If you've added code that should be tested, add tests</li> <li>If you've changed APIs, update the documentation</li> <li>Ensure the test suite passes</li> <li>Make sure your code lints</li> <li>Issue that pull request!</li> </ol>"},{"location":"contributing/#local-development","title":"Local Development","text":"<p>Set up your development environment:</p> <pre><code># Clone the repository\ngit clone https://github.com/zbytealchemy/rivusio.git\ncd rivusio\n\n# Install dependencies\npoetry install\n\n# Install pre-commit hooks\npre-commit install\n</code></pre>"},{"location":"contributing/#running-tests","title":"Running Tests","text":"<pre><code># Run all tests\nmake test\n\n# Run tests with coverage\nmake coverage\n</code></pre>"},{"location":"contributing/#code-style","title":"Code Style","text":"<p>We use <code>black</code> for Python code formatting and <code>isort</code> for import sorting. Both tools are configured in <code>pyproject.toml</code>.</p> <pre><code># Format code\nmake format\n\n# Check code style\nmake lint\n</code></pre>"},{"location":"contributing/#documentation","title":"Documentation","text":"<p>We use MkDocs with the Material theme for documentation:</p> <pre><code># Install documentation dependencies\npoetry install --with docs\n\n# Build documentation\nmake docs\n\n# Serve documentation locally\nmake docs-serve\n</code></pre>"},{"location":"contributing/#pull-request-process","title":"Pull Request Process","text":"<ol> <li>Update the README.md with details of changes to the interface</li> <li>Update the docs with any new API changes</li> <li>The PR may be merged once you have the sign-off of two other developers</li> <li>Make sure all tests pass and code style checks are satisfied</li> </ol>"},{"location":"contributing/#any-contributions-you-make-will-be-under-the-mit-software-license","title":"Any contributions you make will be under the MIT Software License","text":"<p>In short, when you submit code changes, your submissions are understood to be under the same MIT License that covers the project. Feel free to contact the maintainers if that's a concern.</p>"},{"location":"contributing/#report-bugs-using-githubs-issue-tracker","title":"Report bugs using GitHub's issue tracker","text":"<p>We use GitHub issues to track public bugs. Report a bug by opening a new issue.</p>"},{"location":"contributing/#write-bug-reports-with-detail-background-and-sample-code","title":"Write bug reports with detail, background, and sample code","text":"<p>Great Bug Reports tend to have:</p> <ul> <li>A quick summary and/or background</li> <li>Steps to reproduce</li> <li>Be specific!</li> <li>Give sample code if you can</li> <li>What you expected would happen</li> <li>What actually happens</li> <li>Notes (possibly including why you think this might be happening, or stuff you tried that didn't work)</li> </ul>"},{"location":"contributing/#license","title":"License","text":"<p>By contributing, you agree that your contributions will be licensed under its MIT License.</p>"},{"location":"api/core/","title":"Core API Reference","text":""},{"location":"api/core/#basepipe","title":"BasePipe","text":""},{"location":"api/core/#rivusio.core.pipe.BasePipe","title":"rivusio.core.pipe.BasePipe","text":"<p>             Bases: <code>ABC</code>, <code>Generic[InputType, OutputType]</code></p> <p>Base class for all pipes.</p> Source code in <code>src/rivusio/core/pipe.py</code> <pre><code>class BasePipe(ABC, Generic[InputType, OutputType]):\n    \"\"\"Base class for all pipes.\"\"\"\n\n    @abstractmethod\n    def __init__(self) -&gt; None:\n        \"\"\"Initialize pipe.\"\"\"\n        self.name: str = self.__class__.__name__\n</code></pre>"},{"location":"api/core/#rivusio.core.pipe.BasePipe.name","title":"name  <code>instance-attribute</code>","text":"<pre><code>name: str = __name__\n</code></pre>"},{"location":"api/core/#rivusio.core.pipe.BasePipe.__init__","title":"__init__  <code>abstractmethod</code>","text":"<pre><code>__init__() -&gt; None\n</code></pre> <p>Initialize pipe.</p> Source code in <code>src/rivusio/core/pipe.py</code> <pre><code>@abstractmethod\ndef __init__(self) -&gt; None:\n    \"\"\"Initialize pipe.\"\"\"\n    self.name: str = self.__class__.__name__\n</code></pre>"},{"location":"api/core/#asyncbasepipe","title":"AsyncBasePipe","text":""},{"location":"api/core/#rivusio.core.pipe.AsyncBasePipe","title":"rivusio.core.pipe.AsyncBasePipe","text":"<p>             Bases: <code>BasePipe[InputType, OutputType]</code></p> <p>Base class for async pipes.</p> Source code in <code>src/rivusio/core/pipe.py</code> <pre><code>class AsyncBasePipe(BasePipe[InputType, OutputType]):\n    \"\"\"Base class for async pipes.\"\"\"\n\n    def __init__(self) -&gt; None:\n        \"\"\"Initialize pipe.\"\"\"\n        super().__init__()\n\n    @abstractmethod\n    async def process(self, data: InputType) -&gt; OutputType:\n        \"\"\"Process data through the pipe.\n\n        Args:\n        ----\n            data: Input data\n\n        Returns:\n        -------\n            Processed data\n\n        \"\"\"\n\n    async def __call__(self, data: InputType) -&gt; OutputType:\n        \"\"\"Process data when pipe is called as a function.\"\"\"\n        return await self.process(data)\n</code></pre>"},{"location":"api/core/#rivusio.core.pipe.AsyncBasePipe.__call__","title":"__call__  <code>async</code>","text":"<pre><code>__call__(data: InputType) -&gt; OutputType\n</code></pre> <p>Process data when pipe is called as a function.</p> Source code in <code>src/rivusio/core/pipe.py</code> <pre><code>async def __call__(self, data: InputType) -&gt; OutputType:\n    \"\"\"Process data when pipe is called as a function.\"\"\"\n    return await self.process(data)\n</code></pre>"},{"location":"api/core/#rivusio.core.pipe.AsyncBasePipe.__init__","title":"__init__","text":"<pre><code>__init__() -&gt; None\n</code></pre> <p>Initialize pipe.</p> Source code in <code>src/rivusio/core/pipe.py</code> <pre><code>def __init__(self) -&gt; None:\n    \"\"\"Initialize pipe.\"\"\"\n    super().__init__()\n</code></pre>"},{"location":"api/core/#rivusio.core.pipe.AsyncBasePipe.process","title":"process  <code>abstractmethod</code> <code>async</code>","text":"<pre><code>process(data: InputType) -&gt; OutputType\n</code></pre> <p>Process data through the pipe.</p> <pre><code>data: Input data\n</code></pre> <pre><code>Processed data\n</code></pre> Source code in <code>src/rivusio/core/pipe.py</code> <pre><code>@abstractmethod\nasync def process(self, data: InputType) -&gt; OutputType:\n    \"\"\"Process data through the pipe.\n\n    Args:\n    ----\n        data: Input data\n\n    Returns:\n    -------\n        Processed data\n\n    \"\"\"\n</code></pre>"},{"location":"api/core/#syncbasepipe","title":"SyncBasePipe","text":""},{"location":"api/core/#rivusio.core.pipe.SyncBasePipe","title":"rivusio.core.pipe.SyncBasePipe","text":"<p>             Bases: <code>BasePipe[InputType, OutputType]</code></p> <p>Base class for sync pipes.</p> Source code in <code>src/rivusio/core/pipe.py</code> <pre><code>class SyncBasePipe(BasePipe[InputType, OutputType]):\n    \"\"\"Base class for sync pipes.\"\"\"\n\n    def __init__(self) -&gt; None:\n        \"\"\"Initialize pipe.\"\"\"\n        super().__init__()\n\n    @abstractmethod\n    def process(self, data: InputType) -&gt; OutputType:\n        \"\"\"Process data through the pipe.\n\n        Args:\n        ----\n            data: Input data\n\n        Returns:\n        -------\n            Processed data\n\n        \"\"\"\n\n    def __call__(self, data: InputType) -&gt; OutputType:\n        \"\"\"Process data when pipe is called as a function.\"\"\"\n        return self.process(data)\n</code></pre>"},{"location":"api/core/#rivusio.core.pipe.SyncBasePipe.__call__","title":"__call__","text":"<pre><code>__call__(data: InputType) -&gt; OutputType\n</code></pre> <p>Process data when pipe is called as a function.</p> Source code in <code>src/rivusio/core/pipe.py</code> <pre><code>def __call__(self, data: InputType) -&gt; OutputType:\n    \"\"\"Process data when pipe is called as a function.\"\"\"\n    return self.process(data)\n</code></pre>"},{"location":"api/core/#rivusio.core.pipe.SyncBasePipe.__init__","title":"__init__","text":"<pre><code>__init__() -&gt; None\n</code></pre> <p>Initialize pipe.</p> Source code in <code>src/rivusio/core/pipe.py</code> <pre><code>def __init__(self) -&gt; None:\n    \"\"\"Initialize pipe.\"\"\"\n    super().__init__()\n</code></pre>"},{"location":"api/core/#rivusio.core.pipe.SyncBasePipe.process","title":"process  <code>abstractmethod</code>","text":"<pre><code>process(data: InputType) -&gt; OutputType\n</code></pre> <p>Process data through the pipe.</p> <pre><code>data: Input data\n</code></pre> <pre><code>Processed data\n</code></pre> Source code in <code>src/rivusio/core/pipe.py</code> <pre><code>@abstractmethod\ndef process(self, data: InputType) -&gt; OutputType:\n    \"\"\"Process data through the pipe.\n\n    Args:\n    ----\n        data: Input data\n\n    Returns:\n    -------\n        Processed data\n\n    \"\"\"\n</code></pre>"},{"location":"api/core/#pipeconfig","title":"PipeConfig","text":""},{"location":"api/core/#rivusio.core.config.PipeConfig","title":"rivusio.core.config.PipeConfig","text":"<p>             Bases: <code>BaseModel</code></p> <p>Base configuration model for all pipes.</p> <p>All pipe configurations should inherit from this class to ensure consistent configuration handling across the pipeline system.</p> <pre><code>name: Optional name for the pipe instance. Defaults to \"default\"\ndescription: Optional description of the pipe's purpose. Defaults to empty string\n</code></pre>"},{"location":"api/core/#rivusio.core.config.PipeConfig--example","title":"Example:","text":"<pre><code>```python\nclass ProcessorConfig(PipeConfig):\n    retries: int = 3\n    retry_delay: float = 1.0\n    timeout: float = 5.0\n\nconfig = ProcessorConfig(\n    name=\"data_processor\",\n    description=\"Processes incoming data with retries\"\n)\npipe = AsyncPipe(processor_func, config=config)\n```\n</code></pre> Source code in <code>src/rivusio/core/config.py</code> <pre><code>class PipeConfig(BaseModel):\n    \"\"\"Base configuration model for all pipes.\n\n    All pipe configurations should inherit from this class to ensure\n    consistent configuration handling across the pipeline system.\n\n    Attributes:\n    ----------\n        name: Optional name for the pipe instance. Defaults to \"default\"\n        description: Optional description of the pipe's purpose. Defaults to empty string\n\n    Example:\n    -------\n        ```python\n        class ProcessorConfig(PipeConfig):\n            retries: int = 3\n            retry_delay: float = 1.0\n            timeout: float = 5.0\n\n        config = ProcessorConfig(\n            name=\"data_processor\",\n            description=\"Processes incoming data with retries\"\n        )\n        pipe = AsyncPipe(processor_func, config=config)\n        ```\n\n    \"\"\"\n\n    name: str = \"default\"\n    description: str = \"\"\n\n    model_config = {\n        \"extra\": \"forbid\",  # Prevent extra attributes\n        \"validate_assignment\": True,  # Validate during assignment\n        \"arbitrary_types_allowed\": True,  # Allow arbitrary types\n    }\n</code></pre>"},{"location":"api/core/#rivusio.core.config.PipeConfig.description","title":"description  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>description: str = ''\n</code></pre>"},{"location":"api/core/#rivusio.core.config.PipeConfig.model_config","title":"model_config  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>model_config = {'extra': 'forbid', 'validate_assignment': True, 'arbitrary_types_allowed': True}\n</code></pre>"},{"location":"api/core/#rivusio.core.config.PipeConfig.name","title":"name  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>name: str = 'default'\n</code></pre>"},{"location":"api/core/#pipeline-types","title":"Pipeline Types","text":""},{"location":"api/core/#syncpipeline","title":"SyncPipeline","text":""},{"location":"api/core/#rivusio.core.pipeline.SyncPipeline","title":"rivusio.core.pipeline.SyncPipeline","text":"<p>             Bases: <code>SyncBasePipe[InputType, OutputType]</code>, <code>PipelineMetricsMixin</code></p> <p>Pipeline for composing sync pipes.</p> Source code in <code>src/rivusio/core/pipeline.py</code> <pre><code>class SyncPipeline(SyncBasePipe[InputType, OutputType], PipelineMetricsMixin):\n    \"\"\"Pipeline for composing sync pipes.\"\"\"\n\n    def __init__(self, pipes: list[SyncBasePipe[Any, Any]], name: Optional[str] = None) -&gt; None:\n        \"\"\"Initialize pipeline.\"\"\"\n        super().__init__()\n        if not pipes:\n            raise ValueError(\"Pipeline must contain at least one pipe\")\n        self._pipes = pipes\n        self.name = name or \"SyncPipeline\"\n        self._pipe_outputs: dict[SyncBasePipe, list[Any]] = {pipe: [] for pipe in self._pipes}\n        self._parallel_config = ParallelConfig()\n        self._parallel_executor: Optional[ParallelExecutor] = None\n\n    async def __aenter__(self) -&gt; \"SyncPipeline[InputType, OutputType]\":\n        \"\"\"Initialize resources.\"\"\"\n        if not self._parallel_executor and self._parallel_config:\n            self._parallel_executor = await ParallelExecutor(self._parallel_config).__aenter__()\n        return self\n\n    async def __aexit__(\n        self,\n        exc_type: type[BaseException] | None,\n        exc_val: BaseException | None,\n        exc_tb: object,\n    ) -&gt; None:\n        \"\"\"Clean up resources.\"\"\"\n        if self._parallel_executor:\n            await self._parallel_executor.__aexit__(exc_type, exc_val, exc_tb)\n\n    def __rshift__(\n        self,\n        other: SyncBasePipe[InputType, OutputType],\n    ) -&gt; \"SyncPipeline[InputType, OutputType]\":\n        \"\"\"Compose this pipeline with another pipe using the &gt;&gt; operator.\"\"\"\n        return SyncPipeline([*self._pipes, other])\n\n    @property\n    def pipes(self) -&gt; list[SyncBasePipe[Any, Any]]:\n        \"\"\"Get pipeline's pipes.\"\"\"\n        return self._pipes\n\n    def process(self, data: InputType) -&gt; OutputType:\n        \"\"\"Process data through all pipes in the pipeline.\"\"\"\n        result = data\n        for pipe in self._pipes:\n            try:\n                result = pipe.process(result)\n                self._pipe_outputs[pipe].append(result)\n            except Exception as e:\n                raise PipelineError(pipe.__class__.__name__, e) from e\n        return cast(OutputType, result)\n\n    def get_pipe_outputs(self, pipe: SyncBasePipe) -&gt; list[Any]:\n        \"\"\"Get all outputs from a specific pipe.\n\n        Args:\n        ----\n            pipe: Pipe to get outputs for\n\n        Returns:\n        -------\n            List of outputs from the pipe\n\n        \"\"\"\n        return self._pipe_outputs.get(pipe, [])\n\n    async def execute_parallel(self, data: list[Any]) -&gt; list[Any]:\n        \"\"\"Execute the pipeline on multiple inputs in parallel.\n\n        This method supports different execution strategies:\n        - GATHER: Uses asyncio.gather for I/O-bound tasks (default)\n        - THREAD_POOL: Uses ThreadPoolExecutor for I/O and light CPU tasks\n        - PROCESS_POOL: Uses ProcessPoolExecutor for CPU-intensive tasks\n\n        Args:\n        ----\n            data: List of input data\n\n        Returns:\n        -------\n            List of processed results\n\n        Raises:\n        ------\n            PipeError: If any pipe fails\n\n        \"\"\"\n        if not self._parallel_executor:\n            raise PipelineError(\n                self.name, RuntimeError(\"Must use async context manager for parallel execution\"),\n            )\n\n        return await self._parallel_executor.execute(self, data)\n\n    def configure_parallel(\n        self,\n        strategy: Union[ExecutionStrategy, str],\n        max_workers: Optional[int] = None,\n        chunk_size: int = 1000,\n    ) -&gt; None:\n        \"\"\"Configure parallel execution.\n\n        Args:\n        ----\n            strategy: Execution strategy to use\n            max_workers: Maximum number of workers (default: based on strategy)\n            chunk_size: Size of chunks for process pool (default: 1000)\n\n        \"\"\"\n        if isinstance(strategy, str):\n            strategy = ExecutionStrategy(strategy)\n        self._parallel_config = ParallelConfig(\n            strategy=strategy, max_workers=max_workers, chunk_size=chunk_size,\n        )\n        self._parallel_executor = ParallelExecutor(self._parallel_config)\n\n    def execute_conditional(\n        self, data: InputType, condition: Callable[[InputType], bool],\n    ) -&gt; Union[InputType, OutputType]:\n        \"\"\"Execute pipeline only if condition is met.\n\n        Args:\n        ----\n            data: Input data\n            condition: Function that takes input data and returns bool\n\n        Returns:\n        -------\n            Processed data if condition is met, otherwise input data\n\n        \"\"\"\n        if condition(data):\n            return self.process(data)\n        return data\n</code></pre>"},{"location":"api/core/#rivusio.core.pipeline.SyncPipeline.name","title":"name  <code>instance-attribute</code>","text":"<pre><code>name = name or 'SyncPipeline'\n</code></pre>"},{"location":"api/core/#rivusio.core.pipeline.SyncPipeline.pipes","title":"pipes  <code>property</code>","text":"<pre><code>pipes: list[SyncBasePipe[Any, Any]]\n</code></pre> <p>Get pipeline's pipes.</p>"},{"location":"api/core/#rivusio.core.pipeline.SyncPipeline.__aenter__","title":"__aenter__  <code>async</code>","text":"<pre><code>__aenter__() -&gt; SyncPipeline[InputType, OutputType]\n</code></pre> <p>Initialize resources.</p> Source code in <code>src/rivusio/core/pipeline.py</code> <pre><code>async def __aenter__(self) -&gt; \"SyncPipeline[InputType, OutputType]\":\n    \"\"\"Initialize resources.\"\"\"\n    if not self._parallel_executor and self._parallel_config:\n        self._parallel_executor = await ParallelExecutor(self._parallel_config).__aenter__()\n    return self\n</code></pre>"},{"location":"api/core/#rivusio.core.pipeline.SyncPipeline.__aexit__","title":"__aexit__  <code>async</code>","text":"<pre><code>__aexit__(exc_type: type[BaseException] | None, exc_val: BaseException | None, exc_tb: object) -&gt; None\n</code></pre> <p>Clean up resources.</p> Source code in <code>src/rivusio/core/pipeline.py</code> <pre><code>async def __aexit__(\n    self,\n    exc_type: type[BaseException] | None,\n    exc_val: BaseException | None,\n    exc_tb: object,\n) -&gt; None:\n    \"\"\"Clean up resources.\"\"\"\n    if self._parallel_executor:\n        await self._parallel_executor.__aexit__(exc_type, exc_val, exc_tb)\n</code></pre>"},{"location":"api/core/#rivusio.core.pipeline.SyncPipeline.__init__","title":"__init__","text":"<pre><code>__init__(pipes: list[SyncBasePipe[Any, Any]], name: Optional[str] = None) -&gt; None\n</code></pre> <p>Initialize pipeline.</p> Source code in <code>src/rivusio/core/pipeline.py</code> <pre><code>def __init__(self, pipes: list[SyncBasePipe[Any, Any]], name: Optional[str] = None) -&gt; None:\n    \"\"\"Initialize pipeline.\"\"\"\n    super().__init__()\n    if not pipes:\n        raise ValueError(\"Pipeline must contain at least one pipe\")\n    self._pipes = pipes\n    self.name = name or \"SyncPipeline\"\n    self._pipe_outputs: dict[SyncBasePipe, list[Any]] = {pipe: [] for pipe in self._pipes}\n    self._parallel_config = ParallelConfig()\n    self._parallel_executor: Optional[ParallelExecutor] = None\n</code></pre>"},{"location":"api/core/#rivusio.core.pipeline.SyncPipeline.__rshift__","title":"__rshift__","text":"<pre><code>__rshift__(other: SyncBasePipe[InputType, OutputType]) -&gt; SyncPipeline[InputType, OutputType]\n</code></pre> <p>Compose this pipeline with another pipe using the &gt;&gt; operator.</p> Source code in <code>src/rivusio/core/pipeline.py</code> <pre><code>def __rshift__(\n    self,\n    other: SyncBasePipe[InputType, OutputType],\n) -&gt; \"SyncPipeline[InputType, OutputType]\":\n    \"\"\"Compose this pipeline with another pipe using the &gt;&gt; operator.\"\"\"\n    return SyncPipeline([*self._pipes, other])\n</code></pre>"},{"location":"api/core/#rivusio.core.pipeline.SyncPipeline.configure_parallel","title":"configure_parallel","text":"<pre><code>configure_parallel(strategy: Union[ExecutionStrategy, str], max_workers: Optional[int] = None, chunk_size: int = 1000) -&gt; None\n</code></pre> <p>Configure parallel execution.</p> <pre><code>strategy: Execution strategy to use\nmax_workers: Maximum number of workers (default: based on strategy)\nchunk_size: Size of chunks for process pool (default: 1000)\n</code></pre> Source code in <code>src/rivusio/core/pipeline.py</code> <pre><code>def configure_parallel(\n    self,\n    strategy: Union[ExecutionStrategy, str],\n    max_workers: Optional[int] = None,\n    chunk_size: int = 1000,\n) -&gt; None:\n    \"\"\"Configure parallel execution.\n\n    Args:\n    ----\n        strategy: Execution strategy to use\n        max_workers: Maximum number of workers (default: based on strategy)\n        chunk_size: Size of chunks for process pool (default: 1000)\n\n    \"\"\"\n    if isinstance(strategy, str):\n        strategy = ExecutionStrategy(strategy)\n    self._parallel_config = ParallelConfig(\n        strategy=strategy, max_workers=max_workers, chunk_size=chunk_size,\n    )\n    self._parallel_executor = ParallelExecutor(self._parallel_config)\n</code></pre>"},{"location":"api/core/#rivusio.core.pipeline.SyncPipeline.execute_conditional","title":"execute_conditional","text":"<pre><code>execute_conditional(data: InputType, condition: Callable[[InputType], bool]) -&gt; Union[InputType, OutputType]\n</code></pre> <p>Execute pipeline only if condition is met.</p> <pre><code>data: Input data\ncondition: Function that takes input data and returns bool\n</code></pre> <pre><code>Processed data if condition is met, otherwise input data\n</code></pre> Source code in <code>src/rivusio/core/pipeline.py</code> <pre><code>def execute_conditional(\n    self, data: InputType, condition: Callable[[InputType], bool],\n) -&gt; Union[InputType, OutputType]:\n    \"\"\"Execute pipeline only if condition is met.\n\n    Args:\n    ----\n        data: Input data\n        condition: Function that takes input data and returns bool\n\n    Returns:\n    -------\n        Processed data if condition is met, otherwise input data\n\n    \"\"\"\n    if condition(data):\n        return self.process(data)\n    return data\n</code></pre>"},{"location":"api/core/#rivusio.core.pipeline.SyncPipeline.execute_parallel","title":"execute_parallel  <code>async</code>","text":"<pre><code>execute_parallel(data: list[Any]) -&gt; list[Any]\n</code></pre> <p>Execute the pipeline on multiple inputs in parallel.</p> <p>This method supports different execution strategies: - GATHER: Uses asyncio.gather for I/O-bound tasks (default) - THREAD_POOL: Uses ThreadPoolExecutor for I/O and light CPU tasks - PROCESS_POOL: Uses ProcessPoolExecutor for CPU-intensive tasks</p> <pre><code>data: List of input data\n</code></pre> <pre><code>List of processed results\n</code></pre> <pre><code>PipeError: If any pipe fails\n</code></pre> Source code in <code>src/rivusio/core/pipeline.py</code> <pre><code>async def execute_parallel(self, data: list[Any]) -&gt; list[Any]:\n    \"\"\"Execute the pipeline on multiple inputs in parallel.\n\n    This method supports different execution strategies:\n    - GATHER: Uses asyncio.gather for I/O-bound tasks (default)\n    - THREAD_POOL: Uses ThreadPoolExecutor for I/O and light CPU tasks\n    - PROCESS_POOL: Uses ProcessPoolExecutor for CPU-intensive tasks\n\n    Args:\n    ----\n        data: List of input data\n\n    Returns:\n    -------\n        List of processed results\n\n    Raises:\n    ------\n        PipeError: If any pipe fails\n\n    \"\"\"\n    if not self._parallel_executor:\n        raise PipelineError(\n            self.name, RuntimeError(\"Must use async context manager for parallel execution\"),\n        )\n\n    return await self._parallel_executor.execute(self, data)\n</code></pre>"},{"location":"api/core/#rivusio.core.pipeline.SyncPipeline.get_pipe_outputs","title":"get_pipe_outputs","text":"<pre><code>get_pipe_outputs(pipe: SyncBasePipe) -&gt; list[Any]\n</code></pre> <p>Get all outputs from a specific pipe.</p> <pre><code>pipe: Pipe to get outputs for\n</code></pre> <pre><code>List of outputs from the pipe\n</code></pre> Source code in <code>src/rivusio/core/pipeline.py</code> <pre><code>def get_pipe_outputs(self, pipe: SyncBasePipe) -&gt; list[Any]:\n    \"\"\"Get all outputs from a specific pipe.\n\n    Args:\n    ----\n        pipe: Pipe to get outputs for\n\n    Returns:\n    -------\n        List of outputs from the pipe\n\n    \"\"\"\n    return self._pipe_outputs.get(pipe, [])\n</code></pre>"},{"location":"api/core/#rivusio.core.pipeline.SyncPipeline.process","title":"process","text":"<pre><code>process(data: InputType) -&gt; OutputType\n</code></pre> <p>Process data through all pipes in the pipeline.</p> Source code in <code>src/rivusio/core/pipeline.py</code> <pre><code>def process(self, data: InputType) -&gt; OutputType:\n    \"\"\"Process data through all pipes in the pipeline.\"\"\"\n    result = data\n    for pipe in self._pipes:\n        try:\n            result = pipe.process(result)\n            self._pipe_outputs[pipe].append(result)\n        except Exception as e:\n            raise PipelineError(pipe.__class__.__name__, e) from e\n    return cast(OutputType, result)\n</code></pre>"},{"location":"api/core/#asyncpipeline","title":"AsyncPipeline","text":""},{"location":"api/core/#rivusio.core.pipeline.AsyncPipeline","title":"rivusio.core.pipeline.AsyncPipeline","text":"<p>             Bases: <code>AsyncBasePipe[InputType, OutputType]</code>, <code>PipelineMetricsMixin</code></p> <p>Pipeline for composing async pipes.</p> Source code in <code>src/rivusio/core/pipeline.py</code> <pre><code>class AsyncPipeline(AsyncBasePipe[InputType, OutputType], PipelineMetricsMixin):\n    \"\"\"Pipeline for composing async pipes.\"\"\"\n\n    def __init__(self, pipes: list[AsyncBasePipe[Any, Any]], name: Optional[str] = None) -&gt; None:\n        \"\"\"Initialize pipeline.\"\"\"\n        super().__init__()\n        if not pipes:\n            raise ValueError(\"Pipeline must contain at least one pipe\")\n        self._pipes = pipes\n        self.name = name or \"AsyncPipeline\"\n        self._pipe_outputs: dict[AsyncBasePipe, list[Any]] = {pipe: [] for pipe in self._pipes}\n        self._parallel_config = ParallelConfig()\n        self._parallel_executor: Optional[ParallelExecutor] = None\n\n    async def __aenter__(self) -&gt; \"AsyncPipeline[InputType, OutputType]\":\n        \"\"\"Initialize resources.\"\"\"\n        if not self._parallel_executor and self._parallel_config:\n            self._parallel_executor = await ParallelExecutor(self._parallel_config).__aenter__()\n        return self\n\n    async def __aexit__(\n        self,\n        exc_type: Optional[type[BaseException]],\n        exc_val: Optional[BaseException],\n        exc_tb: types.TracebackType | None,\n    ) -&gt; None:\n        \"\"\"Clean up resources.\"\"\"\n        if self._parallel_executor:\n            await self._parallel_executor.__aexit__(exc_type, exc_val, exc_tb)\n\n    def __rshift__(\n        self,\n        other: AsyncBasePipe[InputType, OutputType],\n    ) -&gt; \"AsyncPipeline[InputType, OutputType]\":\n        \"\"\"Compose this pipeline with another pipe using the &gt;&gt; operator.\"\"\"\n        if isinstance(other, AsyncBasePipe):\n            return AsyncPipeline([*self._pipes, other])\n        raise TypeError(f\"Cannot compose AsyncPipeline with {type(other)}\")\n\n    @property\n    def pipes(self) -&gt; list[AsyncBasePipe[Any, Any]]:\n        \"\"\"Get pipeline's pipes.\"\"\"\n        return self._pipes\n\n    async def process(self, data: InputType) -&gt; OutputType:\n        \"\"\"Process data through all pipes in the pipeline.\"\"\"\n        result = data\n        for pipe in self._pipes:\n            try:\n                result = await pipe.process(result)\n                self._pipe_outputs[pipe].append(result)\n            except Exception as e:\n                raise PipelineError(pipe.__class__.__name__, e) from e\n        return cast(OutputType, result)\n\n    def get_pipe_outputs(self, pipe: AsyncBasePipe) -&gt; list[Any]:\n        \"\"\"Get all outputs from a specific pipe.\n\n        Args:\n        ----\n            pipe: Pipe to get outputs for\n\n        Returns:\n        -------\n            List of outputs from the pipe\n\n        \"\"\"\n        return self._pipe_outputs.get(pipe, [])\n\n    async def execute_parallel(self, data: list[Any]) -&gt; list[Any]:\n        \"\"\"Execute the pipeline on multiple inputs in parallel.\n\n        This method supports different execution strategies:\n        - GATHER: Uses asyncio.gather for I/O-bound tasks (default)\n        - THREAD_POOL: Uses ThreadPoolExecutor for I/O and light CPU tasks\n        - PROCESS_POOL: Uses ProcessPoolExecutor for CPU-intensive tasks\n\n        Args:\n        ----\n            data: List of input data\n\n        Returns:\n        -------\n            List of processed results\n\n        Raises:\n        ------\n            PipeError: If any pipe fails\n\n        \"\"\"\n        if not self._parallel_executor:\n            raise PipelineError(\n                self.name, RuntimeError(\"Must use async context manager for parallel execution\"),\n            )\n\n        return await self._parallel_executor.execute(self, data)\n\n    def configure_parallel(\n        self,\n        strategy: Union[ExecutionStrategy, str],\n        max_workers: Optional[int] = None,\n        chunk_size: int = 1000,\n    ) -&gt; None:\n        \"\"\"Configure parallel execution.\n\n        Args:\n        ----\n            strategy: Execution strategy to use\n            max_workers: Maximum number of workers (default: based on strategy)\n            chunk_size: Size of chunks for process pool (default: 1000)\n\n        \"\"\"\n        if isinstance(strategy, str):\n            strategy = ExecutionStrategy(strategy)\n        self._parallel_config = ParallelConfig(\n            strategy=strategy, max_workers=max_workers, chunk_size=chunk_size,\n        )\n        self._parallel_executor = ParallelExecutor(self._parallel_config)\n\n    async def execute_conditional(\n        self, data: InputType, condition: Callable[[InputType], bool],\n    ) -&gt; Union[InputType, OutputType]:\n        \"\"\"Execute pipeline only if condition is met.\n\n        Args:\n        ----\n            data: Input data\n            condition: Function that takes input data and returns bool\n\n        Returns:\n        -------\n            Processed data if condition is met, otherwise input data\n\n        \"\"\"\n        if condition(data):\n            return await self.process(data)\n        return data\n</code></pre>"},{"location":"api/core/#rivusio.core.pipeline.AsyncPipeline.name","title":"name  <code>instance-attribute</code>","text":"<pre><code>name = name or 'AsyncPipeline'\n</code></pre>"},{"location":"api/core/#rivusio.core.pipeline.AsyncPipeline.pipes","title":"pipes  <code>property</code>","text":"<pre><code>pipes: list[AsyncBasePipe[Any, Any]]\n</code></pre> <p>Get pipeline's pipes.</p>"},{"location":"api/core/#rivusio.core.pipeline.AsyncPipeline.__aenter__","title":"__aenter__  <code>async</code>","text":"<pre><code>__aenter__() -&gt; AsyncPipeline[InputType, OutputType]\n</code></pre> <p>Initialize resources.</p> Source code in <code>src/rivusio/core/pipeline.py</code> <pre><code>async def __aenter__(self) -&gt; \"AsyncPipeline[InputType, OutputType]\":\n    \"\"\"Initialize resources.\"\"\"\n    if not self._parallel_executor and self._parallel_config:\n        self._parallel_executor = await ParallelExecutor(self._parallel_config).__aenter__()\n    return self\n</code></pre>"},{"location":"api/core/#rivusio.core.pipeline.AsyncPipeline.__aexit__","title":"__aexit__  <code>async</code>","text":"<pre><code>__aexit__(exc_type: Optional[type[BaseException]], exc_val: Optional[BaseException], exc_tb: types.TracebackType | None) -&gt; None\n</code></pre> <p>Clean up resources.</p> Source code in <code>src/rivusio/core/pipeline.py</code> <pre><code>async def __aexit__(\n    self,\n    exc_type: Optional[type[BaseException]],\n    exc_val: Optional[BaseException],\n    exc_tb: types.TracebackType | None,\n) -&gt; None:\n    \"\"\"Clean up resources.\"\"\"\n    if self._parallel_executor:\n        await self._parallel_executor.__aexit__(exc_type, exc_val, exc_tb)\n</code></pre>"},{"location":"api/core/#rivusio.core.pipeline.AsyncPipeline.__init__","title":"__init__","text":"<pre><code>__init__(pipes: list[AsyncBasePipe[Any, Any]], name: Optional[str] = None) -&gt; None\n</code></pre> <p>Initialize pipeline.</p> Source code in <code>src/rivusio/core/pipeline.py</code> <pre><code>def __init__(self, pipes: list[AsyncBasePipe[Any, Any]], name: Optional[str] = None) -&gt; None:\n    \"\"\"Initialize pipeline.\"\"\"\n    super().__init__()\n    if not pipes:\n        raise ValueError(\"Pipeline must contain at least one pipe\")\n    self._pipes = pipes\n    self.name = name or \"AsyncPipeline\"\n    self._pipe_outputs: dict[AsyncBasePipe, list[Any]] = {pipe: [] for pipe in self._pipes}\n    self._parallel_config = ParallelConfig()\n    self._parallel_executor: Optional[ParallelExecutor] = None\n</code></pre>"},{"location":"api/core/#rivusio.core.pipeline.AsyncPipeline.__rshift__","title":"__rshift__","text":"<pre><code>__rshift__(other: AsyncBasePipe[InputType, OutputType]) -&gt; AsyncPipeline[InputType, OutputType]\n</code></pre> <p>Compose this pipeline with another pipe using the &gt;&gt; operator.</p> Source code in <code>src/rivusio/core/pipeline.py</code> <pre><code>def __rshift__(\n    self,\n    other: AsyncBasePipe[InputType, OutputType],\n) -&gt; \"AsyncPipeline[InputType, OutputType]\":\n    \"\"\"Compose this pipeline with another pipe using the &gt;&gt; operator.\"\"\"\n    if isinstance(other, AsyncBasePipe):\n        return AsyncPipeline([*self._pipes, other])\n    raise TypeError(f\"Cannot compose AsyncPipeline with {type(other)}\")\n</code></pre>"},{"location":"api/core/#rivusio.core.pipeline.AsyncPipeline.configure_parallel","title":"configure_parallel","text":"<pre><code>configure_parallel(strategy: Union[ExecutionStrategy, str], max_workers: Optional[int] = None, chunk_size: int = 1000) -&gt; None\n</code></pre> <p>Configure parallel execution.</p> <pre><code>strategy: Execution strategy to use\nmax_workers: Maximum number of workers (default: based on strategy)\nchunk_size: Size of chunks for process pool (default: 1000)\n</code></pre> Source code in <code>src/rivusio/core/pipeline.py</code> <pre><code>def configure_parallel(\n    self,\n    strategy: Union[ExecutionStrategy, str],\n    max_workers: Optional[int] = None,\n    chunk_size: int = 1000,\n) -&gt; None:\n    \"\"\"Configure parallel execution.\n\n    Args:\n    ----\n        strategy: Execution strategy to use\n        max_workers: Maximum number of workers (default: based on strategy)\n        chunk_size: Size of chunks for process pool (default: 1000)\n\n    \"\"\"\n    if isinstance(strategy, str):\n        strategy = ExecutionStrategy(strategy)\n    self._parallel_config = ParallelConfig(\n        strategy=strategy, max_workers=max_workers, chunk_size=chunk_size,\n    )\n    self._parallel_executor = ParallelExecutor(self._parallel_config)\n</code></pre>"},{"location":"api/core/#rivusio.core.pipeline.AsyncPipeline.execute_conditional","title":"execute_conditional  <code>async</code>","text":"<pre><code>execute_conditional(data: InputType, condition: Callable[[InputType], bool]) -&gt; Union[InputType, OutputType]\n</code></pre> <p>Execute pipeline only if condition is met.</p> <pre><code>data: Input data\ncondition: Function that takes input data and returns bool\n</code></pre> <pre><code>Processed data if condition is met, otherwise input data\n</code></pre> Source code in <code>src/rivusio/core/pipeline.py</code> <pre><code>async def execute_conditional(\n    self, data: InputType, condition: Callable[[InputType], bool],\n) -&gt; Union[InputType, OutputType]:\n    \"\"\"Execute pipeline only if condition is met.\n\n    Args:\n    ----\n        data: Input data\n        condition: Function that takes input data and returns bool\n\n    Returns:\n    -------\n        Processed data if condition is met, otherwise input data\n\n    \"\"\"\n    if condition(data):\n        return await self.process(data)\n    return data\n</code></pre>"},{"location":"api/core/#rivusio.core.pipeline.AsyncPipeline.execute_parallel","title":"execute_parallel  <code>async</code>","text":"<pre><code>execute_parallel(data: list[Any]) -&gt; list[Any]\n</code></pre> <p>Execute the pipeline on multiple inputs in parallel.</p> <p>This method supports different execution strategies: - GATHER: Uses asyncio.gather for I/O-bound tasks (default) - THREAD_POOL: Uses ThreadPoolExecutor for I/O and light CPU tasks - PROCESS_POOL: Uses ProcessPoolExecutor for CPU-intensive tasks</p> <pre><code>data: List of input data\n</code></pre> <pre><code>List of processed results\n</code></pre> <pre><code>PipeError: If any pipe fails\n</code></pre> Source code in <code>src/rivusio/core/pipeline.py</code> <pre><code>async def execute_parallel(self, data: list[Any]) -&gt; list[Any]:\n    \"\"\"Execute the pipeline on multiple inputs in parallel.\n\n    This method supports different execution strategies:\n    - GATHER: Uses asyncio.gather for I/O-bound tasks (default)\n    - THREAD_POOL: Uses ThreadPoolExecutor for I/O and light CPU tasks\n    - PROCESS_POOL: Uses ProcessPoolExecutor for CPU-intensive tasks\n\n    Args:\n    ----\n        data: List of input data\n\n    Returns:\n    -------\n        List of processed results\n\n    Raises:\n    ------\n        PipeError: If any pipe fails\n\n    \"\"\"\n    if not self._parallel_executor:\n        raise PipelineError(\n            self.name, RuntimeError(\"Must use async context manager for parallel execution\"),\n        )\n\n    return await self._parallel_executor.execute(self, data)\n</code></pre>"},{"location":"api/core/#rivusio.core.pipeline.AsyncPipeline.get_pipe_outputs","title":"get_pipe_outputs","text":"<pre><code>get_pipe_outputs(pipe: AsyncBasePipe) -&gt; list[Any]\n</code></pre> <p>Get all outputs from a specific pipe.</p> <pre><code>pipe: Pipe to get outputs for\n</code></pre> <pre><code>List of outputs from the pipe\n</code></pre> Source code in <code>src/rivusio/core/pipeline.py</code> <pre><code>def get_pipe_outputs(self, pipe: AsyncBasePipe) -&gt; list[Any]:\n    \"\"\"Get all outputs from a specific pipe.\n\n    Args:\n    ----\n        pipe: Pipe to get outputs for\n\n    Returns:\n    -------\n        List of outputs from the pipe\n\n    \"\"\"\n    return self._pipe_outputs.get(pipe, [])\n</code></pre>"},{"location":"api/core/#rivusio.core.pipeline.AsyncPipeline.process","title":"process  <code>async</code>","text":"<pre><code>process(data: InputType) -&gt; OutputType\n</code></pre> <p>Process data through all pipes in the pipeline.</p> Source code in <code>src/rivusio/core/pipeline.py</code> <pre><code>async def process(self, data: InputType) -&gt; OutputType:\n    \"\"\"Process data through all pipes in the pipeline.\"\"\"\n    result = data\n    for pipe in self._pipes:\n        try:\n            result = await pipe.process(result)\n            self._pipe_outputs[pipe].append(result)\n        except Exception as e:\n            raise PipelineError(pipe.__class__.__name__, e) from e\n    return cast(OutputType, result)\n</code></pre>"},{"location":"api/core/#mixedpipeline","title":"MixedPipeline","text":""},{"location":"api/core/#rivusio.core.pipeline.MixedPipeline","title":"rivusio.core.pipeline.MixedPipeline","text":"<p>             Bases: <code>AsyncBasePipe[InputType, OutputType]</code>, <code>PipelineMetricsMixin</code></p> <p>Pipeline for composing both sync and async pipes.</p> Source code in <code>src/rivusio/core/pipeline.py</code> <pre><code>class MixedPipeline(AsyncBasePipe[InputType, OutputType], PipelineMetricsMixin):\n    \"\"\"Pipeline for composing both sync and async pipes.\"\"\"\n\n    def __init__(\n        self,\n        pipes: Optional[list[Union[SyncBasePipe[Any, Any], AsyncBasePipe[Any, Any]]]] = None,\n        name: Optional[str] = None,\n    ) -&gt; None:\n        \"\"\"Initialize pipeline.\"\"\"\n        super().__init__()\n        self._pipes = pipes or []\n        self.name = name or \"MixedPipeline\"\n        self._pipe_outputs: dict[Union[SyncBasePipe, AsyncBasePipe], list[Any]] = {\n            pipe: [] for pipe in self._pipes\n        }\n        self._parallel_config = ParallelConfig()\n        self._parallel_executor: Optional[ParallelExecutor] = None\n\n    async def __aenter__(self) -&gt; \"MixedPipeline[InputType, OutputType]\":\n        \"\"\"Initialize resources.\"\"\"\n        if not self._parallel_executor and self._parallel_config:\n            self._parallel_executor = await ParallelExecutor(self._parallel_config).__aenter__()\n        return self\n\n    async def __aexit__(\n        self,\n        exc_type: Optional[type[BaseException]],\n        exc_val: Optional[BaseException],\n        exc_tb: types.TracebackType | None,\n    ) -&gt; None:\n        \"\"\"Clean up resources.\"\"\"\n        if self._parallel_executor:\n            await self._parallel_executor.__aexit__(exc_type, exc_val, exc_tb)\n\n\n    def __rshift__(\n        self,\n        other: Union[SyncBasePipe[InputType, OutputType], AsyncBasePipe[InputType, OutputType]],\n    ) -&gt; \"MixedPipeline[InputType, OutputType]\":\n        \"\"\"Compose this pipeline with another pipe using the &gt;&gt; operator.\"\"\"\n        return MixedPipeline([*self._pipes, other])\n\n\n    @property\n    def pipes(self) -&gt; list[Union[SyncBasePipe[Any, Any], AsyncBasePipe[Any, Any]]]:\n        \"\"\"Get pipeline's pipes.\"\"\"\n        return self._pipes\n\n    async def process(self, data: InputType) -&gt; OutputType:\n        \"\"\"Process data through all pipes in the pipeline.\"\"\"\n        result = data\n        for pipe in self._pipes:\n            try:\n                if isinstance(pipe, AsyncBasePipe):\n                    result = await pipe.process(result)\n                else:\n                    result = pipe.process(result)\n                self._pipe_outputs[pipe].append(result)\n            except Exception as e:\n                raise PipelineError(pipe.__class__.__name__, e) from e\n        return cast(OutputType, result)\n\n    def get_pipe_outputs(self, pipe: Union[SyncBasePipe, AsyncBasePipe]) -&gt; list[Any]:\n        \"\"\"Get all outputs from a specific pipe.\n\n        Args:\n        ----\n            pipe: Pipe to get outputs for\n\n        Returns:\n        -------\n            List of outputs from the pipe\n\n        \"\"\"\n        return self._pipe_outputs.get(pipe, [])\n\n    async def execute_parallel(self, data: list[Any]) -&gt; list[Any]:\n        \"\"\"Execute the pipeline on multiple inputs in parallel.\n\n        This method supports different execution strategies:\n        - GATHER: Uses asyncio.gather for I/O-bound tasks (default)\n        - THREAD_POOL: Uses ThreadPoolExecutor for I/O and light CPU tasks\n        - PROCESS_POOL: Uses ProcessPoolExecutor for CPU-intensive tasks\n\n        Args:\n        ----\n            data: List of input data\n\n        Returns:\n        -------\n            List of processed results\n\n        Raises:\n        ------\n            PipeError: If any pipe fails\n\n        \"\"\"\n        if not self._parallel_executor:\n            raise PipelineError(\n                self.name, RuntimeError(\"Must use async context manager for parallel execution\"),\n            )\n\n        return await self._parallel_executor.execute(self, data)\n\n    def configure_parallel(\n        self,\n        strategy: Union[ExecutionStrategy, str],\n        max_workers: Optional[int] = None,\n        chunk_size: int = 1000,\n    ) -&gt; None:\n        \"\"\"Configure parallel execution.\n\n        Args:\n        ----\n            strategy: Execution strategy to use\n            max_workers: Maximum number of workers (default: based on strategy)\n            chunk_size: Size of chunks for process pool (default: 1000)\n\n        \"\"\"\n        if isinstance(strategy, str):\n            strategy = ExecutionStrategy(strategy)\n        self._parallel_config = ParallelConfig(\n            strategy=strategy, max_workers=max_workers, chunk_size=chunk_size,\n        )\n        self._parallel_executor = ParallelExecutor(self._parallel_config)\n\n    async def execute_conditional(\n        self, data: InputType, condition: Callable[[InputType], bool],\n    ) -&gt; Union[InputType, OutputType]:\n        \"\"\"Execute pipeline only if condition is met.\n\n        Args:\n        ----\n            data: Input data\n            condition: Function that takes input data and returns bool\n\n        Returns:\n        -------\n            Processed data if condition is met, otherwise input data\n\n        \"\"\"\n        if condition(data):\n            return await self.process(data)\n        return data\n\n    def add(self, pipe: Union[AsyncBasePipe, SyncBasePipe]) -&gt; None:\n        \"\"\"Add a pipe to the pipeline.\n\n        Args:\n        ----\n            pipe: Pipe to add\n\n        \"\"\"\n        self._pipes.append(pipe)\n        self._pipe_outputs[pipe] = []\n</code></pre>"},{"location":"api/core/#rivusio.core.pipeline.MixedPipeline.name","title":"name  <code>instance-attribute</code>","text":"<pre><code>name = name or 'MixedPipeline'\n</code></pre>"},{"location":"api/core/#rivusio.core.pipeline.MixedPipeline.pipes","title":"pipes  <code>property</code>","text":"<pre><code>pipes: list[Union[SyncBasePipe[Any, Any], AsyncBasePipe[Any, Any]]]\n</code></pre> <p>Get pipeline's pipes.</p>"},{"location":"api/core/#rivusio.core.pipeline.MixedPipeline.__aenter__","title":"__aenter__  <code>async</code>","text":"<pre><code>__aenter__() -&gt; MixedPipeline[InputType, OutputType]\n</code></pre> <p>Initialize resources.</p> Source code in <code>src/rivusio/core/pipeline.py</code> <pre><code>async def __aenter__(self) -&gt; \"MixedPipeline[InputType, OutputType]\":\n    \"\"\"Initialize resources.\"\"\"\n    if not self._parallel_executor and self._parallel_config:\n        self._parallel_executor = await ParallelExecutor(self._parallel_config).__aenter__()\n    return self\n</code></pre>"},{"location":"api/core/#rivusio.core.pipeline.MixedPipeline.__aexit__","title":"__aexit__  <code>async</code>","text":"<pre><code>__aexit__(exc_type: Optional[type[BaseException]], exc_val: Optional[BaseException], exc_tb: types.TracebackType | None) -&gt; None\n</code></pre> <p>Clean up resources.</p> Source code in <code>src/rivusio/core/pipeline.py</code> <pre><code>async def __aexit__(\n    self,\n    exc_type: Optional[type[BaseException]],\n    exc_val: Optional[BaseException],\n    exc_tb: types.TracebackType | None,\n) -&gt; None:\n    \"\"\"Clean up resources.\"\"\"\n    if self._parallel_executor:\n        await self._parallel_executor.__aexit__(exc_type, exc_val, exc_tb)\n</code></pre>"},{"location":"api/core/#rivusio.core.pipeline.MixedPipeline.__init__","title":"__init__","text":"<pre><code>__init__(pipes: Optional[list[Union[SyncBasePipe[Any, Any], AsyncBasePipe[Any, Any]]]] = None, name: Optional[str] = None) -&gt; None\n</code></pre> <p>Initialize pipeline.</p> Source code in <code>src/rivusio/core/pipeline.py</code> <pre><code>def __init__(\n    self,\n    pipes: Optional[list[Union[SyncBasePipe[Any, Any], AsyncBasePipe[Any, Any]]]] = None,\n    name: Optional[str] = None,\n) -&gt; None:\n    \"\"\"Initialize pipeline.\"\"\"\n    super().__init__()\n    self._pipes = pipes or []\n    self.name = name or \"MixedPipeline\"\n    self._pipe_outputs: dict[Union[SyncBasePipe, AsyncBasePipe], list[Any]] = {\n        pipe: [] for pipe in self._pipes\n    }\n    self._parallel_config = ParallelConfig()\n    self._parallel_executor: Optional[ParallelExecutor] = None\n</code></pre>"},{"location":"api/core/#rivusio.core.pipeline.MixedPipeline.__rshift__","title":"__rshift__","text":"<pre><code>__rshift__(other: Union[SyncBasePipe[InputType, OutputType], AsyncBasePipe[InputType, OutputType]]) -&gt; MixedPipeline[InputType, OutputType]\n</code></pre> <p>Compose this pipeline with another pipe using the &gt;&gt; operator.</p> Source code in <code>src/rivusio/core/pipeline.py</code> <pre><code>def __rshift__(\n    self,\n    other: Union[SyncBasePipe[InputType, OutputType], AsyncBasePipe[InputType, OutputType]],\n) -&gt; \"MixedPipeline[InputType, OutputType]\":\n    \"\"\"Compose this pipeline with another pipe using the &gt;&gt; operator.\"\"\"\n    return MixedPipeline([*self._pipes, other])\n</code></pre>"},{"location":"api/core/#rivusio.core.pipeline.MixedPipeline.add","title":"add","text":"<pre><code>add(pipe: Union[AsyncBasePipe, SyncBasePipe]) -&gt; None\n</code></pre> <p>Add a pipe to the pipeline.</p> <pre><code>pipe: Pipe to add\n</code></pre> Source code in <code>src/rivusio/core/pipeline.py</code> <pre><code>def add(self, pipe: Union[AsyncBasePipe, SyncBasePipe]) -&gt; None:\n    \"\"\"Add a pipe to the pipeline.\n\n    Args:\n    ----\n        pipe: Pipe to add\n\n    \"\"\"\n    self._pipes.append(pipe)\n    self._pipe_outputs[pipe] = []\n</code></pre>"},{"location":"api/core/#rivusio.core.pipeline.MixedPipeline.configure_parallel","title":"configure_parallel","text":"<pre><code>configure_parallel(strategy: Union[ExecutionStrategy, str], max_workers: Optional[int] = None, chunk_size: int = 1000) -&gt; None\n</code></pre> <p>Configure parallel execution.</p> <pre><code>strategy: Execution strategy to use\nmax_workers: Maximum number of workers (default: based on strategy)\nchunk_size: Size of chunks for process pool (default: 1000)\n</code></pre> Source code in <code>src/rivusio/core/pipeline.py</code> <pre><code>def configure_parallel(\n    self,\n    strategy: Union[ExecutionStrategy, str],\n    max_workers: Optional[int] = None,\n    chunk_size: int = 1000,\n) -&gt; None:\n    \"\"\"Configure parallel execution.\n\n    Args:\n    ----\n        strategy: Execution strategy to use\n        max_workers: Maximum number of workers (default: based on strategy)\n        chunk_size: Size of chunks for process pool (default: 1000)\n\n    \"\"\"\n    if isinstance(strategy, str):\n        strategy = ExecutionStrategy(strategy)\n    self._parallel_config = ParallelConfig(\n        strategy=strategy, max_workers=max_workers, chunk_size=chunk_size,\n    )\n    self._parallel_executor = ParallelExecutor(self._parallel_config)\n</code></pre>"},{"location":"api/core/#rivusio.core.pipeline.MixedPipeline.execute_conditional","title":"execute_conditional  <code>async</code>","text":"<pre><code>execute_conditional(data: InputType, condition: Callable[[InputType], bool]) -&gt; Union[InputType, OutputType]\n</code></pre> <p>Execute pipeline only if condition is met.</p> <pre><code>data: Input data\ncondition: Function that takes input data and returns bool\n</code></pre> <pre><code>Processed data if condition is met, otherwise input data\n</code></pre> Source code in <code>src/rivusio/core/pipeline.py</code> <pre><code>async def execute_conditional(\n    self, data: InputType, condition: Callable[[InputType], bool],\n) -&gt; Union[InputType, OutputType]:\n    \"\"\"Execute pipeline only if condition is met.\n\n    Args:\n    ----\n        data: Input data\n        condition: Function that takes input data and returns bool\n\n    Returns:\n    -------\n        Processed data if condition is met, otherwise input data\n\n    \"\"\"\n    if condition(data):\n        return await self.process(data)\n    return data\n</code></pre>"},{"location":"api/core/#rivusio.core.pipeline.MixedPipeline.execute_parallel","title":"execute_parallel  <code>async</code>","text":"<pre><code>execute_parallel(data: list[Any]) -&gt; list[Any]\n</code></pre> <p>Execute the pipeline on multiple inputs in parallel.</p> <p>This method supports different execution strategies: - GATHER: Uses asyncio.gather for I/O-bound tasks (default) - THREAD_POOL: Uses ThreadPoolExecutor for I/O and light CPU tasks - PROCESS_POOL: Uses ProcessPoolExecutor for CPU-intensive tasks</p> <pre><code>data: List of input data\n</code></pre> <pre><code>List of processed results\n</code></pre> <pre><code>PipeError: If any pipe fails\n</code></pre> Source code in <code>src/rivusio/core/pipeline.py</code> <pre><code>async def execute_parallel(self, data: list[Any]) -&gt; list[Any]:\n    \"\"\"Execute the pipeline on multiple inputs in parallel.\n\n    This method supports different execution strategies:\n    - GATHER: Uses asyncio.gather for I/O-bound tasks (default)\n    - THREAD_POOL: Uses ThreadPoolExecutor for I/O and light CPU tasks\n    - PROCESS_POOL: Uses ProcessPoolExecutor for CPU-intensive tasks\n\n    Args:\n    ----\n        data: List of input data\n\n    Returns:\n    -------\n        List of processed results\n\n    Raises:\n    ------\n        PipeError: If any pipe fails\n\n    \"\"\"\n    if not self._parallel_executor:\n        raise PipelineError(\n            self.name, RuntimeError(\"Must use async context manager for parallel execution\"),\n        )\n\n    return await self._parallel_executor.execute(self, data)\n</code></pre>"},{"location":"api/core/#rivusio.core.pipeline.MixedPipeline.get_pipe_outputs","title":"get_pipe_outputs","text":"<pre><code>get_pipe_outputs(pipe: Union[SyncBasePipe, AsyncBasePipe]) -&gt; list[Any]\n</code></pre> <p>Get all outputs from a specific pipe.</p> <pre><code>pipe: Pipe to get outputs for\n</code></pre> <pre><code>List of outputs from the pipe\n</code></pre> Source code in <code>src/rivusio/core/pipeline.py</code> <pre><code>def get_pipe_outputs(self, pipe: Union[SyncBasePipe, AsyncBasePipe]) -&gt; list[Any]:\n    \"\"\"Get all outputs from a specific pipe.\n\n    Args:\n    ----\n        pipe: Pipe to get outputs for\n\n    Returns:\n    -------\n        List of outputs from the pipe\n\n    \"\"\"\n    return self._pipe_outputs.get(pipe, [])\n</code></pre>"},{"location":"api/core/#rivusio.core.pipeline.MixedPipeline.process","title":"process  <code>async</code>","text":"<pre><code>process(data: InputType) -&gt; OutputType\n</code></pre> <p>Process data through all pipes in the pipeline.</p> Source code in <code>src/rivusio/core/pipeline.py</code> <pre><code>async def process(self, data: InputType) -&gt; OutputType:\n    \"\"\"Process data through all pipes in the pipeline.\"\"\"\n    result = data\n    for pipe in self._pipes:\n        try:\n            if isinstance(pipe, AsyncBasePipe):\n                result = await pipe.process(result)\n            else:\n                result = pipe.process(result)\n            self._pipe_outputs[pipe].append(result)\n        except Exception as e:\n            raise PipelineError(pipe.__class__.__name__, e) from e\n    return cast(OutputType, result)\n</code></pre>"},{"location":"api/core/#exceptions","title":"Exceptions","text":""},{"location":"api/core/#rivusio.core.exceptions.PipeError","title":"rivusio.core.exceptions.PipeError","text":"<p>             Bases: <code>Exception</code></p> <p>Exception raised when an error occurs within a single pipe.</p> <p>This exception wraps the original error that occurred during pipe processing and provides context about which pipe failed.</p> <pre><code>pipe: Name or identifier of the pipe where the error occurred\nerror: The original exception that was raised\n</code></pre>"},{"location":"api/core/#rivusio.core.exceptions.PipeError--example","title":"Example:","text":"<pre><code>&gt;&gt;&gt; class DataValidationPipe(BasePipe[Dict, Dict]):\n...     async def process(self, data: Dict) -&gt; Dict:\n...         try:\n...             return await validate_data(data)\n...         except ValidationError as e:\n...             raise PipeError(self.__class__.__name__, e)\n...\n&gt;&gt;&gt; pipe = DataValidationPipe()\n&gt;&gt;&gt; try:\n...     await pipe.process({\"invalid\": \"data\"})\n... except PipeError as e:\n...     print(f\"Pipe: {e.pipe}, Error: {e.error}\")\n</code></pre> Source code in <code>src/rivusio/core/exceptions.py</code> <pre><code>class PipeError(Exception):\n    \"\"\"Exception raised when an error occurs within a single pipe.\n\n    This exception wraps the original error that occurred during pipe processing\n    and provides context about which pipe failed.\n\n    Attributes:\n    ----------\n        pipe: Name or identifier of the pipe where the error occurred\n        error: The original exception that was raised\n\n    Example:\n    -------\n        &gt;&gt;&gt; class DataValidationPipe(BasePipe[Dict, Dict]):\n        ...     async def process(self, data: Dict) -&gt; Dict:\n        ...         try:\n        ...             return await validate_data(data)\n        ...         except ValidationError as e:\n        ...             raise PipeError(self.__class__.__name__, e)\n        ...\n        &gt;&gt;&gt; pipe = DataValidationPipe()\n        &gt;&gt;&gt; try:\n        ...     await pipe.process({\"invalid\": \"data\"})\n        ... except PipeError as e:\n        ...     print(f\"Pipe: {e.pipe}, Error: {e.error}\")\n\n    \"\"\"\n\n    def __init__(self, pipe: str, error: Exception) -&gt; None:\n        \"\"\"Initialize a PipeError.\n\n        Args:\n        ----\n            pipe: Name or identifier of the pipe where the error occurred\n            error: The original exception that was raised\n\n        \"\"\"\n        self.pipe = pipe\n        self.error = error\n        super().__init__(f\"Error in pipe {pipe}: {error}\")\n</code></pre>"},{"location":"api/core/#rivusio.core.exceptions.PipeError.error","title":"error  <code>instance-attribute</code>","text":"<pre><code>error = error\n</code></pre>"},{"location":"api/core/#rivusio.core.exceptions.PipeError.pipe","title":"pipe  <code>instance-attribute</code>","text":"<pre><code>pipe = pipe\n</code></pre>"},{"location":"api/core/#rivusio.core.exceptions.PipeError.__init__","title":"__init__","text":"<pre><code>__init__(pipe: str, error: Exception) -&gt; None\n</code></pre> <p>Initialize a PipeError.</p> <pre><code>pipe: Name or identifier of the pipe where the error occurred\nerror: The original exception that was raised\n</code></pre> Source code in <code>src/rivusio/core/exceptions.py</code> <pre><code>def __init__(self, pipe: str, error: Exception) -&gt; None:\n    \"\"\"Initialize a PipeError.\n\n    Args:\n    ----\n        pipe: Name or identifier of the pipe where the error occurred\n        error: The original exception that was raised\n\n    \"\"\"\n    self.pipe = pipe\n    self.error = error\n    super().__init__(f\"Error in pipe {pipe}: {error}\")\n</code></pre>"},{"location":"api/core/#rivusio.core.exceptions.PipelineError","title":"rivusio.core.exceptions.PipelineError","text":"<p>             Bases: <code>PipeError</code></p> <p>Exception raised when an error occurs at the pipeline level.</p> <p>This exception is typically raised when there's an error coordinating multiple pipes or when pipeline-wide operations fail. It captures both the failing pipe and the original error.</p> <pre><code>pipe: Name or identifier of the pipe where the error occurred\nerror: The original exception that was raised\n</code></pre>"},{"location":"api/core/#rivusio.core.exceptions.PipelineError--example","title":"Example:","text":"<pre><code>&gt;&gt;&gt; pipeline = AsyncPipeline([pipe1, pipe2, pipe3])\n&gt;&gt;&gt; try:\n...     await pipeline.process(data)\n... except PipelineError as e:\n...     print(f\"Failed pipe: {e.pipe}, Error: {e.error}\")\n</code></pre> Source code in <code>src/rivusio/core/exceptions.py</code> <pre><code>class PipelineError(PipeError):\n    \"\"\"Exception raised when an error occurs at the pipeline level.\n\n    This exception is typically raised when there's an error coordinating\n    multiple pipes or when pipeline-wide operations fail. It captures both\n    the failing pipe and the original error.\n\n    Attributes:\n    ----------\n        pipe: Name or identifier of the pipe where the error occurred\n        error: The original exception that was raised\n\n    Example:\n    -------\n        &gt;&gt;&gt; pipeline = AsyncPipeline([pipe1, pipe2, pipe3])\n        &gt;&gt;&gt; try:\n        ...     await pipeline.process(data)\n        ... except PipelineError as e:\n        ...     print(f\"Failed pipe: {e.pipe}, Error: {e.error}\")\n\n    \"\"\"\n\n    def __init__(self, pipe: str, error: Exception) -&gt; None:\n        \"\"\"Initialize a PipelineError.\n\n        Args:\n        ----\n            pipe: Name or identifier of the pipe where the error occurred\n            error: The original exception that was raised\n\n        \"\"\"\n        super().__init__(pipe, error)\n</code></pre>"},{"location":"api/core/#rivusio.core.exceptions.PipelineError.__init__","title":"__init__","text":"<pre><code>__init__(pipe: str, error: Exception) -&gt; None\n</code></pre> <p>Initialize a PipelineError.</p> <pre><code>pipe: Name or identifier of the pipe where the error occurred\nerror: The original exception that was raised\n</code></pre> Source code in <code>src/rivusio/core/exceptions.py</code> <pre><code>def __init__(self, pipe: str, error: Exception) -&gt; None:\n    \"\"\"Initialize a PipelineError.\n\n    Args:\n    ----\n        pipe: Name or identifier of the pipe where the error occurred\n        error: The original exception that was raised\n\n    \"\"\"\n    super().__init__(pipe, error)\n</code></pre>"},{"location":"api/monitoring/","title":"Monitoring API Reference","text":""},{"location":"api/monitoring/#metricscollector","title":"MetricsCollector","text":""},{"location":"api/monitoring/#rivusio.monitoring.metrics.MetricsCollector","title":"rivusio.monitoring.metrics.MetricsCollector","text":"<p>Collects and aggregates multiple types of metrics over time windows.</p> <p>Manages multiple metric windows for different performance indicators like processing times, error rates, and throughput. Provides a unified interface for recording measurements and retrieving aggregated statistics.</p>"},{"location":"api/monitoring/#rivusio.monitoring.metrics.MetricsCollector--example","title":"Example:","text":"<pre><code>```python\nfrom datetime import timedelta\n\n# Create collector with 5-minute windows\ncollector = MetricsCollector(window_size=timedelta(minutes=5))\n\n# Record various metrics\ncollector.record_processing_time(0.123)  # 123ms processing time\ncollector.record_success()  # Record successful operation\ncollector.record_throughput(100)  # Processed 100 items\n\n# Get aggregated metrics\nmetrics = collector.get_metrics()\nprint(f\"Avg processing time: {metrics.processing_time.average:.3f}\")\nprint(f\"Error rate: {metrics.error_rate.average:.1f}\")  # Error rate: 0.0\nprint(f\"Throughput: {metrics.throughput.average:.0f}\")  # Throughput: 100\n```\n</code></pre> Source code in <code>src/rivusio/monitoring/metrics.py</code> <pre><code>class MetricsCollector:\n    \"\"\"Collects and aggregates multiple types of metrics over time windows.\n\n    Manages multiple metric windows for different performance indicators like\n    processing times, error rates, and throughput. Provides a unified interface\n    for recording measurements and retrieving aggregated statistics.\n\n    Example:\n    -------\n        ```python\n        from datetime import timedelta\n\n        # Create collector with 5-minute windows\n        collector = MetricsCollector(window_size=timedelta(minutes=5))\n\n        # Record various metrics\n        collector.record_processing_time(0.123)  # 123ms processing time\n        collector.record_success()  # Record successful operation\n        collector.record_throughput(100)  # Processed 100 items\n\n        # Get aggregated metrics\n        metrics = collector.get_metrics()\n        print(f\"Avg processing time: {metrics.processing_time.average:.3f}\")\n        print(f\"Error rate: {metrics.error_rate.average:.1f}\")  # Error rate: 0.0\n        print(f\"Throughput: {metrics.throughput.average:.0f}\")  # Throughput: 100\n        ```\n\n    \"\"\"\n\n    def __init__(self, window_size: timedelta = timedelta(minutes=5)) -&gt; None:\n        \"\"\"Initialize metrics collector with specified window size.\n\n        Args:\n        ----\n            window_size: Duration for the sliding windows. Defaults to 5 minutes\n\n        \"\"\"\n        self.window_size = window_size\n        self.processing_times = MetricWindow(values=[], window_size=window_size)\n        self.error_rates = MetricWindow(values=[], window_size=window_size)\n        self.throughput = MetricWindow(values=[], window_size=window_size)\n\n    def record_processing_time(self, duration: float) -&gt; None:\n        \"\"\"Record the duration of a processing operation.\n\n        Args:\n        ----\n            duration: Processing time in seconds\n\n        \"\"\"\n        self.processing_times.add_value(duration)\n\n    def record_error(self) -&gt; None:\n        \"\"\"Record an error occurrence.\n\n        Adds a value of 1.0 to the error rate window.\n        \"\"\"\n        self.error_rates.add_value(1.0)\n\n    def record_success(self) -&gt; None:\n        \"\"\"Record a successful operation.\n\n        Adds a value of 0.0 to the error rate window.\n        \"\"\"\n        self.error_rates.add_value(0.0)\n\n    def record_throughput(self, items: int) -&gt; None:\n        \"\"\"Record the number of items processed.\n\n        Args:\n        ----\n            items: Number of items processed in this operation\n\n        \"\"\"\n        self.throughput.add_value(float(items))\n\n    def get_metrics(self) -&gt; dict[str, MetricWindow]:\n        \"\"\"Get current metrics for all monitored indicators.\n\n        Returns\n        -------\n            Dictionary containing aggregated statistics for all metrics:\n            processing_time: Statistics about processing duration\n            error_rate: Statistics about error frequency\n            throughput: Statistics about items processed\n\n        \"\"\"\n        return {\n            \"processing_time\": self.processing_times,\n            \"error_rate\": self.error_rates,\n            \"throughput\": self.throughput,\n        }\n</code></pre>"},{"location":"api/monitoring/#rivusio.monitoring.metrics.MetricsCollector.error_rates","title":"error_rates  <code>instance-attribute</code>","text":"<pre><code>error_rates = MetricWindow(values=[], window_size=window_size)\n</code></pre>"},{"location":"api/monitoring/#rivusio.monitoring.metrics.MetricsCollector.processing_times","title":"processing_times  <code>instance-attribute</code>","text":"<pre><code>processing_times = MetricWindow(values=[], window_size=window_size)\n</code></pre>"},{"location":"api/monitoring/#rivusio.monitoring.metrics.MetricsCollector.throughput","title":"throughput  <code>instance-attribute</code>","text":"<pre><code>throughput = MetricWindow(values=[], window_size=window_size)\n</code></pre>"},{"location":"api/monitoring/#rivusio.monitoring.metrics.MetricsCollector.window_size","title":"window_size  <code>instance-attribute</code>","text":"<pre><code>window_size = window_size\n</code></pre>"},{"location":"api/monitoring/#rivusio.monitoring.metrics.MetricsCollector.__init__","title":"__init__","text":"<pre><code>__init__(window_size: timedelta = timedelta(minutes=5)) -&gt; None\n</code></pre> <p>Initialize metrics collector with specified window size.</p> <pre><code>window_size: Duration for the sliding windows. Defaults to 5 minutes\n</code></pre> Source code in <code>src/rivusio/monitoring/metrics.py</code> <pre><code>def __init__(self, window_size: timedelta = timedelta(minutes=5)) -&gt; None:\n    \"\"\"Initialize metrics collector with specified window size.\n\n    Args:\n    ----\n        window_size: Duration for the sliding windows. Defaults to 5 minutes\n\n    \"\"\"\n    self.window_size = window_size\n    self.processing_times = MetricWindow(values=[], window_size=window_size)\n    self.error_rates = MetricWindow(values=[], window_size=window_size)\n    self.throughput = MetricWindow(values=[], window_size=window_size)\n</code></pre>"},{"location":"api/monitoring/#rivusio.monitoring.metrics.MetricsCollector.get_metrics","title":"get_metrics","text":"<pre><code>get_metrics() -&gt; dict[str, MetricWindow]\n</code></pre> <p>Get current metrics for all monitored indicators.</p>"},{"location":"api/monitoring/#rivusio.monitoring.metrics.MetricsCollector.get_metrics--returns","title":"Returns","text":"<pre><code>Dictionary containing aggregated statistics for all metrics:\nprocessing_time: Statistics about processing duration\nerror_rate: Statistics about error frequency\nthroughput: Statistics about items processed\n</code></pre> Source code in <code>src/rivusio/monitoring/metrics.py</code> <pre><code>def get_metrics(self) -&gt; dict[str, MetricWindow]:\n    \"\"\"Get current metrics for all monitored indicators.\n\n    Returns\n    -------\n        Dictionary containing aggregated statistics for all metrics:\n        processing_time: Statistics about processing duration\n        error_rate: Statistics about error frequency\n        throughput: Statistics about items processed\n\n    \"\"\"\n    return {\n        \"processing_time\": self.processing_times,\n        \"error_rate\": self.error_rates,\n        \"throughput\": self.throughput,\n    }\n</code></pre>"},{"location":"api/monitoring/#rivusio.monitoring.metrics.MetricsCollector.record_error","title":"record_error","text":"<pre><code>record_error() -&gt; None\n</code></pre> <p>Record an error occurrence.</p> <p>Adds a value of 1.0 to the error rate window.</p> Source code in <code>src/rivusio/monitoring/metrics.py</code> <pre><code>def record_error(self) -&gt; None:\n    \"\"\"Record an error occurrence.\n\n    Adds a value of 1.0 to the error rate window.\n    \"\"\"\n    self.error_rates.add_value(1.0)\n</code></pre>"},{"location":"api/monitoring/#rivusio.monitoring.metrics.MetricsCollector.record_processing_time","title":"record_processing_time","text":"<pre><code>record_processing_time(duration: float) -&gt; None\n</code></pre> <p>Record the duration of a processing operation.</p> <pre><code>duration: Processing time in seconds\n</code></pre> Source code in <code>src/rivusio/monitoring/metrics.py</code> <pre><code>def record_processing_time(self, duration: float) -&gt; None:\n    \"\"\"Record the duration of a processing operation.\n\n    Args:\n    ----\n        duration: Processing time in seconds\n\n    \"\"\"\n    self.processing_times.add_value(duration)\n</code></pre>"},{"location":"api/monitoring/#rivusio.monitoring.metrics.MetricsCollector.record_success","title":"record_success","text":"<pre><code>record_success() -&gt; None\n</code></pre> <p>Record a successful operation.</p> <p>Adds a value of 0.0 to the error rate window.</p> Source code in <code>src/rivusio/monitoring/metrics.py</code> <pre><code>def record_success(self) -&gt; None:\n    \"\"\"Record a successful operation.\n\n    Adds a value of 0.0 to the error rate window.\n    \"\"\"\n    self.error_rates.add_value(0.0)\n</code></pre>"},{"location":"api/monitoring/#rivusio.monitoring.metrics.MetricsCollector.record_throughput","title":"record_throughput","text":"<pre><code>record_throughput(items: int) -&gt; None\n</code></pre> <p>Record the number of items processed.</p> <pre><code>items: Number of items processed in this operation\n</code></pre> Source code in <code>src/rivusio/monitoring/metrics.py</code> <pre><code>def record_throughput(self, items: int) -&gt; None:\n    \"\"\"Record the number of items processed.\n\n    Args:\n    ----\n        items: Number of items processed in this operation\n\n    \"\"\"\n    self.throughput.add_value(float(items))\n</code></pre>"},{"location":"api/monitoring/#pipelinemonitor","title":"PipelineMonitor","text":""},{"location":"api/monitoring/#rivusio.monitoring.pipeline_monitor.PipelineMonitor","title":"rivusio.monitoring.pipeline_monitor.PipelineMonitor","text":"<p>             Bases: <code>MetricsCollector</code></p> <p>Monitor for collecting pipeline execution metrics with timing capabilities.</p> <p>Extends the base MetricsCollector with pipeline-specific features: - Pipeline execution timing (start/stop/total time) - Pipeline-level success/error rates - Processing throughput monitoring</p>"},{"location":"api/monitoring/#rivusio.monitoring.pipeline_monitor.PipelineMonitor--example","title":"Example:","text":"<pre><code>```python\nfrom datetime import timedelta\nfrom rivusio.monitoring import PipelineMonitor\n\n# Create and attach monitor to pipeline\nmonitor = PipelineMonitor(window_size=timedelta(minutes=10))\npipeline.set_monitor(monitor)\n\n# Start monitoring\nmonitor.start()\n\ntry:\n    # Process data\n    await pipeline.process(data)\n    monitor.record_success()\nexcept Exception as e:\n    monitor.record_error()\n    raise\nfinally:\n    monitor.stop()\n\n# Get metrics\nmetrics = monitor.get_metrics()\nprint(f\"Total time: {metrics['total_time']:.2f}s\")  # Total time: 1.23s\nprint(f\"Error rate: {metrics['error_rate']['avg']:.2%}\")  # Error rate: 0.00%\n```\n</code></pre> Source code in <code>src/rivusio/monitoring/pipeline_monitor.py</code> <pre><code>class PipelineMonitor(MetricsCollector):\n    \"\"\"Monitor for collecting pipeline execution metrics with timing capabilities.\n\n    Extends the base MetricsCollector with pipeline-specific features:\n    - Pipeline execution timing (start/stop/total time)\n    - Pipeline-level success/error rates\n    - Processing throughput monitoring\n\n    Example:\n    -------\n        ```python\n        from datetime import timedelta\n        from rivusio.monitoring import PipelineMonitor\n\n        # Create and attach monitor to pipeline\n        monitor = PipelineMonitor(window_size=timedelta(minutes=10))\n        pipeline.set_monitor(monitor)\n\n        # Start monitoring\n        monitor.start()\n\n        try:\n            # Process data\n            await pipeline.process(data)\n            monitor.record_success()\n        except Exception as e:\n            monitor.record_error()\n            raise\n        finally:\n            monitor.stop()\n\n        # Get metrics\n        metrics = monitor.get_metrics()\n        print(f\"Total time: {metrics['total_time']:.2f}s\")  # Total time: 1.23s\n        print(f\"Error rate: {metrics['error_rate']['avg']:.2%}\")  # Error rate: 0.00%\n        ```\n\n    \"\"\"\n\n    def __init__(self, window_size: timedelta = timedelta(minutes=5)) -&gt; None:\n        \"\"\"Initialize pipeline monitor.\n\n        Args:\n        ----\n            window_size: Time window for metrics aggregation. Controls how long\n                        measurements are kept for calculating statistics.\n                        Defaults to 5 minutes.\n\n        \"\"\"\n        super().__init__(window_size=window_size)\n        self._start_time: Optional[datetime] = None\n        self._end_time: Optional[datetime] = None\n\n    def start(self) -&gt; None:\n        \"\"\"Start monitoring pipeline execution.\n\n        Records the start time for pipeline execution timing. Should be called\n        before pipeline processing begins.\n        \"\"\"\n        self._start_time = datetime.now()\n\n    def stop(self) -&gt; None:\n        \"\"\"Stop monitoring pipeline execution.\n\n        Records the end time for pipeline execution timing. Should be called\n        after pipeline processing completes (in success or failure cases).\n\n        Note:\n        ----\n            Should typically be called in a finally block to ensure timing\n            is recorded even if processing fails.\n\n        \"\"\"\n        self._end_time = datetime.now()\n\n    @property\n    def total_time(self) -&gt; float:\n        \"\"\"Get total execution time in seconds.\n\n        Calculates the total time elapsed between start() and stop() calls.\n        If stop() hasn't been called yet, uses the current time as end time.\n        If start() hasn't been called, returns 0.0.\n\n        Returns\n        -------\n            Total execution time in seconds as a float.\n            Returns 0.0 if monitoring hasn't started.\n\n        \"\"\"\n        if not self._start_time:\n            return 0.0\n        end = self._end_time or datetime.now()\n        return (end - self._start_time).total_seconds()\n\n    def get_metrics(self) -&gt; dict[str, Any]:\n        \"\"\"Get all collected metrics including total execution time.\n\n        Extends the base metrics collection with pipeline-specific metrics\n        including total execution time.\n\n        Returns\n        -------\n            Dictionary containing all metrics:\n            - All base metrics (processing_time, error_rate, throughput)\n            - total_time: Total pipeline execution time in seconds\n\n        \"\"\"\n        metrics = super().get_metrics()\n        metrics[\"total_time\"] = self.total_time  # type: ignore\n        return metrics\n</code></pre>"},{"location":"api/monitoring/#rivusio.monitoring.pipeline_monitor.PipelineMonitor.total_time","title":"total_time  <code>property</code>","text":"<pre><code>total_time: float\n</code></pre> <p>Get total execution time in seconds.</p> <p>Calculates the total time elapsed between start() and stop() calls. If stop() hasn't been called yet, uses the current time as end time. If start() hasn't been called, returns 0.0.</p>"},{"location":"api/monitoring/#rivusio.monitoring.pipeline_monitor.PipelineMonitor.total_time--returns","title":"Returns","text":"<pre><code>Total execution time in seconds as a float.\nReturns 0.0 if monitoring hasn't started.\n</code></pre>"},{"location":"api/monitoring/#rivusio.monitoring.pipeline_monitor.PipelineMonitor.__init__","title":"__init__","text":"<pre><code>__init__(window_size: timedelta = timedelta(minutes=5)) -&gt; None\n</code></pre> <p>Initialize pipeline monitor.</p> <pre><code>window_size: Time window for metrics aggregation. Controls how long\n            measurements are kept for calculating statistics.\n            Defaults to 5 minutes.\n</code></pre> Source code in <code>src/rivusio/monitoring/pipeline_monitor.py</code> <pre><code>def __init__(self, window_size: timedelta = timedelta(minutes=5)) -&gt; None:\n    \"\"\"Initialize pipeline monitor.\n\n    Args:\n    ----\n        window_size: Time window for metrics aggregation. Controls how long\n                    measurements are kept for calculating statistics.\n                    Defaults to 5 minutes.\n\n    \"\"\"\n    super().__init__(window_size=window_size)\n    self._start_time: Optional[datetime] = None\n    self._end_time: Optional[datetime] = None\n</code></pre>"},{"location":"api/monitoring/#rivusio.monitoring.pipeline_monitor.PipelineMonitor.get_metrics","title":"get_metrics","text":"<pre><code>get_metrics() -&gt; dict[str, Any]\n</code></pre> <p>Get all collected metrics including total execution time.</p> <p>Extends the base metrics collection with pipeline-specific metrics including total execution time.</p>"},{"location":"api/monitoring/#rivusio.monitoring.pipeline_monitor.PipelineMonitor.get_metrics--returns","title":"Returns","text":"<pre><code>Dictionary containing all metrics:\n- All base metrics (processing_time, error_rate, throughput)\n- total_time: Total pipeline execution time in seconds\n</code></pre> Source code in <code>src/rivusio/monitoring/pipeline_monitor.py</code> <pre><code>def get_metrics(self) -&gt; dict[str, Any]:\n    \"\"\"Get all collected metrics including total execution time.\n\n    Extends the base metrics collection with pipeline-specific metrics\n    including total execution time.\n\n    Returns\n    -------\n        Dictionary containing all metrics:\n        - All base metrics (processing_time, error_rate, throughput)\n        - total_time: Total pipeline execution time in seconds\n\n    \"\"\"\n    metrics = super().get_metrics()\n    metrics[\"total_time\"] = self.total_time  # type: ignore\n    return metrics\n</code></pre>"},{"location":"api/monitoring/#rivusio.monitoring.pipeline_monitor.PipelineMonitor.start","title":"start","text":"<pre><code>start() -&gt; None\n</code></pre> <p>Start monitoring pipeline execution.</p> <p>Records the start time for pipeline execution timing. Should be called before pipeline processing begins.</p> Source code in <code>src/rivusio/monitoring/pipeline_monitor.py</code> <pre><code>def start(self) -&gt; None:\n    \"\"\"Start monitoring pipeline execution.\n\n    Records the start time for pipeline execution timing. Should be called\n    before pipeline processing begins.\n    \"\"\"\n    self._start_time = datetime.now()\n</code></pre>"},{"location":"api/monitoring/#rivusio.monitoring.pipeline_monitor.PipelineMonitor.stop","title":"stop","text":"<pre><code>stop() -&gt; None\n</code></pre> <p>Stop monitoring pipeline execution.</p> <p>Records the end time for pipeline execution timing. Should be called after pipeline processing completes (in success or failure cases).</p>"},{"location":"api/monitoring/#rivusio.monitoring.pipeline_monitor.PipelineMonitor.stop--note","title":"Note:","text":"<pre><code>Should typically be called in a finally block to ensure timing\nis recorded even if processing fails.\n</code></pre> Source code in <code>src/rivusio/monitoring/pipeline_monitor.py</code> <pre><code>def stop(self) -&gt; None:\n    \"\"\"Stop monitoring pipeline execution.\n\n    Records the end time for pipeline execution timing. Should be called\n    after pipeline processing completes (in success or failure cases).\n\n    Note:\n    ----\n        Should typically be called in a finally block to ensure timing\n        is recorded even if processing fails.\n\n    \"\"\"\n    self._end_time = datetime.now()\n</code></pre>"},{"location":"api/monitoring/#metricvalue","title":"MetricValue","text":""},{"location":"api/monitoring/#rivusio.monitoring.metrics.MetricValue","title":"rivusio.monitoring.metrics.MetricValue","text":"<p>             Bases: <code>BaseModel</code></p> <p>A single metric measurement with its timestamp.</p> <p>Represents an individual metric measurement taken at a specific point in time. Used as the basic building block for time-series metrics collection.</p>"},{"location":"api/monitoring/#rivusio.monitoring.metrics.MetricValue--attributes","title":"Attributes","text":"<pre><code>timestamp: When the measurement was taken\nvalue: The measured value\n</code></pre> Source code in <code>src/rivusio/monitoring/metrics.py</code> <pre><code>class MetricValue(BaseModel):\n    \"\"\"A single metric measurement with its timestamp.\n\n    Represents an individual metric measurement taken at a specific point in time.\n    Used as the basic building block for time-series metrics collection.\n\n    Attributes\n    ----------\n        timestamp: When the measurement was taken\n        value: The measured value\n\n    \"\"\"\n\n    model_config = ConfigDict(arbitrary_types_allowed=True)\n\n    timestamp: datetime\n    value: float\n</code></pre>"},{"location":"api/monitoring/#rivusio.monitoring.metrics.MetricValue.model_config","title":"model_config  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>model_config = ConfigDict(arbitrary_types_allowed=True)\n</code></pre>"},{"location":"api/monitoring/#rivusio.monitoring.metrics.MetricValue.timestamp","title":"timestamp  <code>instance-attribute</code>","text":"<pre><code>timestamp: datetime\n</code></pre>"},{"location":"api/monitoring/#rivusio.monitoring.metrics.MetricValue.value","title":"value  <code>instance-attribute</code>","text":"<pre><code>value: float\n</code></pre>"},{"location":"api/monitoring/#metricwindow","title":"MetricWindow","text":""},{"location":"api/monitoring/#rivusio.monitoring.metrics.MetricWindow","title":"rivusio.monitoring.metrics.MetricWindow","text":"<p>             Bases: <code>BaseModel</code></p> <p>A sliding time window of metric values.</p> <p>Maintains a collection of metric values within a specified time window, automatically discarding values that fall outside the window. Provides statistical aggregations like average, min, and max.</p> <pre><code>values: List of MetricValue objects within the current window\nwindow_size: Duration of the sliding window\n</code></pre>"},{"location":"api/monitoring/#rivusio.monitoring.metrics.MetricWindow--example","title":"Example:","text":"<pre><code>```python\nfrom datetime import timedelta\n\n# Create a 5-minute window for response times\nwindow = MetricWindow(\n    values=[],\n    window_size=timedelta(minutes=5)\n)\n\n# Add measurements\nwindow.add_value(0.123)  # 123ms response time\n\n# Get statistics\nprint(f\"Average: {window.average:.3f}\")  # Average: 0.123\nprint(f\"Peak: {window.max:.3f}\")  # Peak: 0.123\n```\n</code></pre> Source code in <code>src/rivusio/monitoring/metrics.py</code> <pre><code>class MetricWindow(BaseModel):\n    \"\"\"A sliding time window of metric values.\n\n    Maintains a collection of metric values within a specified time window,\n    automatically discarding values that fall outside the window. Provides\n    statistical aggregations like average, min, and max.\n\n    Attributes:\n    ----------\n        values: List of MetricValue objects within the current window\n        window_size: Duration of the sliding window\n\n    Example:\n    -------\n        ```python\n        from datetime import timedelta\n\n        # Create a 5-minute window for response times\n        window = MetricWindow(\n            values=[],\n            window_size=timedelta(minutes=5)\n        )\n\n        # Add measurements\n        window.add_value(0.123)  # 123ms response time\n\n        # Get statistics\n        print(f\"Average: {window.average:.3f}\")  # Average: 0.123\n        print(f\"Peak: {window.max:.3f}\")  # Peak: 0.123\n        ```\n\n    \"\"\"\n\n    model_config = ConfigDict(arbitrary_types_allowed=True)\n    values: list[MetricValue]\n    window_size: timedelta\n\n    def add_value(self, value: float) -&gt; None:\n        \"\"\"Add a new measurement to the window.\n\n        Records a new metric value with the current timestamp and removes\n        any values that have fallen outside the window.\n\n        Args:\n        ----\n            value: The metric value to record\n\n        \"\"\"\n        now = datetime.now()\n        cutoff = now - self.window_size\n        self.values = [v for v in self.values if v.timestamp &gt; cutoff]\n        self.values.append(MetricValue(timestamp=now, value=value))\n\n    @property\n    def average(self) -&gt; Optional[float]:\n        \"\"\"Calculate the mean value within the current window.\n\n        Returns\n        -------\n            The average value, or None if the window is empty\n\n        \"\"\"\n        if not self.values:\n            return None\n        return sum(v.value for v in self.values) / len(self.values)\n\n    @property\n    def min(self) -&gt; Optional[float]:\n        \"\"\"Find the minimum value within the current window.\n\n        Returns\n        -------\n            The minimum value, or None if the window is empty\n\n        \"\"\"\n        if not self.values:\n            return None\n        return min(v.value for v in self.values)\n\n    @property\n    def max(self) -&gt; Optional[float]:\n        \"\"\"Find the maximum value within the current window.\n\n        Returns\n        -------\n            The maximum value, or None if the window is empty\n\n        \"\"\"\n        if not self.values:\n            return None\n        return max(v.value for v in self.values)\n</code></pre>"},{"location":"api/monitoring/#rivusio.monitoring.metrics.MetricWindow.average","title":"average  <code>property</code>","text":"<pre><code>average: Optional[float]\n</code></pre> <p>Calculate the mean value within the current window.</p>"},{"location":"api/monitoring/#rivusio.monitoring.metrics.MetricWindow.average--returns","title":"Returns","text":"<pre><code>The average value, or None if the window is empty\n</code></pre>"},{"location":"api/monitoring/#rivusio.monitoring.metrics.MetricWindow.max","title":"max  <code>property</code>","text":"<pre><code>max: Optional[float]\n</code></pre> <p>Find the maximum value within the current window.</p>"},{"location":"api/monitoring/#rivusio.monitoring.metrics.MetricWindow.max--returns","title":"Returns","text":"<pre><code>The maximum value, or None if the window is empty\n</code></pre>"},{"location":"api/monitoring/#rivusio.monitoring.metrics.MetricWindow.min","title":"min  <code>property</code>","text":"<pre><code>min: Optional[float]\n</code></pre> <p>Find the minimum value within the current window.</p>"},{"location":"api/monitoring/#rivusio.monitoring.metrics.MetricWindow.min--returns","title":"Returns","text":"<pre><code>The minimum value, or None if the window is empty\n</code></pre>"},{"location":"api/monitoring/#rivusio.monitoring.metrics.MetricWindow.model_config","title":"model_config  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>model_config = ConfigDict(arbitrary_types_allowed=True)\n</code></pre>"},{"location":"api/monitoring/#rivusio.monitoring.metrics.MetricWindow.values","title":"values  <code>instance-attribute</code>","text":"<pre><code>values: list[MetricValue]\n</code></pre>"},{"location":"api/monitoring/#rivusio.monitoring.metrics.MetricWindow.window_size","title":"window_size  <code>instance-attribute</code>","text":"<pre><code>window_size: timedelta\n</code></pre>"},{"location":"api/monitoring/#rivusio.monitoring.metrics.MetricWindow.add_value","title":"add_value","text":"<pre><code>add_value(value: float) -&gt; None\n</code></pre> <p>Add a new measurement to the window.</p> <p>Records a new metric value with the current timestamp and removes any values that have fallen outside the window.</p> <pre><code>value: The metric value to record\n</code></pre> Source code in <code>src/rivusio/monitoring/metrics.py</code> <pre><code>def add_value(self, value: float) -&gt; None:\n    \"\"\"Add a new measurement to the window.\n\n    Records a new metric value with the current timestamp and removes\n    any values that have fallen outside the window.\n\n    Args:\n    ----\n        value: The metric value to record\n\n    \"\"\"\n    now = datetime.now()\n    cutoff = now - self.window_size\n    self.values = [v for v in self.values if v.timestamp &gt; cutoff]\n    self.values.append(MetricValue(timestamp=now, value=value))\n</code></pre>"},{"location":"api/plugins/","title":"Plugins API Reference","text":""},{"location":"api/plugins/#pluginregistry","title":"PluginRegistry","text":""},{"location":"api/plugins/#rivusio.plugins.plugins.PluginRegistry","title":"rivusio.plugins.plugins.PluginRegistry","text":"<p>Registry for rivusio plugins.</p> Source code in <code>src/rivusio/plugins/plugins.py</code> <pre><code>class PluginRegistry:\n    \"\"\"Registry for rivusio plugins.\"\"\"\n\n    _instance: Optional[\"PluginRegistry\"] = None\n    _initialized: bool\n    _async_sources: dict[str, type[AsyncBasePipe]]\n    _async_sinks: dict[str, type[AsyncBasePipe]]\n    _sync_sources: dict[str, type[SyncBasePipe]]\n    _sync_sinks: dict[str, type[SyncBasePipe]]\n\n    def __new__(cls) -&gt; \"PluginRegistry\":\n        \"\"\"Create or return singleton instance.\"\"\"\n        if cls._instance is None:\n            cls._instance = super().__new__(cls)\n            cls._instance._initialized = False\n        return cls._instance\n\n    def __init__(self) -&gt; None:\n        \"\"\"Initialize plugin registry.\"\"\"\n        if not hasattr(self, \"_initialized\") or not self._initialized:\n            self._async_sources = {}\n            self._async_sinks = {}\n            self._sync_sources = {}\n            self._sync_sinks = {}\n            self._initialized = True\n\n    def register_async_source(self, name: str) -&gt; Callable[[type[AsyncT]], type[AsyncT]]:\n        \"\"\"Register an async source pipe.\n\n        Args:\n        ----\n            name: Name of the source pipe\n\n        Returns:\n        -------\n            Decorator function\n\n        \"\"\"\n\n        def decorator(cls: type[AsyncT]) -&gt; type[AsyncT]:\n            if not issubclass(cls, AsyncBasePipe):\n                raise TypeError(f\"{cls.__name__} must inherit from AsyncBasePipe\")\n            if name in self._async_sources:\n                raise ValueError(f\"Async source '{name}' is already registered\")\n            self._async_sources[name] = cls\n            return cls\n\n        return decorator\n\n    def register_async_sink(self, name: str) -&gt; Callable[[type[AsyncT]], type[AsyncT]]:\n        \"\"\"Register an async sink pipe.\n\n        Args:\n        ----\n            name: Name of the sink pipe\n\n        Returns:\n        -------\n            Decorator function\n\n        \"\"\"\n\n        def decorator(cls: type[AsyncT]) -&gt; type[AsyncT]:\n            if not issubclass(cls, AsyncBasePipe):\n                raise TypeError(f\"{cls.__name__} must inherit from AsyncBasePipe\")\n            if name in self._async_sinks:\n                raise ValueError(f\"Async sink '{name}' is already registered\")\n            self._async_sinks[name] = cls\n            return cls\n\n        return decorator\n\n    def register_sync_source(self, name: str) -&gt; Callable[[type[SyncT]], type[SyncT]]:\n        \"\"\"Register a sync source pipe.\n\n        Args:\n        ----\n            name: Name of the source pipe\n\n        Returns:\n        -------\n            Decorator function\n\n        \"\"\"\n\n        def decorator(cls: type[SyncT]) -&gt; type[SyncT]:\n            if not issubclass(cls, SyncBasePipe):\n                raise TypeError(f\"{cls.__name__} must inherit from SyncBasePipe\")\n            if name in self._sync_sources:\n                raise ValueError(f\"Sync source '{name}' is already registered\")\n            self._sync_sources[name] = cls\n            return cls\n\n        return decorator\n\n    def register_sync_sink(self, name: str) -&gt; Callable[[type[SyncT]], type[SyncT]]:\n        \"\"\"Register a sync sink pipe.\n\n        Args:\n        ----\n            name: Name of the sink pipe\n\n        Returns:\n        -------\n            Decorator function\n\n        \"\"\"\n\n        def decorator(cls: type[SyncT]) -&gt; type[SyncT]:\n            if not issubclass(cls, SyncBasePipe):\n                raise TypeError(f\"{cls.__name__} must inherit from SyncBasePipe\")\n            if name in self._sync_sinks:\n                raise ValueError(f\"Sync sink '{name}' is already registered\")\n            self._sync_sinks[name] = cls\n            return cls\n\n        return decorator\n\n    def get_async_source(self, name: str) -&gt; type[AsyncBasePipe]:\n        \"\"\"Get an async source pipe by name.\n\n        Args:\n        ----\n            name: Name of the source pipe\n\n        Returns:\n        -------\n            Source pipe class\n\n        Raises:\n        ------\n            KeyError: If source pipe not found\n\n        \"\"\"\n        try:\n            return self._async_sources[name]\n        except KeyError as e:\n            raise KeyError(f\"Async source '{name}' not found\") from e\n\n    def get_async_sink(self, name: str) -&gt; type[AsyncBasePipe]:\n        \"\"\"Get an async sink pipe by name.\n\n        Args:\n        ----\n            name: Name of the sink pipe\n\n        Returns:\n        -------\n            Sink pipe class\n\n        Raises:\n        ------\n            KeyError: If sink pipe not found\n\n        \"\"\"\n        try:\n            return self._async_sinks[name]\n        except KeyError as e:\n            raise KeyError(f\"Async sink '{name}' not found\") from e\n\n    def get_sync_source(self, name: str) -&gt; type[SyncBasePipe]:\n        \"\"\"Get a sync source pipe by name.\n\n        Args:\n        ----\n            name: Name of the source pipe\n\n        Returns:\n        -------\n            Source pipe class\n\n        Raises:\n        ------\n            KeyError: If source pipe not found\n\n        \"\"\"\n        try:\n            return self._sync_sources[name]\n        except KeyError as e:\n            raise KeyError(f\"Sync source '{name}' not found\") from e\n\n    def get_sync_sink(self, name: str) -&gt; type[SyncBasePipe]:\n        \"\"\"Get a sync sink pipe by name.\n\n        Args:\n        ----\n            name: Name of the sink pipe\n\n        Returns:\n        -------\n            Sink pipe class\n\n        Raises:\n        ------\n            KeyError: If sink pipe not found\n\n        \"\"\"\n        try:\n            return self._sync_sinks[name]\n        except KeyError as e:\n            raise KeyError(f\"Sync sink '{name}' not found\") from e\n\n    def list_async_sources(self) -&gt; dict[str, type[AsyncBasePipe]]:\n        \"\"\"List all registered async source pipes.\"\"\"\n        return self._async_sources.copy()\n\n    def list_async_sinks(self) -&gt; dict[str, type[AsyncBasePipe]]:\n        \"\"\"List all registered async sink pipes.\"\"\"\n        return self._async_sinks.copy()\n\n    def list_sync_sources(self) -&gt; dict[str, type[SyncBasePipe]]:\n        \"\"\"List all registered sync source pipes.\"\"\"\n        return self._sync_sources.copy()\n\n    def list_sync_sinks(self) -&gt; dict[str, type[SyncBasePipe]]:\n        \"\"\"List all registered sync sink pipes.\"\"\"\n        return self._sync_sinks.copy()\n\n    def get_registered_plugins(self) -&gt; dict[str, dict[str, type[AnyPipe]]]:\n        \"\"\"Get all registered plugins.\n\n        Returns\n        -------\n            Dictionary containing all registered plugins grouped by type:\n            {\n                \"async_sources\": {...},\n                \"async_sinks\": {...},\n                \"sync_sources\": {...},\n                \"sync_sinks\": {...}\n            }\n\n        \"\"\"\n        return {\n            \"async_sources\": self._async_sources.copy(),  # type: ignore\n            \"async_sinks\": self._async_sinks.copy(),  # type: ignore\n            \"sync_sources\": self._sync_sources.copy(),  # type: ignore\n            \"sync_sinks\": self._sync_sinks.copy(),  # type: ignore\n        }\n</code></pre>"},{"location":"api/plugins/#rivusio.plugins.plugins.PluginRegistry.__init__","title":"__init__","text":"<pre><code>__init__() -&gt; None\n</code></pre> <p>Initialize plugin registry.</p> Source code in <code>src/rivusio/plugins/plugins.py</code> <pre><code>def __init__(self) -&gt; None:\n    \"\"\"Initialize plugin registry.\"\"\"\n    if not hasattr(self, \"_initialized\") or not self._initialized:\n        self._async_sources = {}\n        self._async_sinks = {}\n        self._sync_sources = {}\n        self._sync_sinks = {}\n        self._initialized = True\n</code></pre>"},{"location":"api/plugins/#rivusio.plugins.plugins.PluginRegistry.__new__","title":"__new__","text":"<pre><code>__new__() -&gt; PluginRegistry\n</code></pre> <p>Create or return singleton instance.</p> Source code in <code>src/rivusio/plugins/plugins.py</code> <pre><code>def __new__(cls) -&gt; \"PluginRegistry\":\n    \"\"\"Create or return singleton instance.\"\"\"\n    if cls._instance is None:\n        cls._instance = super().__new__(cls)\n        cls._instance._initialized = False\n    return cls._instance\n</code></pre>"},{"location":"api/plugins/#rivusio.plugins.plugins.PluginRegistry.get_async_sink","title":"get_async_sink","text":"<pre><code>get_async_sink(name: str) -&gt; type[AsyncBasePipe]\n</code></pre> <p>Get an async sink pipe by name.</p> <pre><code>name: Name of the sink pipe\n</code></pre> <pre><code>Sink pipe class\n</code></pre> <pre><code>KeyError: If sink pipe not found\n</code></pre> Source code in <code>src/rivusio/plugins/plugins.py</code> <pre><code>def get_async_sink(self, name: str) -&gt; type[AsyncBasePipe]:\n    \"\"\"Get an async sink pipe by name.\n\n    Args:\n    ----\n        name: Name of the sink pipe\n\n    Returns:\n    -------\n        Sink pipe class\n\n    Raises:\n    ------\n        KeyError: If sink pipe not found\n\n    \"\"\"\n    try:\n        return self._async_sinks[name]\n    except KeyError as e:\n        raise KeyError(f\"Async sink '{name}' not found\") from e\n</code></pre>"},{"location":"api/plugins/#rivusio.plugins.plugins.PluginRegistry.get_async_source","title":"get_async_source","text":"<pre><code>get_async_source(name: str) -&gt; type[AsyncBasePipe]\n</code></pre> <p>Get an async source pipe by name.</p> <pre><code>name: Name of the source pipe\n</code></pre> <pre><code>Source pipe class\n</code></pre> <pre><code>KeyError: If source pipe not found\n</code></pre> Source code in <code>src/rivusio/plugins/plugins.py</code> <pre><code>def get_async_source(self, name: str) -&gt; type[AsyncBasePipe]:\n    \"\"\"Get an async source pipe by name.\n\n    Args:\n    ----\n        name: Name of the source pipe\n\n    Returns:\n    -------\n        Source pipe class\n\n    Raises:\n    ------\n        KeyError: If source pipe not found\n\n    \"\"\"\n    try:\n        return self._async_sources[name]\n    except KeyError as e:\n        raise KeyError(f\"Async source '{name}' not found\") from e\n</code></pre>"},{"location":"api/plugins/#rivusio.plugins.plugins.PluginRegistry.get_registered_plugins","title":"get_registered_plugins","text":"<pre><code>get_registered_plugins() -&gt; dict[str, dict[str, type[AnyPipe]]]\n</code></pre> <p>Get all registered plugins.</p>"},{"location":"api/plugins/#rivusio.plugins.plugins.PluginRegistry.get_registered_plugins--returns","title":"Returns","text":"<pre><code>Dictionary containing all registered plugins grouped by type:\n{\n    \"async_sources\": {...},\n    \"async_sinks\": {...},\n    \"sync_sources\": {...},\n    \"sync_sinks\": {...}\n}\n</code></pre> Source code in <code>src/rivusio/plugins/plugins.py</code> <pre><code>def get_registered_plugins(self) -&gt; dict[str, dict[str, type[AnyPipe]]]:\n    \"\"\"Get all registered plugins.\n\n    Returns\n    -------\n        Dictionary containing all registered plugins grouped by type:\n        {\n            \"async_sources\": {...},\n            \"async_sinks\": {...},\n            \"sync_sources\": {...},\n            \"sync_sinks\": {...}\n        }\n\n    \"\"\"\n    return {\n        \"async_sources\": self._async_sources.copy(),  # type: ignore\n        \"async_sinks\": self._async_sinks.copy(),  # type: ignore\n        \"sync_sources\": self._sync_sources.copy(),  # type: ignore\n        \"sync_sinks\": self._sync_sinks.copy(),  # type: ignore\n    }\n</code></pre>"},{"location":"api/plugins/#rivusio.plugins.plugins.PluginRegistry.get_sync_sink","title":"get_sync_sink","text":"<pre><code>get_sync_sink(name: str) -&gt; type[SyncBasePipe]\n</code></pre> <p>Get a sync sink pipe by name.</p> <pre><code>name: Name of the sink pipe\n</code></pre> <pre><code>Sink pipe class\n</code></pre> <pre><code>KeyError: If sink pipe not found\n</code></pre> Source code in <code>src/rivusio/plugins/plugins.py</code> <pre><code>def get_sync_sink(self, name: str) -&gt; type[SyncBasePipe]:\n    \"\"\"Get a sync sink pipe by name.\n\n    Args:\n    ----\n        name: Name of the sink pipe\n\n    Returns:\n    -------\n        Sink pipe class\n\n    Raises:\n    ------\n        KeyError: If sink pipe not found\n\n    \"\"\"\n    try:\n        return self._sync_sinks[name]\n    except KeyError as e:\n        raise KeyError(f\"Sync sink '{name}' not found\") from e\n</code></pre>"},{"location":"api/plugins/#rivusio.plugins.plugins.PluginRegistry.get_sync_source","title":"get_sync_source","text":"<pre><code>get_sync_source(name: str) -&gt; type[SyncBasePipe]\n</code></pre> <p>Get a sync source pipe by name.</p> <pre><code>name: Name of the source pipe\n</code></pre> <pre><code>Source pipe class\n</code></pre> <pre><code>KeyError: If source pipe not found\n</code></pre> Source code in <code>src/rivusio/plugins/plugins.py</code> <pre><code>def get_sync_source(self, name: str) -&gt; type[SyncBasePipe]:\n    \"\"\"Get a sync source pipe by name.\n\n    Args:\n    ----\n        name: Name of the source pipe\n\n    Returns:\n    -------\n        Source pipe class\n\n    Raises:\n    ------\n        KeyError: If source pipe not found\n\n    \"\"\"\n    try:\n        return self._sync_sources[name]\n    except KeyError as e:\n        raise KeyError(f\"Sync source '{name}' not found\") from e\n</code></pre>"},{"location":"api/plugins/#rivusio.plugins.plugins.PluginRegistry.list_async_sinks","title":"list_async_sinks","text":"<pre><code>list_async_sinks() -&gt; dict[str, type[AsyncBasePipe]]\n</code></pre> <p>List all registered async sink pipes.</p> Source code in <code>src/rivusio/plugins/plugins.py</code> <pre><code>def list_async_sinks(self) -&gt; dict[str, type[AsyncBasePipe]]:\n    \"\"\"List all registered async sink pipes.\"\"\"\n    return self._async_sinks.copy()\n</code></pre>"},{"location":"api/plugins/#rivusio.plugins.plugins.PluginRegistry.list_async_sources","title":"list_async_sources","text":"<pre><code>list_async_sources() -&gt; dict[str, type[AsyncBasePipe]]\n</code></pre> <p>List all registered async source pipes.</p> Source code in <code>src/rivusio/plugins/plugins.py</code> <pre><code>def list_async_sources(self) -&gt; dict[str, type[AsyncBasePipe]]:\n    \"\"\"List all registered async source pipes.\"\"\"\n    return self._async_sources.copy()\n</code></pre>"},{"location":"api/plugins/#rivusio.plugins.plugins.PluginRegistry.list_sync_sinks","title":"list_sync_sinks","text":"<pre><code>list_sync_sinks() -&gt; dict[str, type[SyncBasePipe]]\n</code></pre> <p>List all registered sync sink pipes.</p> Source code in <code>src/rivusio/plugins/plugins.py</code> <pre><code>def list_sync_sinks(self) -&gt; dict[str, type[SyncBasePipe]]:\n    \"\"\"List all registered sync sink pipes.\"\"\"\n    return self._sync_sinks.copy()\n</code></pre>"},{"location":"api/plugins/#rivusio.plugins.plugins.PluginRegistry.list_sync_sources","title":"list_sync_sources","text":"<pre><code>list_sync_sources() -&gt; dict[str, type[SyncBasePipe]]\n</code></pre> <p>List all registered sync source pipes.</p> Source code in <code>src/rivusio/plugins/plugins.py</code> <pre><code>def list_sync_sources(self) -&gt; dict[str, type[SyncBasePipe]]:\n    \"\"\"List all registered sync source pipes.\"\"\"\n    return self._sync_sources.copy()\n</code></pre>"},{"location":"api/plugins/#rivusio.plugins.plugins.PluginRegistry.register_async_sink","title":"register_async_sink","text":"<pre><code>register_async_sink(name: str) -&gt; Callable[[type[AsyncT]], type[AsyncT]]\n</code></pre> <p>Register an async sink pipe.</p> <pre><code>name: Name of the sink pipe\n</code></pre> <pre><code>Decorator function\n</code></pre> Source code in <code>src/rivusio/plugins/plugins.py</code> <pre><code>def register_async_sink(self, name: str) -&gt; Callable[[type[AsyncT]], type[AsyncT]]:\n    \"\"\"Register an async sink pipe.\n\n    Args:\n    ----\n        name: Name of the sink pipe\n\n    Returns:\n    -------\n        Decorator function\n\n    \"\"\"\n\n    def decorator(cls: type[AsyncT]) -&gt; type[AsyncT]:\n        if not issubclass(cls, AsyncBasePipe):\n            raise TypeError(f\"{cls.__name__} must inherit from AsyncBasePipe\")\n        if name in self._async_sinks:\n            raise ValueError(f\"Async sink '{name}' is already registered\")\n        self._async_sinks[name] = cls\n        return cls\n\n    return decorator\n</code></pre>"},{"location":"api/plugins/#rivusio.plugins.plugins.PluginRegistry.register_async_source","title":"register_async_source","text":"<pre><code>register_async_source(name: str) -&gt; Callable[[type[AsyncT]], type[AsyncT]]\n</code></pre> <p>Register an async source pipe.</p> <pre><code>name: Name of the source pipe\n</code></pre> <pre><code>Decorator function\n</code></pre> Source code in <code>src/rivusio/plugins/plugins.py</code> <pre><code>def register_async_source(self, name: str) -&gt; Callable[[type[AsyncT]], type[AsyncT]]:\n    \"\"\"Register an async source pipe.\n\n    Args:\n    ----\n        name: Name of the source pipe\n\n    Returns:\n    -------\n        Decorator function\n\n    \"\"\"\n\n    def decorator(cls: type[AsyncT]) -&gt; type[AsyncT]:\n        if not issubclass(cls, AsyncBasePipe):\n            raise TypeError(f\"{cls.__name__} must inherit from AsyncBasePipe\")\n        if name in self._async_sources:\n            raise ValueError(f\"Async source '{name}' is already registered\")\n        self._async_sources[name] = cls\n        return cls\n\n    return decorator\n</code></pre>"},{"location":"api/plugins/#rivusio.plugins.plugins.PluginRegistry.register_sync_sink","title":"register_sync_sink","text":"<pre><code>register_sync_sink(name: str) -&gt; Callable[[type[SyncT]], type[SyncT]]\n</code></pre> <p>Register a sync sink pipe.</p> <pre><code>name: Name of the sink pipe\n</code></pre> <pre><code>Decorator function\n</code></pre> Source code in <code>src/rivusio/plugins/plugins.py</code> <pre><code>def register_sync_sink(self, name: str) -&gt; Callable[[type[SyncT]], type[SyncT]]:\n    \"\"\"Register a sync sink pipe.\n\n    Args:\n    ----\n        name: Name of the sink pipe\n\n    Returns:\n    -------\n        Decorator function\n\n    \"\"\"\n\n    def decorator(cls: type[SyncT]) -&gt; type[SyncT]:\n        if not issubclass(cls, SyncBasePipe):\n            raise TypeError(f\"{cls.__name__} must inherit from SyncBasePipe\")\n        if name in self._sync_sinks:\n            raise ValueError(f\"Sync sink '{name}' is already registered\")\n        self._sync_sinks[name] = cls\n        return cls\n\n    return decorator\n</code></pre>"},{"location":"api/plugins/#rivusio.plugins.plugins.PluginRegistry.register_sync_source","title":"register_sync_source","text":"<pre><code>register_sync_source(name: str) -&gt; Callable[[type[SyncT]], type[SyncT]]\n</code></pre> <p>Register a sync source pipe.</p> <pre><code>name: Name of the source pipe\n</code></pre> <pre><code>Decorator function\n</code></pre> Source code in <code>src/rivusio/plugins/plugins.py</code> <pre><code>def register_sync_source(self, name: str) -&gt; Callable[[type[SyncT]], type[SyncT]]:\n    \"\"\"Register a sync source pipe.\n\n    Args:\n    ----\n        name: Name of the source pipe\n\n    Returns:\n    -------\n        Decorator function\n\n    \"\"\"\n\n    def decorator(cls: type[SyncT]) -&gt; type[SyncT]:\n        if not issubclass(cls, SyncBasePipe):\n            raise TypeError(f\"{cls.__name__} must inherit from SyncBasePipe\")\n        if name in self._sync_sources:\n            raise ValueError(f\"Sync source '{name}' is already registered\")\n        self._sync_sources[name] = cls\n        return cls\n\n    return decorator\n</code></pre>"},{"location":"api/plugins/#plugin-decorators","title":"Plugin Decorators","text":""},{"location":"api/plugins/#register_async_source","title":"register_async_source","text":""},{"location":"api/plugins/#rivusio.plugins.plugins.PluginRegistry.register_async_source","title":"rivusio.plugins.plugins.PluginRegistry.register_async_source","text":"<pre><code>register_async_source(name: str) -&gt; Callable[[type[AsyncT]], type[AsyncT]]\n</code></pre> <p>Register an async source pipe.</p> <pre><code>name: Name of the source pipe\n</code></pre> <pre><code>Decorator function\n</code></pre> Source code in <code>src/rivusio/plugins/plugins.py</code> <pre><code>def register_async_source(self, name: str) -&gt; Callable[[type[AsyncT]], type[AsyncT]]:\n    \"\"\"Register an async source pipe.\n\n    Args:\n    ----\n        name: Name of the source pipe\n\n    Returns:\n    -------\n        Decorator function\n\n    \"\"\"\n\n    def decorator(cls: type[AsyncT]) -&gt; type[AsyncT]:\n        if not issubclass(cls, AsyncBasePipe):\n            raise TypeError(f\"{cls.__name__} must inherit from AsyncBasePipe\")\n        if name in self._async_sources:\n            raise ValueError(f\"Async source '{name}' is already registered\")\n        self._async_sources[name] = cls\n        return cls\n\n    return decorator\n</code></pre>"},{"location":"api/plugins/#register_async_sink","title":"register_async_sink","text":""},{"location":"api/plugins/#rivusio.plugins.plugins.PluginRegistry.register_async_sink","title":"rivusio.plugins.plugins.PluginRegistry.register_async_sink","text":"<pre><code>register_async_sink(name: str) -&gt; Callable[[type[AsyncT]], type[AsyncT]]\n</code></pre> <p>Register an async sink pipe.</p> <pre><code>name: Name of the sink pipe\n</code></pre> <pre><code>Decorator function\n</code></pre> Source code in <code>src/rivusio/plugins/plugins.py</code> <pre><code>def register_async_sink(self, name: str) -&gt; Callable[[type[AsyncT]], type[AsyncT]]:\n    \"\"\"Register an async sink pipe.\n\n    Args:\n    ----\n        name: Name of the sink pipe\n\n    Returns:\n    -------\n        Decorator function\n\n    \"\"\"\n\n    def decorator(cls: type[AsyncT]) -&gt; type[AsyncT]:\n        if not issubclass(cls, AsyncBasePipe):\n            raise TypeError(f\"{cls.__name__} must inherit from AsyncBasePipe\")\n        if name in self._async_sinks:\n            raise ValueError(f\"Async sink '{name}' is already registered\")\n        self._async_sinks[name] = cls\n        return cls\n\n    return decorator\n</code></pre>"},{"location":"api/plugins/#register_sync_source","title":"register_sync_source","text":""},{"location":"api/plugins/#rivusio.plugins.plugins.PluginRegistry.register_sync_source","title":"rivusio.plugins.plugins.PluginRegistry.register_sync_source","text":"<pre><code>register_sync_source(name: str) -&gt; Callable[[type[SyncT]], type[SyncT]]\n</code></pre> <p>Register a sync source pipe.</p> <pre><code>name: Name of the source pipe\n</code></pre> <pre><code>Decorator function\n</code></pre> Source code in <code>src/rivusio/plugins/plugins.py</code> <pre><code>def register_sync_source(self, name: str) -&gt; Callable[[type[SyncT]], type[SyncT]]:\n    \"\"\"Register a sync source pipe.\n\n    Args:\n    ----\n        name: Name of the source pipe\n\n    Returns:\n    -------\n        Decorator function\n\n    \"\"\"\n\n    def decorator(cls: type[SyncT]) -&gt; type[SyncT]:\n        if not issubclass(cls, SyncBasePipe):\n            raise TypeError(f\"{cls.__name__} must inherit from SyncBasePipe\")\n        if name in self._sync_sources:\n            raise ValueError(f\"Sync source '{name}' is already registered\")\n        self._sync_sources[name] = cls\n        return cls\n\n    return decorator\n</code></pre>"},{"location":"api/plugins/#register_sync_sink","title":"register_sync_sink","text":""},{"location":"api/plugins/#rivusio.plugins.plugins.PluginRegistry.register_sync_sink","title":"rivusio.plugins.plugins.PluginRegistry.register_sync_sink","text":"<pre><code>register_sync_sink(name: str) -&gt; Callable[[type[SyncT]], type[SyncT]]\n</code></pre> <p>Register a sync sink pipe.</p> <pre><code>name: Name of the sink pipe\n</code></pre> <pre><code>Decorator function\n</code></pre> Source code in <code>src/rivusio/plugins/plugins.py</code> <pre><code>def register_sync_sink(self, name: str) -&gt; Callable[[type[SyncT]], type[SyncT]]:\n    \"\"\"Register a sync sink pipe.\n\n    Args:\n    ----\n        name: Name of the sink pipe\n\n    Returns:\n    -------\n        Decorator function\n\n    \"\"\"\n\n    def decorator(cls: type[SyncT]) -&gt; type[SyncT]:\n        if not issubclass(cls, SyncBasePipe):\n            raise TypeError(f\"{cls.__name__} must inherit from SyncBasePipe\")\n        if name in self._sync_sinks:\n            raise ValueError(f\"Sync sink '{name}' is already registered\")\n        self._sync_sinks[name] = cls\n        return cls\n\n    return decorator\n</code></pre>"},{"location":"api/plugins/#global-registry","title":"Global Registry","text":"<p>The <code>registry</code> object is a global singleton instance of <code>PluginRegistry</code> that should be used for all plugin registration:</p> <pre><code>from rivusio.plugins import registry\n</code></pre>"},{"location":"api/streams/","title":"Streams API Reference","text":""},{"location":"api/streams/#asyncstream","title":"AsyncStream","text":""},{"location":"api/streams/#rivusio.streams.stream.AsyncStream","title":"rivusio.streams.stream.AsyncStream","text":"<p>             Bases: <code>Generic[T]</code></p> <p>An asynchronous stream of data that can be processed through a pipeline.</p> <p>Provides functionality for processing data streams with support for batching, windowing, and error handling. Designed for async operations.</p> <pre><code>source: The async iterator providing the data\nconfig: Stream processing configuration\nmetrics: Optional metrics collector for monitoring\n</code></pre>"},{"location":"api/streams/#rivusio.streams.stream.AsyncStream--example","title":"Example:","text":"<pre><code>```python\nasync def data_source():\n    for i in range(100):\n        yield {\"value\": i}\n\nconfig = StreamConfig(batch_size=10)\nstream = AsyncStream(data_source(), config=config)\n\nasync def process_batch(items):\n    return [item[\"value\"] * 2 for item in items]\n\nasync for result in stream.process(process_batch):\n    print(f\"Processed batch: {result}\")\n```\n</code></pre> Source code in <code>src/rivusio/streams/stream.py</code> <pre><code>class AsyncStream(Generic[T]):\n    \"\"\"An asynchronous stream of data that can be processed through a pipeline.\n\n    Provides functionality for processing data streams with support for batching,\n    windowing, and error handling. Designed for async operations.\n\n    Attributes:\n    ----------\n        source: The async iterator providing the data\n        config: Stream processing configuration\n        metrics: Optional metrics collector for monitoring\n\n    Example:\n    -------\n        ```python\n        async def data_source():\n            for i in range(100):\n                yield {\"value\": i}\n\n        config = StreamConfig(batch_size=10)\n        stream = AsyncStream(data_source(), config=config)\n\n        async def process_batch(items):\n            return [item[\"value\"] * 2 for item in items]\n\n        async for result in stream.process(process_batch):\n            print(f\"Processed batch: {result}\")\n        ```\n\n    \"\"\"\n\n    def __init__(\n        self,\n        source: AsyncIterator[T],\n        config: Optional[StreamConfig] = None,\n        metrics: Optional[MetricsCollector] = None,\n    ) -&gt; None:\n        \"\"\"Initialize async stream.\n\n        Args:\n        ----\n            source: Source iterator providing data\n            config: Stream processing configuration\n            metrics: Optional metrics collector for monitoring\n\n        \"\"\"\n        self.source = source\n        self.config = config or StreamConfig()\n        self.metrics = metrics or MetricsCollector()\n        self._buffer: list[T] = []\n        self._window_buffer: list[T] = []\n        self._last_window_time = time.time()\n\n    @property\n    def buffer(self) -&gt; list[T]:\n        \"\"\"Get buffer.\"\"\"\n        return self._buffer\n\n    def metrics_dict(self) -&gt; dict:\n        \"\"\"Get current metrics including buffer size.\"\"\"\n        return {\n            \"buffer_size\": len(self._buffer),\n            \"metrics\": self.metrics.get_metrics() if self.metrics else {},\n        }\n\n    async def sliding_window(self, window_size: int, step_size: int) -&gt; AsyncIterator[list[T]]:\n        \"\"\"Process items using sliding window.\n\n        Args:\n        ----\n            window_size: Size of the sliding window\n            step_size: Number of items to slide by\n\n        Yields:\n        ------\n            List of items in the current window\n\n        \"\"\"\n        window: list[T] = []\n        async for item in self.source:\n            window.append(item)\n            if len(window) &gt;= window_size:\n                yield window[-window_size:]\n                window = window[step_size:]\n\n    async def tumbling_window(self, window_size: int) -&gt; AsyncIterator[list[T]]:\n        \"\"\"Process items using tumbling window.\n\n        Args:\n        ----\n            window_size: Size of the tumbling window\n\n        Yields:\n        ------\n            List of items in the current window\n\n        \"\"\"\n        window: list[T] = []\n        async for item in self.source:\n            window.append(item)\n            if len(window) &gt;= window_size:\n                yield window\n                window = []\n        if window:\n            yield window\n\n    async def time_window(self, duration: timedelta) -&gt; AsyncIterator[list[T]]:\n        \"\"\"Process items using time-based window.\n\n        Args:\n        ----\n            duration: Time duration for the window\n\n        Yields:\n        ------\n            List of items collected within the time window\n\n        \"\"\"\n        window: list[T] = []\n        start_time = time.time()\n        async for item in self.source:\n            current_time = time.time()\n            if current_time - start_time &gt;= duration.total_seconds():\n                if window:\n                    yield window\n                    window = []\n                start_time = current_time\n            window.append(item)\n        if window:\n            yield window\n\n    async def process(\n        self, pipe: AsyncBasePipe[InputType, OutputType | BatchT],\n    ) -&gt; AsyncIterator[Any]:\n        \"\"\"Process the stream through the provided pipeline.\n\n        Automatically selects the appropriate processing mode based on configuration:\n        - Single item mode if batch_size=1 and window_size=0\n        - Batch mode if batch_size &gt; 1\n        - Window mode if window_size &gt; 0\n\n        Args:\n        ----\n            pipe: Pipeline to process items/batches/windows\n\n        Yields:\n        ------\n            Processed items/batches/windows depending on processing mode\n\n        Raises:\n        ------\n            PipeError: If processing fails and retries are exhausted\n\n        Example:\n        -------\n            ```python\n            from rivusio.streams import AsyncStream, StreamConfig\n            from rivusio.core import AsyncBasePipe\n\n            async def data_source():\n                for i in range(5):\n                    yield {\"value\": i}\n\n            class MultiplyPipe(AsyncBasePipe[Dict, Dict]):\n                async def process(self, data: Dict) -&gt; Dict:\n                    return {\"value\": data[\"value\"] * 2}\n\n            config = StreamConfig(batch_size=2)\n            stream = AsyncStream(data_source(), config=config)\n            pipe = MultiplyPipe()\n\n            async for result in stream.process(pipe):\n                print(result)\n            ```\n\n        \"\"\"\n        if self.config.batch_size &gt; 1:\n            async for batch in self._process_batches(pipe):\n                yield batch\n        elif self.config.window_size.total_seconds() &gt; 0:\n            async for window in self._process_windows(pipe):\n                yield window\n        else:\n            async for item in self.source:\n                yield await pipe.process(item)\n\n    async def _process_item(\n        self, pipe: AsyncBasePipe[InputType, OutputType], item: T,\n    ) -&gt; Optional[OutputType]:\n        \"\"\"Process a single item with retry logic.\n\n        Args:\n        ----\n            pipe: Pipeline to process the item\n            item: Item to process\n\n        Returns:\n        -------\n            Processed item or None if processing failed\n\n        Raises:\n        ------\n            PipeError: If all retry attempts fail\n\n        \"\"\"\n        for attempt in range(self.config.retry_attempts):\n            try:\n                result = await pipe(item)\n                return result if not isinstance(result, list) else result[0]\n            except Exception as e:\n                if attempt == self.config.retry_attempts - 1:\n                    raise PipeError(pipe.__class__.__name__, e) from e\n                await asyncio.sleep(self.config.retry_delay * (attempt + 1))\n        return None\n\n    async def _process_batches(self, pipe: AsyncBasePipe[BatchT, BatchT]) -&gt; AsyncIterator[BatchT]:\n        \"\"\"Process items in batches.\n\n        Args:\n        ----\n            pipe: Pipeline to process batches\n\n        Yields:\n        ------\n            Processed batches of items\n\n        Raises:\n        ------\n            PipeError: If batch processing fails\n\n        \"\"\"\n        batch: list[T] = []\n        async for item in self.source:\n            batch.append(item)\n            if len(batch) &gt;= self.config.batch_size:\n                try:\n                    yield await pipe.process(cast(BatchT, batch))\n                    batch = []\n                except Exception as e:\n                    raise PipeError(pipe.__class__.__name__, e) from e\n\n        # Process remaining items if any\n        if batch:\n            try:\n                yield await pipe.process(cast(BatchT, batch))\n            except Exception as e:\n                raise PipeError(pipe.__class__.__name__, e) from e\n\n    async def _process_windows(self, pipe: AsyncBasePipe[BatchT, BatchT]) -&gt; AsyncIterator[BatchT]:\n        \"\"\"Process items in time-based windows.\n\n        Collects items into windows based on configured window_size.\n\n        Args:\n        ----\n            pipe: Pipeline to process windows\n\n        Yields:\n        ------\n            Processed windows of items\n\n        Raises:\n        ------\n            PipeError: If window processing fails\n\n        \"\"\"\n        async for item in self.source:\n            current_time = time.time()\n            if current_time - self._last_window_time &gt;= self.config.window_size.total_seconds():\n                if self._window_buffer:\n                    try:\n                        yield await pipe(cast(BatchT, self._window_buffer))\n                    except Exception as e:\n                        raise PipeError(pipe.__class__.__name__, e) from e\n                    self._window_buffer = []\n                self._last_window_time = current_time\n            self._window_buffer.append(item)\n\n        if self._window_buffer:\n            try:\n                yield await pipe(cast(BatchT, self._window_buffer))\n            except Exception as e:\n                raise PipeError(pipe.__class__.__name__, e) from e\n</code></pre>"},{"location":"api/streams/#rivusio.streams.stream.AsyncStream.buffer","title":"buffer  <code>property</code>","text":"<pre><code>buffer: list[T]\n</code></pre> <p>Get buffer.</p>"},{"location":"api/streams/#rivusio.streams.stream.AsyncStream.config","title":"config  <code>instance-attribute</code>","text":"<pre><code>config = config or StreamConfig()\n</code></pre>"},{"location":"api/streams/#rivusio.streams.stream.AsyncStream.metrics","title":"metrics  <code>instance-attribute</code>","text":"<pre><code>metrics = metrics or MetricsCollector()\n</code></pre>"},{"location":"api/streams/#rivusio.streams.stream.AsyncStream.source","title":"source  <code>instance-attribute</code>","text":"<pre><code>source = source\n</code></pre>"},{"location":"api/streams/#rivusio.streams.stream.AsyncStream.__init__","title":"__init__","text":"<pre><code>__init__(source: AsyncIterator[T], config: Optional[StreamConfig] = None, metrics: Optional[MetricsCollector] = None) -&gt; None\n</code></pre> <p>Initialize async stream.</p> <pre><code>source: Source iterator providing data\nconfig: Stream processing configuration\nmetrics: Optional metrics collector for monitoring\n</code></pre> Source code in <code>src/rivusio/streams/stream.py</code> <pre><code>def __init__(\n    self,\n    source: AsyncIterator[T],\n    config: Optional[StreamConfig] = None,\n    metrics: Optional[MetricsCollector] = None,\n) -&gt; None:\n    \"\"\"Initialize async stream.\n\n    Args:\n    ----\n        source: Source iterator providing data\n        config: Stream processing configuration\n        metrics: Optional metrics collector for monitoring\n\n    \"\"\"\n    self.source = source\n    self.config = config or StreamConfig()\n    self.metrics = metrics or MetricsCollector()\n    self._buffer: list[T] = []\n    self._window_buffer: list[T] = []\n    self._last_window_time = time.time()\n</code></pre>"},{"location":"api/streams/#rivusio.streams.stream.AsyncStream.metrics_dict","title":"metrics_dict","text":"<pre><code>metrics_dict() -&gt; dict\n</code></pre> <p>Get current metrics including buffer size.</p> Source code in <code>src/rivusio/streams/stream.py</code> <pre><code>def metrics_dict(self) -&gt; dict:\n    \"\"\"Get current metrics including buffer size.\"\"\"\n    return {\n        \"buffer_size\": len(self._buffer),\n        \"metrics\": self.metrics.get_metrics() if self.metrics else {},\n    }\n</code></pre>"},{"location":"api/streams/#rivusio.streams.stream.AsyncStream.process","title":"process  <code>async</code>","text":"<pre><code>process(pipe: AsyncBasePipe[InputType, OutputType | BatchT]) -&gt; AsyncIterator[Any]\n</code></pre> <p>Process the stream through the provided pipeline.</p> <p>Automatically selects the appropriate processing mode based on configuration: - Single item mode if batch_size=1 and window_size=0 - Batch mode if batch_size &gt; 1 - Window mode if window_size &gt; 0</p> <pre><code>pipe: Pipeline to process items/batches/windows\n</code></pre> <pre><code>Processed items/batches/windows depending on processing mode\n</code></pre> <pre><code>PipeError: If processing fails and retries are exhausted\n</code></pre>"},{"location":"api/streams/#rivusio.streams.stream.AsyncStream.process--example","title":"Example:","text":"<pre><code>```python\nfrom rivusio.streams import AsyncStream, StreamConfig\nfrom rivusio.core import AsyncBasePipe\n\nasync def data_source():\n    for i in range(5):\n        yield {\"value\": i}\n\nclass MultiplyPipe(AsyncBasePipe[Dict, Dict]):\n    async def process(self, data: Dict) -&gt; Dict:\n        return {\"value\": data[\"value\"] * 2}\n\nconfig = StreamConfig(batch_size=2)\nstream = AsyncStream(data_source(), config=config)\npipe = MultiplyPipe()\n\nasync for result in stream.process(pipe):\n    print(result)\n```\n</code></pre> Source code in <code>src/rivusio/streams/stream.py</code> <pre><code>async def process(\n    self, pipe: AsyncBasePipe[InputType, OutputType | BatchT],\n) -&gt; AsyncIterator[Any]:\n    \"\"\"Process the stream through the provided pipeline.\n\n    Automatically selects the appropriate processing mode based on configuration:\n    - Single item mode if batch_size=1 and window_size=0\n    - Batch mode if batch_size &gt; 1\n    - Window mode if window_size &gt; 0\n\n    Args:\n    ----\n        pipe: Pipeline to process items/batches/windows\n\n    Yields:\n    ------\n        Processed items/batches/windows depending on processing mode\n\n    Raises:\n    ------\n        PipeError: If processing fails and retries are exhausted\n\n    Example:\n    -------\n        ```python\n        from rivusio.streams import AsyncStream, StreamConfig\n        from rivusio.core import AsyncBasePipe\n\n        async def data_source():\n            for i in range(5):\n                yield {\"value\": i}\n\n        class MultiplyPipe(AsyncBasePipe[Dict, Dict]):\n            async def process(self, data: Dict) -&gt; Dict:\n                return {\"value\": data[\"value\"] * 2}\n\n        config = StreamConfig(batch_size=2)\n        stream = AsyncStream(data_source(), config=config)\n        pipe = MultiplyPipe()\n\n        async for result in stream.process(pipe):\n            print(result)\n        ```\n\n    \"\"\"\n    if self.config.batch_size &gt; 1:\n        async for batch in self._process_batches(pipe):\n            yield batch\n    elif self.config.window_size.total_seconds() &gt; 0:\n        async for window in self._process_windows(pipe):\n            yield window\n    else:\n        async for item in self.source:\n            yield await pipe.process(item)\n</code></pre>"},{"location":"api/streams/#rivusio.streams.stream.AsyncStream.sliding_window","title":"sliding_window  <code>async</code>","text":"<pre><code>sliding_window(window_size: int, step_size: int) -&gt; AsyncIterator[list[T]]\n</code></pre> <p>Process items using sliding window.</p> <pre><code>window_size: Size of the sliding window\nstep_size: Number of items to slide by\n</code></pre> <pre><code>List of items in the current window\n</code></pre> Source code in <code>src/rivusio/streams/stream.py</code> <pre><code>async def sliding_window(self, window_size: int, step_size: int) -&gt; AsyncIterator[list[T]]:\n    \"\"\"Process items using sliding window.\n\n    Args:\n    ----\n        window_size: Size of the sliding window\n        step_size: Number of items to slide by\n\n    Yields:\n    ------\n        List of items in the current window\n\n    \"\"\"\n    window: list[T] = []\n    async for item in self.source:\n        window.append(item)\n        if len(window) &gt;= window_size:\n            yield window[-window_size:]\n            window = window[step_size:]\n</code></pre>"},{"location":"api/streams/#rivusio.streams.stream.AsyncStream.time_window","title":"time_window  <code>async</code>","text":"<pre><code>time_window(duration: timedelta) -&gt; AsyncIterator[list[T]]\n</code></pre> <p>Process items using time-based window.</p> <pre><code>duration: Time duration for the window\n</code></pre> <pre><code>List of items collected within the time window\n</code></pre> Source code in <code>src/rivusio/streams/stream.py</code> <pre><code>async def time_window(self, duration: timedelta) -&gt; AsyncIterator[list[T]]:\n    \"\"\"Process items using time-based window.\n\n    Args:\n    ----\n        duration: Time duration for the window\n\n    Yields:\n    ------\n        List of items collected within the time window\n\n    \"\"\"\n    window: list[T] = []\n    start_time = time.time()\n    async for item in self.source:\n        current_time = time.time()\n        if current_time - start_time &gt;= duration.total_seconds():\n            if window:\n                yield window\n                window = []\n            start_time = current_time\n        window.append(item)\n    if window:\n        yield window\n</code></pre>"},{"location":"api/streams/#rivusio.streams.stream.AsyncStream.tumbling_window","title":"tumbling_window  <code>async</code>","text":"<pre><code>tumbling_window(window_size: int) -&gt; AsyncIterator[list[T]]\n</code></pre> <p>Process items using tumbling window.</p> <pre><code>window_size: Size of the tumbling window\n</code></pre> <pre><code>List of items in the current window\n</code></pre> Source code in <code>src/rivusio/streams/stream.py</code> <pre><code>async def tumbling_window(self, window_size: int) -&gt; AsyncIterator[list[T]]:\n    \"\"\"Process items using tumbling window.\n\n    Args:\n    ----\n        window_size: Size of the tumbling window\n\n    Yields:\n    ------\n        List of items in the current window\n\n    \"\"\"\n    window: list[T] = []\n    async for item in self.source:\n        window.append(item)\n        if len(window) &gt;= window_size:\n            yield window\n            window = []\n    if window:\n        yield window\n</code></pre>"},{"location":"api/streams/#syncstream","title":"SyncStream","text":""},{"location":"api/streams/#rivusio.streams.stream.SyncStream","title":"rivusio.streams.stream.SyncStream","text":"<p>             Bases: <code>Generic[T]</code></p> <p>A synchronous stream of data that can be processed through a pipeline.</p> <p>Provides the same functionality as AsyncStream but for synchronous operations. Supports batching, windowing, and error handling for sync data sources.</p> <pre><code>source: The iterator providing the data\nconfig: Stream processing configuration\nmetrics: Optional metrics collector for monitoring\n</code></pre>"},{"location":"api/streams/#rivusio.streams.stream.SyncStream--example","title":"Example:","text":"<pre><code>```python\ndef data_source():\n    for i in range(100):\n        yield {\"value\": i}\n\nconfig = StreamConfig(window_size=timedelta(seconds=30))\nstream = SyncStream(data_source(), config=config)\n\ndef process_window(items):\n    return [sum(item[\"value\"] for item in items)]\n\nfor result in stream.process(process_window):\n    print(f\"Window sum: {result[0]}\")\n```\n</code></pre> Source code in <code>src/rivusio/streams/stream.py</code> <pre><code>class SyncStream(Generic[T]):\n    \"\"\"A synchronous stream of data that can be processed through a pipeline.\n\n    Provides the same functionality as AsyncStream but for synchronous operations.\n    Supports batching, windowing, and error handling for sync data sources.\n\n    Attributes:\n    ----------\n        source: The iterator providing the data\n        config: Stream processing configuration\n        metrics: Optional metrics collector for monitoring\n\n    Example:\n    -------\n        ```python\n        def data_source():\n            for i in range(100):\n                yield {\"value\": i}\n\n        config = StreamConfig(window_size=timedelta(seconds=30))\n        stream = SyncStream(data_source(), config=config)\n\n        def process_window(items):\n            return [sum(item[\"value\"] for item in items)]\n\n        for result in stream.process(process_window):\n            print(f\"Window sum: {result[0]}\")\n        ```\n\n    \"\"\"\n\n    def __init__(\n        self,\n        source: Iterator[T],\n        config: Optional[StreamConfig] = None,\n        metrics: Optional[MetricsCollector] = None,\n    ) -&gt; None:\n        \"\"\"Initialize sync stream.\n\n        Args:\n        ----\n            source: Source iterator providing data\n            config: Stream processing configuration\n            metrics: Optional metrics collector for monitoring\n\n        \"\"\"\n        self.source = source\n        self.config = config or StreamConfig()\n        self.metrics = metrics or MetricsCollector()\n        self._buffer: list[T] = []\n        self._window_buffer: list[T] = []\n        self._last_window_time = time.time()\n\n    def process(self, pipe: SyncBasePipe[InputType, OutputType]) -&gt; Iterator[OutputType]:\n        \"\"\"Process the stream through the provided pipeline.\n\n        Synchronous version of AsyncStream.process().\n        See AsyncStream.process() for detailed documentation.\n\n        Args:\n        ----\n            pipe: Pipeline to process data through\n\n        Yields:\n        ------\n            Processed items/batches/windows depending on processing mode\n\n        Raises:\n        ------\n            PipeError: If processing fails\n            Exception: If pipe raises an unhandled exception\n\n        \"\"\"\n        if self.config.batch_size &gt; 1:\n            yield from self._process_batches(pipe)\n        elif self.config.window_size.total_seconds() &gt; 0:\n            yield from self._process_windows(pipe)\n        else:\n            for item in self.source:\n                try:\n                    yield self._process_item(pipe, item)\n                except Exception as e:\n                    raise PipeError(pipe.__class__.__name__, e) from e\n\n    def _process_item(self, pipe: SyncBasePipe[InputType, OutputType], item: T) -&gt; OutputType:\n        \"\"\"Process a single item with retry logic.\n\n        Synchronous version of AsyncStream._process_item().\n        See AsyncStream._process_item() for detailed documentation.\n        \"\"\"\n        for attempt in range(self.config.retry_attempts):\n            try:\n                return pipe(item)\n            except Exception as e:\n                if attempt == self.config.retry_attempts - 1:\n                    raise PipeError(pipe.__class__.__name__, e) from e\n                time.sleep(self.config.retry_delay * (attempt + 1))\n\n        raise RuntimeError(f\"Runtime error in: {pipe.__class__.__name__}\")\n\n    def _process_batches(self, pipe: SyncBasePipe[BatchT, BatchT]) -&gt; Iterator[BatchT]:\n        \"\"\"Process items in batches.\n\n        Args:\n        ----\n            pipe: Pipeline to process data through\n\n        Yields:\n        ------\n            Processed batches of items\n\n        \"\"\"\n        batch: list[T] = []\n\n        for item in self.source:\n            batch.append(item)\n            if len(batch) &gt;= self.config.batch_size:\n                yield pipe(cast(BatchT, batch))\n                batch = []\n\n        # Process remaining items\n        if batch:\n            yield pipe(cast(BatchT, batch))\n\n    def _process_windows(self, pipe: SyncBasePipe[BatchT, BatchT]) -&gt; Iterator[BatchT]:\n        \"\"\"Process items in time-based windows.\n\n        Synchronous version of AsyncStream._process_windows().\n        See AsyncStream._process_windows() for detailed documentation.\n        \"\"\"\n        for item in self.source:\n            current_time = time.time()\n            if current_time - self._last_window_time &gt;= self.config.window_size.total_seconds():\n                if self._window_buffer:\n                    batch: BatchT = cast(BatchT, self._window_buffer)\n                    yield pipe(batch)\n                    self._window_buffer = []\n                self._last_window_time = current_time\n            self._window_buffer.append(item)\n\n        if self._window_buffer:\n            yield pipe(batch)\n\n    def sliding_window(self, window_size: int, step_size: int) -&gt; Iterator[list[T]]:\n        \"\"\"Process items using sliding window.\n\n        Args:\n        ----\n            window_size: Size of the window\n            step_size: Number of items to slide window by\n\n        Yields:\n        ------\n            List of items in current window\n\n        \"\"\"\n        buffer: list[T] = []\n\n        for item in self.source:\n            buffer.append(item)\n            if len(buffer) &gt;= window_size:\n                yield buffer[-window_size:]\n                buffer = buffer[step_size:]\n\n    def tumbling_window(self, window_size: int) -&gt; Iterator[list[T]]:\n        \"\"\"Process items using tumbling window.\n\n        Args:\n        ----\n            window_size: Size of the window\n\n        Yields:\n        ------\n            List of items in current window\n\n        \"\"\"\n        buffer: list[T] = []\n\n        for item in self.source:\n            buffer.append(item)\n            if len(buffer) &gt;= window_size:\n                yield buffer\n                buffer = []\n\n        if buffer:  # Yield remaining items\n            yield buffer\n\n    def time_window(self, duration: timedelta) -&gt; Iterator[list[T]]:\n        \"\"\"Process items using time-based window.\n\n        Args:\n        ----\n            duration: Time duration for each window\n\n        Yields:\n        ------\n            List of items in current window\n\n        \"\"\"\n        buffer: list[T] = []\n        window_start = time.time()\n\n        for item in self.source:\n            current_time = time.time()\n            if current_time - window_start &gt;= duration.total_seconds():\n                if buffer:\n                    yield buffer\n                buffer = []\n                window_start = current_time\n            buffer.append(item)\n            time.sleep(duration.total_seconds())  # Force time separation between items\n\n        if buffer:\n            yield buffer\n</code></pre>"},{"location":"api/streams/#rivusio.streams.stream.SyncStream.config","title":"config  <code>instance-attribute</code>","text":"<pre><code>config = config or StreamConfig()\n</code></pre>"},{"location":"api/streams/#rivusio.streams.stream.SyncStream.metrics","title":"metrics  <code>instance-attribute</code>","text":"<pre><code>metrics = metrics or MetricsCollector()\n</code></pre>"},{"location":"api/streams/#rivusio.streams.stream.SyncStream.source","title":"source  <code>instance-attribute</code>","text":"<pre><code>source = source\n</code></pre>"},{"location":"api/streams/#rivusio.streams.stream.SyncStream.__init__","title":"__init__","text":"<pre><code>__init__(source: Iterator[T], config: Optional[StreamConfig] = None, metrics: Optional[MetricsCollector] = None) -&gt; None\n</code></pre> <p>Initialize sync stream.</p> <pre><code>source: Source iterator providing data\nconfig: Stream processing configuration\nmetrics: Optional metrics collector for monitoring\n</code></pre> Source code in <code>src/rivusio/streams/stream.py</code> <pre><code>def __init__(\n    self,\n    source: Iterator[T],\n    config: Optional[StreamConfig] = None,\n    metrics: Optional[MetricsCollector] = None,\n) -&gt; None:\n    \"\"\"Initialize sync stream.\n\n    Args:\n    ----\n        source: Source iterator providing data\n        config: Stream processing configuration\n        metrics: Optional metrics collector for monitoring\n\n    \"\"\"\n    self.source = source\n    self.config = config or StreamConfig()\n    self.metrics = metrics or MetricsCollector()\n    self._buffer: list[T] = []\n    self._window_buffer: list[T] = []\n    self._last_window_time = time.time()\n</code></pre>"},{"location":"api/streams/#rivusio.streams.stream.SyncStream.process","title":"process","text":"<pre><code>process(pipe: SyncBasePipe[InputType, OutputType]) -&gt; Iterator[OutputType]\n</code></pre> <p>Process the stream through the provided pipeline.</p> <p>Synchronous version of AsyncStream.process(). See AsyncStream.process() for detailed documentation.</p> <pre><code>pipe: Pipeline to process data through\n</code></pre> <pre><code>Processed items/batches/windows depending on processing mode\n</code></pre> <pre><code>PipeError: If processing fails\nException: If pipe raises an unhandled exception\n</code></pre> Source code in <code>src/rivusio/streams/stream.py</code> <pre><code>def process(self, pipe: SyncBasePipe[InputType, OutputType]) -&gt; Iterator[OutputType]:\n    \"\"\"Process the stream through the provided pipeline.\n\n    Synchronous version of AsyncStream.process().\n    See AsyncStream.process() for detailed documentation.\n\n    Args:\n    ----\n        pipe: Pipeline to process data through\n\n    Yields:\n    ------\n        Processed items/batches/windows depending on processing mode\n\n    Raises:\n    ------\n        PipeError: If processing fails\n        Exception: If pipe raises an unhandled exception\n\n    \"\"\"\n    if self.config.batch_size &gt; 1:\n        yield from self._process_batches(pipe)\n    elif self.config.window_size.total_seconds() &gt; 0:\n        yield from self._process_windows(pipe)\n    else:\n        for item in self.source:\n            try:\n                yield self._process_item(pipe, item)\n            except Exception as e:\n                raise PipeError(pipe.__class__.__name__, e) from e\n</code></pre>"},{"location":"api/streams/#rivusio.streams.stream.SyncStream.sliding_window","title":"sliding_window","text":"<pre><code>sliding_window(window_size: int, step_size: int) -&gt; Iterator[list[T]]\n</code></pre> <p>Process items using sliding window.</p> <pre><code>window_size: Size of the window\nstep_size: Number of items to slide window by\n</code></pre> <pre><code>List of items in current window\n</code></pre> Source code in <code>src/rivusio/streams/stream.py</code> <pre><code>def sliding_window(self, window_size: int, step_size: int) -&gt; Iterator[list[T]]:\n    \"\"\"Process items using sliding window.\n\n    Args:\n    ----\n        window_size: Size of the window\n        step_size: Number of items to slide window by\n\n    Yields:\n    ------\n        List of items in current window\n\n    \"\"\"\n    buffer: list[T] = []\n\n    for item in self.source:\n        buffer.append(item)\n        if len(buffer) &gt;= window_size:\n            yield buffer[-window_size:]\n            buffer = buffer[step_size:]\n</code></pre>"},{"location":"api/streams/#rivusio.streams.stream.SyncStream.time_window","title":"time_window","text":"<pre><code>time_window(duration: timedelta) -&gt; Iterator[list[T]]\n</code></pre> <p>Process items using time-based window.</p> <pre><code>duration: Time duration for each window\n</code></pre> <pre><code>List of items in current window\n</code></pre> Source code in <code>src/rivusio/streams/stream.py</code> <pre><code>def time_window(self, duration: timedelta) -&gt; Iterator[list[T]]:\n    \"\"\"Process items using time-based window.\n\n    Args:\n    ----\n        duration: Time duration for each window\n\n    Yields:\n    ------\n        List of items in current window\n\n    \"\"\"\n    buffer: list[T] = []\n    window_start = time.time()\n\n    for item in self.source:\n        current_time = time.time()\n        if current_time - window_start &gt;= duration.total_seconds():\n            if buffer:\n                yield buffer\n            buffer = []\n            window_start = current_time\n        buffer.append(item)\n        time.sleep(duration.total_seconds())  # Force time separation between items\n\n    if buffer:\n        yield buffer\n</code></pre>"},{"location":"api/streams/#rivusio.streams.stream.SyncStream.tumbling_window","title":"tumbling_window","text":"<pre><code>tumbling_window(window_size: int) -&gt; Iterator[list[T]]\n</code></pre> <p>Process items using tumbling window.</p> <pre><code>window_size: Size of the window\n</code></pre> <pre><code>List of items in current window\n</code></pre> Source code in <code>src/rivusio/streams/stream.py</code> <pre><code>def tumbling_window(self, window_size: int) -&gt; Iterator[list[T]]:\n    \"\"\"Process items using tumbling window.\n\n    Args:\n    ----\n        window_size: Size of the window\n\n    Yields:\n    ------\n        List of items in current window\n\n    \"\"\"\n    buffer: list[T] = []\n\n    for item in self.source:\n        buffer.append(item)\n        if len(buffer) &gt;= window_size:\n            yield buffer\n            buffer = []\n\n    if buffer:  # Yield remaining items\n        yield buffer\n</code></pre>"},{"location":"api/streams/#streamconfig","title":"StreamConfig","text":""},{"location":"api/streams/#rivusio.streams.stream.StreamConfig","title":"rivusio.streams.stream.StreamConfig","text":"<p>             Bases: <code>BaseModel</code></p> <p>Configuration for stream processing behavior.</p> <p>Controls various aspects of stream processing including retry behavior, timeouts, batch sizes, and window settings.</p> <pre><code>name: Optional name for the stream instance\nretry_attempts: Number of retry attempts for failed operations\nretry_delay: Initial delay between retries in seconds\nretry_backoff: Multiplier for retry delay after each attempt\ntimeout: Operation timeout in seconds\nbatch_size: Number of items to process in each batch\nwindow_size: Time window duration for window-based processing\nbuffer_size: Maximum number of items to buffer\ncollect_metrics: Whether to collect processing metrics\n</code></pre>"},{"location":"api/streams/#rivusio.streams.stream.StreamConfig--example","title":"Example:","text":"<pre><code>```python\nconfig = StreamConfig(\n    name=\"sensor_stream\",\n    batch_size=100,\n    window_size=timedelta(minutes=5),\n    buffer_size=1000\n)\n```\n</code></pre> Source code in <code>src/rivusio/streams/stream.py</code> <pre><code>class StreamConfig(BaseModel):\n    \"\"\"Configuration for stream processing behavior.\n\n    Controls various aspects of stream processing including retry behavior,\n    timeouts, batch sizes, and window settings.\n\n    Attributes:\n    ----------\n        name: Optional name for the stream instance\n        retry_attempts: Number of retry attempts for failed operations\n        retry_delay: Initial delay between retries in seconds\n        retry_backoff: Multiplier for retry delay after each attempt\n        timeout: Operation timeout in seconds\n        batch_size: Number of items to process in each batch\n        window_size: Time window duration for window-based processing\n        buffer_size: Maximum number of items to buffer\n        collect_metrics: Whether to collect processing metrics\n\n    Example:\n    -------\n        ```python\n        config = StreamConfig(\n            name=\"sensor_stream\",\n            batch_size=100,\n            window_size=timedelta(minutes=5),\n            buffer_size=1000\n        )\n        ```\n\n    \"\"\"\n\n    model_config = ConfigDict(arbitrary_types_allowed=True)\n\n    name: Optional[str] = None\n    retry_attempts: int = 3\n    retry_delay: float = 1.0\n    retry_backoff: float = 2.0\n    timeout: float = 30.0\n    batch_size: int = 1\n    window_size: timedelta = timedelta(seconds=0)\n    buffer_size: int = 1000\n    collect_metrics: bool = True\n    skip_none: bool = True\n\n    @field_validator(\"retry_attempts\")\n    @classmethod\n    def validate_retry_attempts(cls, v: int) -&gt; int:\n        \"\"\"Validate retry attempts.\"\"\"\n        if v &lt; 0:\n            raise ValueError(\"retry_attempts must be non-negative\")\n        return v\n\n    @field_validator(\"batch_size\")\n    @classmethod\n    def validate_batch_size(cls, v: int) -&gt; int:\n        \"\"\"Validate batch size.\"\"\"\n        if v &lt; 1:\n            raise ValueError(\"batch_size must be positive\")\n        return v\n</code></pre>"},{"location":"api/streams/#rivusio.streams.stream.StreamConfig.batch_size","title":"batch_size  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>batch_size: int = 1\n</code></pre>"},{"location":"api/streams/#rivusio.streams.stream.StreamConfig.buffer_size","title":"buffer_size  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>buffer_size: int = 1000\n</code></pre>"},{"location":"api/streams/#rivusio.streams.stream.StreamConfig.collect_metrics","title":"collect_metrics  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>collect_metrics: bool = True\n</code></pre>"},{"location":"api/streams/#rivusio.streams.stream.StreamConfig.model_config","title":"model_config  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>model_config = ConfigDict(arbitrary_types_allowed=True)\n</code></pre>"},{"location":"api/streams/#rivusio.streams.stream.StreamConfig.name","title":"name  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>name: Optional[str] = None\n</code></pre>"},{"location":"api/streams/#rivusio.streams.stream.StreamConfig.retry_attempts","title":"retry_attempts  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>retry_attempts: int = 3\n</code></pre>"},{"location":"api/streams/#rivusio.streams.stream.StreamConfig.retry_backoff","title":"retry_backoff  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>retry_backoff: float = 2.0\n</code></pre>"},{"location":"api/streams/#rivusio.streams.stream.StreamConfig.retry_delay","title":"retry_delay  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>retry_delay: float = 1.0\n</code></pre>"},{"location":"api/streams/#rivusio.streams.stream.StreamConfig.skip_none","title":"skip_none  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>skip_none: bool = True\n</code></pre>"},{"location":"api/streams/#rivusio.streams.stream.StreamConfig.timeout","title":"timeout  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>timeout: float = 30.0\n</code></pre>"},{"location":"api/streams/#rivusio.streams.stream.StreamConfig.window_size","title":"window_size  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>window_size: timedelta = timedelta(seconds=0)\n</code></pre>"},{"location":"api/streams/#rivusio.streams.stream.StreamConfig.validate_batch_size","title":"validate_batch_size  <code>classmethod</code>","text":"<pre><code>validate_batch_size(v: int) -&gt; int\n</code></pre> <p>Validate batch size.</p> Source code in <code>src/rivusio/streams/stream.py</code> <pre><code>@field_validator(\"batch_size\")\n@classmethod\ndef validate_batch_size(cls, v: int) -&gt; int:\n    \"\"\"Validate batch size.\"\"\"\n    if v &lt; 1:\n        raise ValueError(\"batch_size must be positive\")\n    return v\n</code></pre>"},{"location":"api/streams/#rivusio.streams.stream.StreamConfig.validate_retry_attempts","title":"validate_retry_attempts  <code>classmethod</code>","text":"<pre><code>validate_retry_attempts(v: int) -&gt; int\n</code></pre> <p>Validate retry attempts.</p> Source code in <code>src/rivusio/streams/stream.py</code> <pre><code>@field_validator(\"retry_attempts\")\n@classmethod\ndef validate_retry_attempts(cls, v: int) -&gt; int:\n    \"\"\"Validate retry attempts.\"\"\"\n    if v &lt; 0:\n        raise ValueError(\"retry_attempts must be non-negative\")\n    return v\n</code></pre>"},{"location":"api/streams/#processing-modes","title":"Processing Modes","text":"<p>The stream processors support three processing modes:</p>"},{"location":"api/streams/#single-item-processing","title":"Single Item Processing","text":"<p>Process items one at a time:</p> <pre><code>config = StreamConfig(batch_size=1)\nstream = AsyncStream(source, config=config)\nasync for result in stream.process(pipe):\n    print(f\"Processed item: {result}\")\n</code></pre>"},{"location":"api/streams/#batch-processing","title":"Batch Processing","text":"<p>Process items in fixed-size batches:</p> <pre><code>config = StreamConfig(batch_size=10)\nstream = AsyncStream(source, config=config)\nasync for batch in stream.process(pipe):\n    print(f\"Processed batch: {batch}\")\n</code></pre>"},{"location":"api/streams/#window-processing","title":"Window Processing","text":"<p>Process items in time-based windows:</p> <pre><code>config = StreamConfig(window_size=timedelta(minutes=5))\nstream = AsyncStream(source, config=config)\nasync for window in stream.process(pipe):\n    print(f\"Processed window: {window}\")\n</code></pre> <p>The same modes are available for <code>SyncStream</code> without the async/await keywords.</p>"},{"location":"development/overview/","title":"Development Overview","text":""},{"location":"development/overview/#introduction","title":"Introduction","text":"<p>This guide provides essential information for developers who want to contribute to Rivusio or build extensions using its framework. It covers development setup, coding standards, testing practices, and contribution workflows.</p>"},{"location":"development/overview/#development-environment","title":"Development Environment","text":""},{"location":"development/overview/#prerequisites","title":"Prerequisites","text":"<ul> <li>Python 3.10 or higher</li> <li>Poetry for dependency management</li> <li>Git for version control</li> <li>Make (optional, but recommended)</li> </ul>"},{"location":"development/overview/#initial-setup","title":"Initial Setup","text":"<pre><code># Clone the repository\ngit clone https://github.com/zbytealchemy/rivusio.git\ncd rivusio\n\n# Install dependencies\npoetry install\n\n# Install pre-commit hooks\npoetry run pre-commit install\n</code></pre>"},{"location":"development/overview/#project-structure","title":"Project Structure","text":"<pre><code>rivusio/\n\u251c\u2500\u2500 src/\n\u2502   \u2514\u2500\u2500 rivusio/\n\u2502       \u251c\u2500\u2500 core/          # Core framework components\n\u2502       \u251c\u2500\u2500 streams/       # Stream processing\n\u2502       \u251c\u2500\u2500 monitoring/    # Metrics and monitoring\n\u2502       \u2514\u2500\u2500 plugins/       # Plugin system\n\u251c\u2500\u2500 tests/\n\u2502   \u251c\u2500\u2500 unit/             # Unit tests\n\u2502   \u2514\u2500\u2500 integration/      # Integration tests\n\u251c\u2500\u2500 docs/                 # Documentation\n\u251c\u2500\u2500 examples/            # Example code\n\u2514\u2500\u2500 scripts/            # Development scripts\n</code></pre>"},{"location":"development/overview/#development-workflow","title":"Development Workflow","text":""},{"location":"development/overview/#1-code-style","title":"1. Code Style","text":"<p>We follow strict coding standards:</p> <ul> <li>Black for code formatting</li> <li>isort for import sorting</li> <li>mypy for type checking</li> <li>ruff for linting</li> </ul> <pre><code># Format code\nmake format\n\n# Run linting\nmake lint\n\n# Type checking\nmake typecheck\n</code></pre>"},{"location":"development/overview/#2-testing","title":"2. Testing","text":"<p>All code changes must include tests:</p> <pre><code># Run unit tests\nmake test-unit\n\n# Run integration tests\nmake test-integration\n\n# Run all tests with coverage\nmake coverage\n</code></pre>"},{"location":"development/overview/#3-documentation","title":"3. Documentation","text":"<p>Documentation is crucial:</p> <ul> <li>Update API documentation for new features</li> <li>Include docstrings for all public APIs</li> <li>Add examples for new functionality</li> <li>Update architecture docs for significant changes</li> </ul> <pre><code># Build documentation\nmake docs\n\n# Serve documentation locally\nmake docs-serve\n</code></pre>"},{"location":"development/overview/#development-guidelines","title":"Development Guidelines","text":""},{"location":"development/overview/#1-type-safety","title":"1. Type Safety","text":"<ul> <li>Use type hints consistently</li> <li>Leverage Pydantic for data validation</li> <li>Run mypy checks before committing</li> </ul> <pre><code>from typing import Dict, List\nfrom pydantic import BaseModel\n\nclass ConfigModel(BaseModel):\n    name: str\n    values: List[int]\n    options: Dict[str, str]\n</code></pre>"},{"location":"development/overview/#2-error-handling","title":"2. Error Handling","text":"<ul> <li>Use custom exception types</li> <li>Provide meaningful error messages</li> <li>Include error context</li> </ul> <pre><code>from rivusio.core.exceptions import PipelineError\n\nclass ValidationError(PipelineError):\n    \"\"\"Raised when data validation fails.\"\"\"\n    def __init__(self, message: str, context: dict):\n        super().__init__(f\"{message}: {context}\")\n        self.context = context\n</code></pre>"},{"location":"development/overview/#3-testing-practices","title":"3. Testing Practices","text":"<ul> <li>Write unit tests for all new code</li> <li>Include integration tests for features</li> <li>Use fixtures for common test data</li> <li>Mock external dependencies</li> </ul> <pre><code>import pytest\nfrom rivusio.core import Pipeline\n\n@pytest.fixture\ndef sample_pipeline():\n    return Pipeline()\n\ndef test_pipeline_processing(sample_pipeline):\n    result = sample_pipeline.process({\"data\": \"test\"})\n    assert result.status == \"success\"\n</code></pre>"},{"location":"development/overview/#4-performance-considerations","title":"4. Performance Considerations","text":"<ul> <li>Profile code changes</li> <li>Consider memory usage</li> <li>Test with large datasets</li> <li>Document performance implications</li> </ul>"},{"location":"development/overview/#contribution-process","title":"Contribution Process","text":"<ol> <li>Feature Planning</li> <li>Discuss new features in issues</li> <li>Get feedback on implementation</li> <li> <p>Review existing solutions</p> </li> <li> <p>Implementation</p> </li> <li>Create feature branch</li> <li>Follow coding standards</li> <li>Add tests and documentation</li> <li> <p>Update changelog</p> </li> <li> <p>Code Review</p> </li> <li>Submit pull request</li> <li>Address review feedback</li> <li> <p>Update based on CI results</p> </li> <li> <p>Release Process</p> </li> <li>Version bumping</li> <li>Changelog updates</li> <li>Documentation updates</li> <li>Release notes</li> </ol>"},{"location":"development/overview/#plugin-development","title":"Plugin Development","text":""},{"location":"development/overview/#1-plugin-structure","title":"1. Plugin Structure","text":"<pre><code>from rivusio.plugins import Plugin, PluginConfig\n\nclass CustomPlugin(Plugin):\n    \"\"\"Custom plugin implementation.\"\"\"\n    def __init__(self, config: PluginConfig):\n        super().__init__(config)\n</code></pre>"},{"location":"development/overview/#2-plugin-registration","title":"2. Plugin Registration","text":"<pre><code>from rivusio.plugins import register_plugin\n\n@register_plugin\nclass MyPlugin(Plugin):\n    \"\"\"Automatically registered plugin.\"\"\"\n</code></pre>"},{"location":"development/overview/#debugging-and-profiling","title":"Debugging and Profiling","text":""},{"location":"development/overview/#1-debugging-tools","title":"1. Debugging Tools","text":"<ul> <li>Built-in debugger support</li> <li>Logging configuration</li> <li>Performance profiling</li> <li>Memory profiling</li> </ul>"},{"location":"development/overview/#2-monitoring","title":"2. Monitoring","text":"<pre><code>from rivusio.monitoring import Monitor\n\nmonitor = Monitor()\nmonitor.track_performance()\n</code></pre>"},{"location":"development/overview/#additional-resources","title":"Additional Resources","text":"<ul> <li>Contributing Guide</li> <li>Testing Guide</li> <li>API Reference</li> <li>Architecture Overview</li> </ul>"},{"location":"development/overview/#getting-help","title":"Getting Help","text":"<ul> <li>GitHub Issues for bug reports</li> <li>Discussions for questions</li> <li>Pull Requests for contributions</li> <li>Documentation for guidance</li> </ul> <p>Remember to check the Contributing Guide for detailed information about the contribution process and coding standards.</p>"},{"location":"development/testing/","title":"Testing Guide","text":""},{"location":"development/testing/#unit-testing-pipes","title":"Unit Testing Pipes","text":"<pre><code>import pytest\nfrom rivusio import BasePipe\n\nclass TestDataPipe(BasePipe[Dict, Dict]):\n    async def process(self, data: Dict) -&gt; Dict:\n        return {\"processed\": data[\"value\"] * 2}\n\n@pytest.mark.asyncio\nasync def test_data_pipe():\n    pipe = TestDataPipe()\n    result = await pipe.process({\"value\": 5})\n    assert result[\"processed\"] == 10\n\n@pytest.mark.asyncio\nasync def test_pipe_error_handling():\n    pipe = TestDataPipe()\n    with pytest.raises(PipeError):\n        await pipe.process({\"invalid\": \"data\"})\n</code></pre>"},{"location":"development/testing/#pipeline-testing","title":"Pipeline Testing","text":"<pre><code>@pytest.mark.asyncio\nasync def test_pipeline_composition():\n    pipe1 = TestDataPipe()\n    pipe2 = TransformPipe()\n\n    pipeline = Pipeline([pipe1, pipe2])\n    result = await pipeline.process({\"value\": 5})\n\n    assert result[\"transformed\"]\n    assert pipeline.get_pipe_outputs(pipe1)[0][\"processed\"] == 10\n</code></pre>"},{"location":"development/testing/#mocking-external-dependencies","title":"Mocking External Dependencies","text":"<pre><code>from unittest.mock import AsyncMock, patch\n\n@pytest.mark.asyncio\nasync def test_api_pipe():\n    mock_api = AsyncMock(return_value={\"api_data\": \"test\"})\n\n    with patch(\"external_api.fetch_data\", mock_api):\n        pipe = ApiPipe()\n        result = await pipe.process({\"id\": 1})\n        assert result[\"api_data\"] == \"test\"\n</code></pre>"},{"location":"examples/advanced-patterns/","title":"Advanced Usage Patterns","text":"<p>This guide demonstrates advanced usage patterns and real-world examples for Rivusio.</p>"},{"location":"examples/advanced-patterns/#1-real-time-data-processing","title":"1. Real-time Data Processing","text":""},{"location":"examples/advanced-patterns/#stock-market-data-processing","title":"Stock Market Data Processing","text":"<pre><code>from rivusio import AsyncBasePipe, AsyncPipeline\nfrom rivusio.streams import AsyncStream, StreamConfig\nfrom datetime import timedelta\nimport aiohttp\nimport json\n\nclass StockDataFetcher(AsyncBasePipe[str, dict]):\n    def __init__(self, api_key: str):\n        super().__init__()\n        self.api_key = api_key\n\n    async def process(self, symbol: str) -&gt; dict:\n        async with aiohttp.ClientSession() as session:\n            url = f\"https://api.example.com/v1/stocks/{symbol}?apikey={self.api_key}\"\n            async with session.get(url) as response:\n                return await response.json()\n\nclass MovingAverageCalculator(AsyncBasePipe[dict, dict]):\n    def __init__(self, window_size: int = 20):\n        super().__init__()\n        self.window_size = window_size\n        self.prices: List[float] = []\n\n    async def process(self, data: dict) -&gt; dict:\n        price = float(data[\"price\"])\n        self.prices.append(price)\n        if len(self.prices) &gt; self.window_size:\n            self.prices.pop(0)\n\n        data[\"moving_average\"] = sum(self.prices) / len(self.prices)\n        return data\n\nclass AlertGenerator(AsyncBasePipe[dict, Optional[str]]):\n    def __init__(self, threshold: float = 0.1):\n        super().__init__()\n        self.threshold = threshold\n\n    async def process(self, data: dict) -&gt; Optional[str]:\n        price = float(data[\"price\"])\n        ma = data[\"moving_average\"]\n        deviation = abs(price - ma) / ma\n\n        if deviation &gt; self.threshold:\n            return f\"Alert: {data['symbol']} price deviation {deviation:.2%}\"\n        return None\n\n# Usage\nasync def main():\n    # Configure pipeline\n    pipeline = AsyncPipeline()\n    pipeline.add_pipe(StockDataFetcher(api_key=\"your_key\"))\n    pipeline.add_pipe(MovingAverageCalculator(window_size=20))\n    pipeline.add_pipe(AlertGenerator(threshold=0.1))\n\n    # Configure stream\n    config = StreamConfig(\n        window_size=timedelta(minutes=5),\n        batch_size=10\n    )\n\n    # Process stream\n    symbols = [\"AAPL\", \"GOOGL\", \"MSFT\"]\n    stream = AsyncStream(symbols, config=config)\n\n    async with pipeline:\n        async for alerts in stream.process(pipeline):\n            if alerts:\n                print(alerts)\n</code></pre>"},{"location":"examples/advanced-patterns/#2-error-handling-and-retry-logic","title":"2. Error Handling and Retry Logic","text":""},{"location":"examples/advanced-patterns/#robust-http-client","title":"Robust HTTP Client","text":"<pre><code>from rivusio import AsyncBasePipe, AsyncPipeline\nfrom rivusio.monitoring import PipelineMonitor\nimport aiohttp\nimport asyncio\nfrom typing import Optional, Dict, Any\n\nclass RobustHTTPClient(AsyncBasePipe[str, Optional[Dict[str, Any]]]):\n    def __init__(\n        self,\n        max_retries: int = 3,\n        base_delay: float = 1.0,\n        max_delay: float = 30.0\n    ):\n        super().__init__()\n        self.max_retries = max_retries\n        self.base_delay = base_delay\n        self.max_delay = max_delay\n\n    async def process(self, url: str) -&gt; Optional[Dict[str, Any]]:\n        retries = 0\n        while retries &lt;= self.max_retries:\n            try:\n                async with aiohttp.ClientSession() as session:\n                    async with session.get(url) as response:\n                        if response.status == 429:  # Rate limited\n                            retry_after = int(response.headers.get(\"Retry-After\", self.base_delay))\n                            await asyncio.sleep(min(retry_after, self.max_delay))\n                            continue\n\n                        response.raise_for_status()\n                        return await response.json()\n\n            except aiohttp.ClientError as e:\n                retries += 1\n                if retries &gt; self.max_retries:\n                    self.logger.error(f\"Failed to fetch {url}: {str(e)}\")\n                    return None\n\n                delay = min(self.base_delay * (2 ** retries), self.max_delay)\n                await asyncio.sleep(delay)\n\n        return None\n\n# Usage\nasync def main():\n    pipeline = AsyncPipeline()\n    pipeline.add_pipe(RobustHTTPClient(max_retries=3))\n\n    # Add monitoring\n    monitor = PipelineMonitor()\n    pipeline.set_monitor(monitor)\n\n    urls = [\n        \"https://api1.example.com/data\",\n        \"https://api2.example.com/data\",\n        \"https://api3.example.com/data\"\n    ]\n\n    async with pipeline:\n        results = await pipeline.execute_parallel(urls)\n\n    # Check metrics\n    print(monitor.get_metrics())\n</code></pre>"},{"location":"examples/advanced-patterns/#3-complex-data-transformation","title":"3. Complex Data Transformation","text":""},{"location":"examples/advanced-patterns/#log-processing-pipeline","title":"Log Processing Pipeline","text":"<pre><code>from rivusio import AsyncBasePipe, AsyncPipeline\nfrom rivusio.streams import AsyncStream, StreamConfig\nfrom datetime import datetime, timedelta\nimport re\nfrom typing import Dict, List, Optional\n\nclass LogParser(AsyncBasePipe[str, Optional[Dict[str, Any]]]):\n    def __init__(self):\n        super().__init__()\n        self.pattern = re.compile(\n            r'(\\d{4}-\\d{2}-\\d{2} \\d{2}:\\d{2}:\\d{2}) \\[(\\w+)\\] (\\w+): (.*)'\n        )\n\n    async def process(self, line: str) -&gt; Optional[Dict[str, Any]]:\n        match = self.pattern.match(line)\n        if not match:\n            return None\n\n        timestamp, level, component, message = match.groups()\n        return {\n            \"timestamp\": datetime.strptime(timestamp, \"%Y-%m-%d %H:%M:%S\"),\n            \"level\": level,\n            \"component\": component,\n            \"message\": message\n        }\n\nclass ErrorAggregator(AsyncBasePipe[List[Dict[str, Any]], Dict[str, int]]):\n    async def process(self, logs: List[Dict[str, Any]]) -&gt; Dict[str, int]:\n        error_counts = {}\n        for log in logs:\n            if log[\"level\"] == \"ERROR\":\n                component = log[\"component\"]\n                error_counts[component] = error_counts.get(component, 0) + 1\n        return error_counts\n\nclass AlertFormatter(AsyncBasePipe[Dict[str, int], Optional[str]]):\n    def __init__(self, threshold: int = 5):\n        super().__init__()\n        self.threshold = threshold\n\n    async def process(self, error_counts: Dict[str, int]) -&gt; Optional[str]:\n        alerts = []\n        for component, count in error_counts.items():\n            if count &gt;= self.threshold:\n                alerts.append(\n                    f\"High error rate in {component}: {count} errors\"\n                )\n        return \"\\n\".join(alerts) if alerts else None\n\n# Usage\nasync def main():\n    pipeline = AsyncPipeline()\n    pipeline.add_pipe(LogParser())\n    pipeline.add_pipe(ErrorAggregator())\n    pipeline.add_pipe(AlertFormatter(threshold=5))\n\n    config = StreamConfig(\n        window_size=timedelta(minutes=5),\n        batch_size=100\n    )\n\n    async def log_generator():\n        while True:\n            with open(\"app.log\", \"r\") as f:\n                for line in f:\n                    yield line\n            await asyncio.sleep(1)\n\n    stream = AsyncStream(log_generator(), config=config)\n\n    async with pipeline:\n        async for alert in stream.process(pipeline):\n            if alert:\n                print(alert)\n</code></pre>"},{"location":"examples/advanced-patterns/#4-performance-optimization-example","title":"4. Performance Optimization Example","text":""},{"location":"examples/advanced-patterns/#parallel-data-processing","title":"Parallel Data Processing","text":"<pre><code>from rivusio import AsyncBasePipe, AsyncPipeline\nfrom rivusio.monitoring import PipelineMonitor\nimport asyncio\nfrom typing import List, Any\nimport numpy as np\n\nclass DataPreprocessor(AsyncBasePipe[np.ndarray, np.ndarray]):\n    async def process(self, data: np.ndarray) -&gt; np.ndarray:\n        # Simulate CPU-intensive preprocessing\n        return np.fft.fft2(data)\n\nclass FeatureExtractor(AsyncBasePipe[np.ndarray, List[float]]):\n    async def process(self, data: np.ndarray) -&gt; List[float]:\n        # Extract features from preprocessed data\n        return [\n            float(np.mean(data)),\n            float(np.std(data)),\n            float(np.max(data)),\n            float(np.min(data))\n        ]\n\nclass ModelPredictor(AsyncBasePipe[List[float], float]):\n    def __init__(self):\n        super().__init__()\n        self.model = self._load_model()\n\n    def _load_model(self):\n        # Simulate model loading\n        return lambda x: sum(x) / len(x)\n\n    async def process(self, features: List[float]) -&gt; float:\n        return self.model(features)\n\n# Usage\nasync def main():\n    # Create pipeline with monitoring\n    pipeline = AsyncPipeline()\n    monitor = PipelineMonitor()\n    pipeline.set_monitor(monitor)\n\n    # Add pipes\n    pipeline.add_pipe(DataPreprocessor())\n    pipeline.add_pipe(FeatureExtractor())\n    pipeline.add_pipe(ModelPredictor())\n\n    # Enable parallel processing\n    pipeline.parallel_execution = True\n    pipeline.max_workers = 4\n\n    # Generate test data\n    data = [np.random.rand(100, 100) for _ in range(1000)]\n\n    # Process in parallel\n    async with pipeline:\n        results = await pipeline.execute_parallel(data)\n\n    # Print performance metrics\n    metrics = monitor.get_metrics()\n    print(f\"Throughput: {metrics.throughput:.2f} items/s\")\n    print(f\"Average latency: {metrics.average_latency:.2f}ms\")\n</code></pre>"},{"location":"examples/basic-transformation/","title":"Basic Transformation Example","text":"<p>This example demonstrates how to perform basic data transformations using Rivusio.</p>"},{"location":"examples/basic-transformation/#setup","title":"Setup","text":"<pre><code>from rivusio import AsyncBasePipe, PipeConfig\nfrom typing import Dict, List\nfrom pydantic import BaseModel, Field\n\n# Define data model\nclass Record(BaseModel):\n    id: int\n    value: float\n    category: str\n</code></pre>"},{"location":"examples/basic-transformation/#create-configuration","title":"Create Configuration","text":"<pre><code>class TransformConfig(PipeConfig):\n    multiply_by: float = Field(default=1.0, gt=0)\n    categories: List[str] = Field(default_factory=lambda: [\"A\", \"B\", \"C\"])\n</code></pre>"},{"location":"examples/basic-transformation/#implement-transformation","title":"Implement Transformation","text":"<pre><code>class TransformPipe(AsyncBasePipe[Record, Record]):\n    def __init__(self, config: TransformConfig):\n        self.config = config\n\n    async def process(self, data: Record) -&gt; Record:\n        if data.category in self.config.categories:\n            data.value *= self.config.multiply_by\n        return data\n</code></pre>"},{"location":"examples/basic-transformation/#use-the-pipe","title":"Use the Pipe","text":"<pre><code>async def main():\n    # Create configuration\n    config = TransformConfig(\n        multiply_by=2.0,\n        categories=[\"A\", \"B\"]\n    )\n\n    # Create pipe\n    pipe = TransformPipe(config)\n\n    # Process data\n    record = Record(\n        id=1,\n        value=10.0,\n        category=\"A\"\n    )\n\n    result = await pipe.process(record)\n    print(result)  # Record(id=1, value=20.0, category='A')\n\nif __name__ == \"__main__\":\n    import asyncio\n    asyncio.run(main())\n</code></pre>"},{"location":"examples/basic-transformation/#explanation","title":"Explanation","text":"<ol> <li>We define a simple data model using Pydantic</li> <li>We create a configuration class for our transformation</li> <li>We implement a pipe that multiplies values by a configurable factor</li> <li>We process data through the pipe asynchronously</li> </ol>"},{"location":"examples/basic-transformation/#next-steps","title":"Next Steps","text":"<ul> <li>Try adding more transformations</li> <li>Combine multiple pipes into a pipeline</li> <li>Add error handling and validation</li> <li>Explore streaming capabilities</li> </ul>"},{"location":"examples/basic_pipeline/","title":"Basic Pipeline Example","text":"<p>This example demonstrates how to create a basic data processing pipeline using Rivusio.</p>"},{"location":"examples/basic_pipeline/#setup","title":"Setup","text":"<p>First, install Rivusio:</p> <pre><code>pip install rivusio\n</code></pre>"},{"location":"examples/basic_pipeline/#creating-the-pipeline","title":"Creating the Pipeline","text":"<p>Here's a simple pipeline that processes user data:</p> <pre><code>from rivusio import AsyncBasePipe, Pipeline\nfrom typing import Dict, List\nfrom pydantic import BaseModel, EmailStr\n\n# Define data models\nclass User(BaseModel):\n    name: str\n    age: int\n    email: EmailStr\n\n# Define pipe configurations\nclass FilterConfig(PipeConfig):\n    min_age: int\n\nclass EnrichConfig(PipeConfig):\n    domain: str\n\n# Create pipes\nclass AgeFilterPipe(AsyncBasePipe[List[User], List[User]]):\n    def __init__(self, min_age: int):\n        self.min_age = min_age\n\n    async def process(self, users: List[User]) -&gt; List[User]:\n        return [user for user in users if user.age &gt;= self.min_age]\n\nclass EmailEnrichPipe(AsyncBasePipe[List[User], List[User]]):\n    def __init__(self, domain: str):\n        self.domain = domain\n\n    async def process(self, users: List[User]) -&gt; List[User]:\n        return users\n\n# Create and run the pipeline\nasync def main():\n    # Create sample data\n    users = [\n        User(name=\"Alice\", age=25, email=\"alice.smith@example.com\"),\n        User(name=\"Bob\", age=17, email=\"bob.jones@example.com\"),\n        User(name=\"Charlie\", age=30, email=\"charlie.brown@example.com\")\n    ]\n\n    # Configure pipes\n    age_filter = AgeFilterPipe(min_age=18)\n    email_enricher = EmailEnrichPipe(domain=\"example.com\")\n\n    # Create pipeline\n    pipeline = Pipeline()\n    pipeline.add_pipe(age_filter)\n    pipeline.add_pipe(email_enricher)\n\n    # Process data\n    result = await pipeline.process(users)\n    print(result)\n\n# Run the pipeline\nif __name__ == \"__main__\":\n    import asyncio\n    asyncio.run(main())\n</code></pre>"},{"location":"examples/basic_pipeline/#output","title":"Output","text":"<p>The pipeline will output: <pre><code>[\n    User(name='Alice', age=25, email='alice.smith@example.com'),\n    User(name='Charlie', age=30, email='charlie.brown@example.com')\n]\n</code></pre></p>"},{"location":"examples/basic_pipeline/#explanation","title":"Explanation","text":"<ol> <li>We define two pipes:</li> <li><code>AgeFilterPipe</code>: Filters out users under 18</li> <li> <p><code>EmailEnrichPipe</code>: Adds domain information to emails</p> </li> <li> <p>Each pipe has its own configuration class</p> </li> <li>The pipeline processes the data sequentially through both pipes</li> <li>Type safety is enforced throughout the pipeline</li> <li>All processing is done asynchronously</li> </ol>"},{"location":"examples/lambda-composition/","title":"Lambda Composition Example","text":"<p>This example shows how to use lambda functions and functional composition in Rivusio.</p>"},{"location":"examples/lambda-composition/#basic-lambda-pipe","title":"Basic Lambda Pipe","text":"<pre><code>from rivusio import ConfigurablePipe, PipeConfig\nfrom typing import Callable, Dict, Any\n\nclass LambdaConfig(PipeConfig):\n    func: Callable\n    args: tuple = ()\n    kwargs: Dict[str, Any] = {}\n\nclass LambdaPipe(ConfigurablePipe[Any, Any]):\n    async def process(self, data: Any) -&gt; Any:\n        return self.config.func(data, *self.config.args, **self.config.kwargs)\n</code></pre>"},{"location":"examples/lambda-composition/#using-lambda-pipes","title":"Using Lambda Pipes","text":"<pre><code># Create lambda pipes\nmultiply = LambdaPipe(LambdaConfig(\n    func=lambda x, y: x * y,\n    args=(2,)\n))\n\nadd = LambdaPipe(LambdaConfig(\n    func=lambda x, y: x + y,\n    args=(5,)\n))\n\n# Compose pipes\npipeline = multiply &gt;&gt; add\n\n# Process data\nresult = await pipeline.process(10)  # (10 * 2) + 5 = 25\n</code></pre>"},{"location":"examples/lambda-composition/#functional-composition","title":"Functional Composition","text":"<pre><code>from functools import partial\n\nclass FunctionalPipe(ConfigurablePipe[Any, Any]):\n    def __init__(self, func: Callable):\n        super().__init__(PipeConfig())\n        self.func = func\n\n    async def process(self, data: Any) -&gt; Any:\n        return self.func(data)\n\n# Create functional pipes\ndouble = FunctionalPipe(lambda x: x * 2)\nincrement = FunctionalPipe(lambda x: x + 1)\nsquare = FunctionalPipe(lambda x: x ** 2)\n\n# Compose pipes\npipeline = double &gt;&gt; increment &gt;&gt; square\n</code></pre>"},{"location":"examples/lambda-composition/#higher-order-functions","title":"Higher-Order Functions","text":"<pre><code>def create_filter(predicate: Callable[[Any], bool]) -&gt; FunctionalPipe:\n    async def filter_func(data: Any) -&gt; Any:\n        return data if predicate(data) else None\n    return FunctionalPipe(filter_func)\n\ndef create_map(func: Callable) -&gt; FunctionalPipe:\n    return FunctionalPipe(func)\n\n# Create pipes using higher-order functions\nis_positive = create_filter(lambda x: x &gt; 0)\ndouble = create_map(lambda x: x * 2)\n</code></pre>"},{"location":"examples/lambda-composition/#advanced-example","title":"Advanced Example","text":"<pre><code>from typing import List\nfrom dataclasses import dataclass\n\n@dataclass\nclass Record:\n    value: float\n    category: str\n\nclass DataPipeline:\n    def __init__(self):\n        self.pipeline = (\n            create_filter(lambda x: x.value &gt; 0)\n            &gt;&gt; create_map(lambda x: Record(x.value * 2, x.category))\n            &gt;&gt; create_filter(lambda x: x.category in [\"A\", \"B\"])\n        )\n\n    async def process(self, records: List[Record]) -&gt; List[Record]:\n        results = []\n        for record in records:\n            result = await self.pipeline.process(record)\n            if result:\n                results.append(result)\n        return results\n\n# Use the pipeline\nasync def main():\n    pipeline = DataPipeline()\n\n    records = [\n        Record(10, \"A\"),\n        Record(-5, \"B\"),\n        Record(15, \"C\"),\n        Record(20, \"A\")\n    ]\n\n    results = await pipeline.process(records)\n    for record in results:\n        print(f\"Value: {record.value}, Category: {record.category}\")\n\nif __name__ == \"__main__\":\n    import asyncio\n    asyncio.run(main())\n</code></pre>"},{"location":"examples/lambda-composition/#composition-with-error-handling","title":"Composition with Error Handling","text":"<pre><code>def safe_pipe(pipe: FunctionalPipe) -&gt; FunctionalPipe:\n    async def safe_process(data: Any) -&gt; Any:\n        try:\n            return await pipe.process(data)\n        except Exception as e:\n            print(f\"Error in pipe: {e}\")\n            return None\n    return FunctionalPipe(safe_process)\n\n# Create safe pipeline\nsafe_pipeline = (\n    safe_pipe(double)\n    &gt;&gt; safe_pipe(increment)\n    &gt;&gt; safe_pipe(square)\n)\n</code></pre>"},{"location":"examples/lambda-composition/#best-practices","title":"Best Practices","text":"<ol> <li>Keep functions pure and simple</li> <li>Use type hints for better safety</li> <li>Handle errors appropriately</li> <li>Document function behavior</li> <li>Test composition chains</li> <li>Consider performance implications</li> </ol>"},{"location":"examples/pipeline-composition/","title":"Pipeline Composition Example","text":"<p>This example demonstrates different ways to compose pipelines in Rivusio.</p>"},{"location":"examples/pipeline-composition/#basic-composition","title":"Basic Composition","text":"<pre><code>from rivusio import Pipeline, ConfigurablePipe, PipeConfig\nfrom typing import Dict, List\n\n# Define pipes\nclass FilterConfig(PipeConfig):\n    min_value: float\n\nclass FilterPipe(ConfigurablePipe[Dict, Dict]):\n    async def process(self, data: Dict) -&gt; Dict:\n        if data[\"value\"] &gt;= self.config.min_value:\n            return data\n        return None\n\nclass TransformPipe(ConfigurablePipe[Dict, Dict]):\n    async def process(self, data: Dict) -&gt; Dict:\n        data[\"value\"] *= 2\n        return data\n\n# Create pipeline\nfilter_pipe = FilterPipe(FilterConfig(min_value=10))\ntransform_pipe = TransformPipe()\n\npipeline = Pipeline()\npipeline.add_pipe(filter_pipe)\npipeline.add_pipe(transform_pipe)\n</code></pre>"},{"location":"examples/pipeline-composition/#using-operators","title":"Using Operators","text":"<pre><code># Using the &gt;&gt; operator\npipeline = filter_pipe &gt;&gt; transform_pipe\n\n# Process data\ndata = {\"value\": 15}\nresult = await pipeline.process(data)  # {\"value\": 30}\n</code></pre>"},{"location":"examples/pipeline-composition/#branching-pipelines","title":"Branching Pipelines","text":"<pre><code>class BranchConfig(PipeConfig):\n    threshold: float\n\nclass BranchPipe(ConfigurablePipe[Dict, Dict]):\n    def __init__(self, config: BranchConfig):\n        super().__init__(config)\n        self.high_branch = Pipeline()\n        self.low_branch = Pipeline()\n\n    async def process(self, data: Dict) -&gt; Dict:\n        if data[\"value\"] &gt;= self.config.threshold:\n            return await self.high_branch.process(data)\n        return await self.low_branch.process(data)\n\n# Create branching pipeline\nbranch = BranchPipe(BranchConfig(threshold=20))\nbranch.high_branch.add_pipe(TransformPipe())\nbranch.low_branch.add_pipe(FilterPipe(FilterConfig(min_value=5)))\n</code></pre>"},{"location":"examples/pipeline-composition/#parallel-processing","title":"Parallel Processing","text":"<pre><code>from asyncio import gather\n\nclass ParallelPipeline(Pipeline):\n    def __init__(self, pipes: List[Pipeline]):\n        super().__init__()\n        self.pipes = pipes\n\n    async def process(self, data: Dict) -&gt; List[Dict]:\n        tasks = [pipe.process(data) for pipe in self.pipes]\n        return await gather(*tasks)\n\n# Create parallel pipelines\npipe1 = Pipeline([FilterPipe(FilterConfig(min_value=10))])\npipe2 = Pipeline([TransformPipe()])\n\nparallel = ParallelPipeline([pipe1, pipe2])\n</code></pre>"},{"location":"examples/pipeline-composition/#error-handling","title":"Error Handling","text":"<pre><code>class SafePipeline(Pipeline):\n    async def process(self, data: Dict) -&gt; Dict:\n        try:\n            return await super().process(data)\n        except Exception as e:\n            print(f\"Error in pipeline: {e}\")\n            return None\n\n# Create safe pipeline\nsafe = SafePipeline()\nsafe.add_pipe(filter_pipe)\nsafe.add_pipe(transform_pipe)\n</code></pre>"},{"location":"examples/pipeline-composition/#full-example","title":"Full Example","text":"<pre><code>async def main():\n    # Create pipes\n    filter_pipe = FilterPipe(FilterConfig(min_value=10))\n    transform_pipe = TransformPipe()\n\n    # Create branching pipeline\n    branch = BranchPipe(BranchConfig(threshold=20))\n    branch.high_branch.add_pipe(transform_pipe)\n    branch.low_branch.add_pipe(filter_pipe)\n\n    # Create safe pipeline\n    pipeline = SafePipeline()\n    pipeline.add_pipe(filter_pipe)\n    pipeline.add_pipe(branch)\n    pipeline.add_pipe(transform_pipe)\n\n    # Process data\n    data = {\"value\": 25}\n    result = await pipeline.process(data)\n    print(result)  # {\"value\": 100}\n\nif __name__ == \"__main__\":\n    import asyncio\n    asyncio.run(main())\n</code></pre>"},{"location":"examples/pipeline-composition/#best-practices","title":"Best Practices","text":"<ol> <li>Keep pipelines focused and modular</li> <li>Use appropriate error handling</li> <li>Consider performance implications</li> <li>Document pipeline behavior</li> <li>Test pipeline components</li> <li>Monitor pipeline execution</li> </ol>"},{"location":"examples/sync_pipeline/","title":"Synchronous Pipeline Examples","text":""},{"location":"examples/sync_pipeline/#basic-sync-pipeline","title":"Basic Sync Pipeline","text":"<pre><code>from rivusio import SyncBasePipe\nfrom typing import Dict, List\n\nclass NumberFilterPipe(SyncBasePipe[Dict, Dict]):\n    def process(self, data: Dict) -&gt; Dict:\n        return {k: v for k, v in data.items() if isinstance(v, (int, float))}\n\nclass SumPipe(SyncBasePipe[Dict, float]):\n    def process(self, data: Dict) -&gt; float:\n        return sum(data.values())\n\ndef main():\n    # Create pipeline\n    pipeline = NumberFilterPipe() &gt;&gt; SumPipe()\n\n    # Process data\n    data = {\"a\": 10, \"b\": \"text\", \"c\": 20, \"d\": 30.5}\n    result = pipeline(data)  # 60.5\n    print(f\"Sum of numbers: {result}\")\n\nif __name__ == \"__main__\":\n    main()\n</code></pre>"},{"location":"examples/sync_pipeline/#batch-processing-example","title":"Batch Processing Example","text":"<pre><code>from rivusio import SyncBasePipe, PipeConfig\nfrom typing import List\nfrom pydantic import BaseModel, Field\n\nclass BatchConfig(PipeConfig):\n    threshold: float = Field(gt=0)\n\nclass Transaction(BaseModel):\n    id: str\n    amount: float\n    currency: str\n\nclass BatchFilterPipe(SyncBasePipe[List[Transaction], List[Transaction]]):\n    def __init__(self, config: BatchConfig):\n        self.config = config\n\n    def process(self, transactions: List[Transaction]) -&gt; List[Transaction]:\n        return [t for t in transactions if t.amount &gt; self.config.threshold]\n\nclass CurrencyNormalizePipe(SyncBasePipe[List[Transaction], List[Transaction]]):\n    def process(self, transactions: List[Transaction]) -&gt; List[Transaction]:\n        return [\n            Transaction(\n                id=t.id,\n                amount=t.amount,\n                currency=t.currency.upper()\n            ) for t in transactions\n        ]\n\ndef main():\n    # Sample data\n    transactions = [\n        Transaction(id=\"1\", amount=100.0, currency=\"usd\"),\n        Transaction(id=\"2\", amount=50.0, currency=\"eur\"),\n        Transaction(id=\"3\", amount=200.0, currency=\"gbp\"),\n    ]\n\n    # Create pipeline\n    pipeline = BatchFilterPipe(BatchConfig(threshold=75.0)) &gt;&gt; CurrencyNormalizePipe()\n\n    # Process batch\n    result = pipeline(transactions)\n    for t in result:\n        print(f\"Processed: {t}\")\n\nif __name__ == \"__main__\":\n    main()\n</code></pre>"},{"location":"getting-started/architecture-diagrams/","title":"Rivusio Architecture","text":""},{"location":"getting-started/architecture-diagrams/#overview","title":"Overview","text":"<p>Rivusio is a high-performance data processing framework built on modern Python, designed for both synchronous and asynchronous data processing pipelines. This document outlines the core architecture and design principles.</p>"},{"location":"getting-started/architecture-diagrams/#core-components","title":"Core Components","text":"<pre><code>graph TD\n    A[Data Source] --&gt; B[Stream]\n    B --&gt; C[Pipeline]\n    C --&gt; D[Pipes]\n    D --&gt; E[Data Sink]\n\n    F[Monitoring] --&gt; B\n    F --&gt; C\n    F --&gt; D\n\n    G[Plugin System] --&gt; D\n\n    H[Configuration] --&gt; B\n    H --&gt; C\n    H --&gt; D\n\n    subgraph \"Core Components\"\n        B\n        C\n        D\n    end\n\n    subgraph \"Support Systems\"\n        F\n        G\n        H\n    end</code></pre>"},{"location":"getting-started/architecture-diagrams/#component-details","title":"Component Details","text":""},{"location":"getting-started/architecture-diagrams/#1-stream-processing","title":"1. Stream Processing","text":"<ul> <li>AsyncStream/SyncStream: Handle data streaming with support for:</li> <li>Batching</li> <li>Windowing</li> <li>Error handling</li> <li>Backpressure (planned)</li> </ul>"},{"location":"getting-started/architecture-diagrams/#2-pipeline","title":"2. Pipeline","text":"<ul> <li>Pipeline Management: Orchestrates the flow of data through:</li> <li>Multiple pipes</li> <li>Error handling</li> <li>Monitoring</li> <li>Resource management</li> </ul>"},{"location":"getting-started/architecture-diagrams/#3-pipes","title":"3. Pipes","text":"<ul> <li>Base Pipes: AsyncBasePipe and SyncBasePipe</li> <li>Configurable Pipes: Support for configuration injection</li> <li>Plugin System: Extensible architecture for custom pipes</li> </ul>"},{"location":"getting-started/architecture-diagrams/#4-monitoring","title":"4. Monitoring","text":"<ul> <li>Metrics Collection: Runtime statistics and performance metrics</li> <li>Pipeline Monitor: Specialized monitoring for pipeline execution</li> <li>Error Tracking: Comprehensive error handling and reporting</li> </ul>"},{"location":"getting-started/architecture-diagrams/#5-plugin-system","title":"5. Plugin System","text":"<ul> <li>Registry: Central plugin management</li> <li>Dynamic Loading: Runtime plugin discovery</li> <li>Type Safety: Strong typing for plugin interfaces</li> </ul>"},{"location":"getting-started/architecture-diagrams/#data-flow","title":"Data Flow","text":"<pre><code>sequenceDiagram\n    participant S as Source\n    participant St as Stream\n    participant P as Pipeline\n    participant Pi as Pipes\n    participant Si as Sink\n\n    S-&gt;&gt;St: Emit Data\n    St-&gt;&gt;P: Process Batch/Window\n    P-&gt;&gt;Pi: Transform Data\n    Pi-&gt;&gt;Si: Output Results\n\n    Note over St,P: Monitoring &amp; Metrics\n    Note over P,Pi: Error Handling</code></pre>"},{"location":"getting-started/architecture-diagrams/#design-principles","title":"Design Principles","text":"<ol> <li>Type Safety</li> <li>Strong typing throughout the system</li> <li>Runtime type checking</li> <li> <p>Pydantic integration for data validation</p> </li> <li> <p>Extensibility</p> </li> <li>Plugin system for custom components</li> <li>Configurable pipeline behavior</li> <li> <p>Custom monitoring integration</p> </li> <li> <p>Performance</p> </li> <li>Efficient batch processing</li> <li>Async/sync support</li> <li> <p>Resource management</p> </li> <li> <p>Reliability</p> </li> <li>Comprehensive error handling</li> <li>Monitoring and metrics</li> <li>Testing and validation</li> </ol>"},{"location":"getting-started/architecture-diagrams/#future-enhancements","title":"Future Enhancements","text":"<p>See our TODO for planned improvements and enhancements.</p>"},{"location":"getting-started/architecture-overview/","title":"Architecture Overview","text":""},{"location":"getting-started/architecture-overview/#introduction","title":"Introduction","text":"<p>Rivusio is designed as a modular, type-safe data processing framework that emphasizes flexibility, performance, and reliability. The architecture follows clean design principles with clear separation of concerns and strong typing throughout the system.</p>"},{"location":"getting-started/architecture-overview/#core-architecture-principles","title":"Core Architecture Principles","text":"<ol> <li>Modularity</li> <li>Each component is self-contained with well-defined interfaces</li> <li>Loose coupling between components enables easy extension</li> <li> <p>Plugin system for custom functionality</p> </li> <li> <p>Type Safety</p> </li> <li>Comprehensive type hints throughout the codebase</li> <li>Runtime type checking for data validation</li> <li> <p>Pydantic integration for configuration management</p> </li> <li> <p>Processing Models</p> </li> <li>Support for both synchronous and asynchronous processing</li> <li>Flexible pipeline composition</li> <li>Stream-based processing capabilities</li> </ol>"},{"location":"getting-started/architecture-overview/#key-components","title":"Key Components","text":""},{"location":"getting-started/architecture-overview/#1-core-components","title":"1. Core Components","text":""},{"location":"getting-started/architecture-overview/#pipes","title":"Pipes","text":"<ul> <li>Base classes: <code>AsyncBasePipe</code> and <code>SyncBasePipe</code></li> <li>Type-safe data transformation units</li> <li>Configurable through Pydantic models</li> <li>Support for both sync and async processing</li> </ul>"},{"location":"getting-started/architecture-overview/#pipelines","title":"Pipelines","text":"<ul> <li>Composition of multiple pipes</li> <li>Automatic handling of sync/async transitions</li> <li>Resource management and cleanup</li> <li>Support for parallel execution</li> </ul>"},{"location":"getting-started/architecture-overview/#streams","title":"Streams","text":"<ul> <li>Efficient data streaming abstraction</li> <li>Windowing and batching support</li> <li>Backpressure handling</li> <li>Error recovery mechanisms</li> </ul>"},{"location":"getting-started/architecture-overview/#2-support-systems","title":"2. Support Systems","text":""},{"location":"getting-started/architecture-overview/#configuration-management","title":"Configuration Management","text":"<ul> <li>Type-safe configuration using Pydantic</li> <li>Hierarchical configuration structure</li> <li>Runtime configuration validation</li> <li>Environment variable integration</li> </ul>"},{"location":"getting-started/architecture-overview/#monitoring-system","title":"Monitoring System","text":"<ul> <li>Real-time metrics collection</li> <li>Performance monitoring</li> <li>Error tracking and reporting</li> <li>Custom metric extensions</li> </ul>"},{"location":"getting-started/architecture-overview/#plugin-system","title":"Plugin System","text":"<ul> <li>Dynamic plugin discovery</li> <li>Type-safe plugin interfaces</li> <li>Central plugin registry</li> <li>Runtime plugin loading</li> </ul>"},{"location":"getting-started/architecture-overview/#data-flow","title":"Data Flow","text":"<ol> <li>Input Processing</li> <li>Data enters through Stream interfaces</li> <li>Optional batching and windowing</li> <li> <p>Type validation and transformation</p> </li> <li> <p>Pipeline Processing</p> </li> <li>Data flows through configured pipes</li> <li>Automatic sync/async handling</li> <li>Error handling and recovery</li> <li> <p>Resource management</p> </li> <li> <p>Output Handling</p> </li> <li>Results collection and aggregation</li> <li>Type-safe output validation</li> <li>Delivery to configured sinks</li> </ol>"},{"location":"getting-started/architecture-overview/#error-handling","title":"Error Handling","text":"<ul> <li>Comprehensive error tracking</li> <li>Automatic retries with backoff</li> <li>Error recovery strategies</li> <li>Detailed error reporting</li> </ul>"},{"location":"getting-started/architecture-overview/#performance-considerations","title":"Performance Considerations","text":"<ul> <li>Efficient batch processing</li> <li>Parallel execution capabilities</li> <li>Resource pooling</li> <li>Memory management</li> <li>Backpressure handling</li> </ul>"},{"location":"getting-started/architecture-overview/#security","title":"Security","text":"<ul> <li>Type-safe data handling</li> <li>Input validation</li> <li>Resource limits</li> <li>Plugin isolation</li> </ul>"},{"location":"getting-started/architecture-overview/#extensibility","title":"Extensibility","text":"<p>The architecture is designed for extensibility through:</p> <ol> <li>Custom Pipes</li> <li>Implement custom transformation logic</li> <li>Add domain-specific processing</li> <li> <p>Integrate with external systems</p> </li> <li> <p>Custom Monitors</p> </li> <li>Add custom metrics</li> <li>Integrate with monitoring systems</li> <li> <p>Custom alerting logic</p> </li> <li> <p>Custom Plugins</p> </li> <li>Extend core functionality</li> <li>Add new features</li> <li>Integrate third-party tools</li> </ol>"},{"location":"getting-started/architecture-overview/#future-considerations","title":"Future Considerations","text":"<ol> <li>Scalability</li> <li>Distributed processing support</li> <li>Cluster coordination</li> <li> <p>State management</p> </li> <li> <p>Integration</p> </li> <li>More data source/sink adapters</li> <li>Cloud service integration</li> <li> <p>Container orchestration</p> </li> <li> <p>Monitoring</p> </li> <li>Advanced metrics</li> </ol>"},{"location":"getting-started/concepts/","title":"Core Concepts","text":""},{"location":"getting-started/concepts/#overview","title":"Overview","text":"<p>Rivusio is built around a few core concepts:</p> <ol> <li>Pipes: Basic units of data transformation</li> <li>Pipelines: Compositions of pipes</li> <li>Configuration: Type-safe settings using Pydantic</li> <li>Error Handling: Comprehensive error tracking</li> <li>Monitoring: Built-in performance metrics</li> </ol>"},{"location":"getting-started/concepts/#pipes","title":"Pipes","text":"<p>Pipes are the fundamental building blocks in Rivusio. Each pipe:</p> <ul> <li>Takes input data</li> <li>Performs a transformation</li> <li>Returns output data</li> </ul> <pre><code>from typing import List, Dict\nfrom rivusio import AsyncBasePipe, PipeConfig\n\nclass FilterConfig(PipeConfig):\n    \"\"\"Configuration for filter pipe.\"\"\"\n    min_value: float = 100.0\n\nclass FilterPipe(AsyncBasePipe[List[Dict], List[Dict]]):\n    \"\"\"Filter items based on value.\"\"\"\n\n    def __init__(self, config: FilterConfig) -&gt; None:\n        super().__init__()\n        self.config = config\n        self.name = config.name or self.__class__.__name__\n\n    async def process(self, data: List[Dict]) -&gt; List[Dict]:\n        return [\n            item for item in data \n            if item.get(\"value\", 0) &gt; self.config.min_value\n        ]\n</code></pre>"},{"location":"getting-started/concepts/#pipelines","title":"Pipelines","text":"<p>Pipelines compose multiple pipes into a single processing unit:</p> <pre><code># Create pipes\nfilter_pipe = FilterPipe(FilterConfig(min_value=100))\ntransform_pipe = TransformPipe(TransformConfig())\n\n# Create pipeline using &gt;&gt; operator\npipeline = filter_pipe &gt;&gt; transform_pipe\n\n# Process data\nresult = await pipeline(data)\n</code></pre>"},{"location":"getting-started/concepts/#configuration","title":"Configuration","text":"<p>All pipes can be configured using Pydantic models:</p> <pre><code>from rivusio import PipeConfig\n\nclass ProcessorConfig(PipeConfig):\n    batch_size: int = 100\n    timeout: float = 30.0\n    retries: int = 3\n</code></pre>"},{"location":"getting-started/concepts/#error-handling","title":"Error Handling","text":"<p>Rivusio provides comprehensive error handling:</p> <pre><code>from rivusio import PipeError\n\ntry:\n    result = await pipeline(data)\nexcept PipeError as e:\n    print(f\"Error in {e.pipe}: {e.error}\")\n</code></pre>"},{"location":"getting-started/concepts/#parallel-processing","title":"Parallel Processing","text":"<p>Process data in parallel using execution strategies:</p> <pre><code>from rivusio.core.parallel import ExecutionStrategy\n\n# Configure parallel execution\npipeline.configure_parallel(\n    strategy=ExecutionStrategy.THREAD_POOL,\n    max_workers=4\n)\n\n# Process batch in parallel\nasync with pipeline:\n    results = await pipeline.execute_parallel(batch_data)\n</code></pre>"},{"location":"getting-started/concepts/#monitoring","title":"Monitoring","text":"<p>Monitor pipeline performance:</p> <p>```python</p>"},{"location":"getting-started/concepts/#get-pipeline-metrics","title":"Get pipeline metrics","text":"<p>metrics = pipeline.get_metrics()</p>"},{"location":"getting-started/concepts/#analyze-performance","title":"Analyze performance","text":"<p>for pipe_name, stats in metrics.items():     print(f\"Pipe: {pipe_name}\")     print(f\"Processing time: {stats['processing_time']}\")     print(f\"Items processed: {stats['item_count']}\")</p>"},{"location":"getting-started/installation/","title":"Installation","text":""},{"location":"getting-started/installation/#requirements","title":"Requirements","text":"<ul> <li>Python 3.10 or higher</li> <li>Poetry (recommended) or pip</li> </ul>"},{"location":"getting-started/installation/#using-poetry-recommended","title":"Using Poetry (Recommended)","text":"<pre><code>poetry add rivusio\n</code></pre>"},{"location":"getting-started/installation/#using-pip","title":"Using pip","text":"<pre><code>pip install rivusio\n</code></pre>"},{"location":"getting-started/installation/#development-installation","title":"Development Installation","text":"<p>To install Rivusio for development:</p> <ol> <li> <p>Clone the repository: <pre><code>git clone https://github.com/zbytealchemy/rivusio.git\ncd rivusio\n</code></pre></p> </li> <li> <p>Install dependencies using Poetry: <pre><code>poetry install\n</code></pre></p> </li> <li> <p>Install pre-commit hooks (optional but recommended): <pre><code>pre-commit install\n</code></pre></p> </li> </ol>"},{"location":"getting-started/installation/#verifying-installation","title":"Verifying Installation","text":"<p>After installation, you can verify everything is working by running the tests:</p> <pre><code>poetry run pytest tests/\n</code></pre> <p>For more detailed testing options, see our Development Testing Guide.</p>"},{"location":"getting-started/quickstart/","title":"Quick Start","text":"<p>This guide will help you get started with Rivusio by walking through a simple example.</p>"},{"location":"getting-started/quickstart/#installation","title":"Installation","text":"<p>Install Rivusio using pip:</p> <pre><code>pip install rivusio\n</code></pre>"},{"location":"getting-started/quickstart/#basic-usage","title":"Basic Usage","text":""},{"location":"getting-started/quickstart/#1-create-a-simple-pipe","title":"1. Create a Simple Pipe","text":"<p>Let's create a pipe that filters dictionary values:</p> <pre><code>from rivusio import BasePipe\nfrom typing import Dict\n\nclass FilterPipe(BasePipe[Dict, Dict]):\n    async def process(self, data: Dict) -&gt; Dict:\n        return {k: v for k, v in data.items() if v &gt; 100}\n</code></pre>"},{"location":"getting-started/quickstart/#2-process-data","title":"2. Process Data","text":"<p>Use the pipe to process some data:</p> <pre><code>filter_pipe = FilterPipe()\n\ndata = {\"a\": 150, \"b\": 50, \"c\": 200}\nresult = await filter_pipe(data)\nprint(result)  # {\"a\": 150, \"c\": 200}\n</code></pre>"},{"location":"getting-started/quickstart/#3-compose-pipes","title":"3. Compose Pipes","text":"<p>Create a pipeline by composing multiple pipes:</p> <pre><code>class TransformPipe(BasePipe[Dict, List]):\n    async def process(self, data: Dict) -&gt; List:\n        return list(data.values())\n\n# Compose pipes using &gt;&gt;\npipeline = FilterPipe() &gt;&gt; TransformPipe()\n\nresult = await pipeline(data)\nprint(result)  # [150, 200]\n</code></pre>"},{"location":"getting-started/quickstart/#4-error-handling","title":"4. Error Handling","text":"<p>Add error handling to your pipe:</p> <pre><code>from rivusio import PipeError\n\nclass SafeFilterPipe(BasePipe[Dict, Dict]):\n    async def process(self, data: Dict) -&gt; Dict:\n        try:\n            return {k: v for k, v in data.items() if v &gt; 100}\n        except Exception as e:\n            raise PipeError(self, e)\n\n    async def handle_error(self, error: Exception, data: Dict) -&gt; Dict:\n        return {}\n</code></pre>"},{"location":"getting-started/quickstart/#5-process-streams","title":"5. Process Streams","text":"<p>Process a stream of data:</p> <pre><code>from rivusio.streams import Stream\n\nasync def data_source():\n    data = [\n        {\"a\": 150, \"b\": 50},\n        {\"c\": 200, \"d\": 75},\n        {\"e\": 300, \"f\": 100}\n    ]\n    for item in data:\n        yield item\n\nstream = Stream(data_source())\n\nasync for result in stream.process(pipeline):\n    print(result)\n</code></pre>"},{"location":"getting-started/quickstart/#next-steps","title":"Next Steps","text":"<ul> <li>Learn about Basic Concepts</li> <li>Explore Error Handling</li> <li>Check out Stream Processing</li> <li>Browse more Examples</li> </ul>"},{"location":"user-guide/benchmarking/","title":"Benchmarking Guide","text":"<p>This guide provides information about benchmarking Rivusio pipelines and understanding performance characteristics.</p>"},{"location":"user-guide/benchmarking/#benchmark-suite","title":"Benchmark Suite","text":""},{"location":"user-guide/benchmarking/#1-basic-benchmarking-tool","title":"1. Basic Benchmarking Tool","text":"<pre><code>import time\nimport asyncio\nfrom dataclasses import dataclass\nfrom typing import Any, List, Dict\nfrom rivusio import AsyncPipeline\nfrom rivusio.monitoring import PipelineMonitor\n\n@dataclass\nclass BenchmarkResult:\n    total_time: float\n    records_processed: int\n    average_latency: float\n    throughput: float\n    memory_usage: Dict[str, float]\n    cpu_usage: float\n\nasync def benchmark_pipeline(\n    pipeline: AsyncPipeline,\n    data: List[Any],\n    warmup_iterations: int = 3\n) -&gt; BenchmarkResult:\n    # Warmup\n    for _ in range(warmup_iterations):\n        async with pipeline:\n            await pipeline.process(data[:100])\n\n    # Actual benchmark\n    start_time = time.time()\n    start_memory = psutil.Process().memory_info().rss / 1024 / 1024\n\n    async with pipeline:\n        results = await pipeline.process(data)\n\n    end_time = time.time()\n    end_memory = psutil.Process().memory_info().rss / 1024 / 1024\n\n    duration = end_time - start_time\n\n    return BenchmarkResult(\n        total_time=duration,\n        records_processed=len(data),\n        average_latency=pipeline.monitor.get_metrics().average_latency,\n        throughput=len(data) / duration,\n        memory_usage={\n            \"start\": start_memory,\n            \"end\": end_memory,\n            \"diff\": end_memory - start_memory\n        },\n        cpu_usage=psutil.Process().cpu_percent()\n    )\n</code></pre>"},{"location":"user-guide/benchmarking/#benchmark-results","title":"Benchmark Results","text":"<p>Below are benchmark results for common operations on different data sizes and configurations.</p>"},{"location":"user-guide/benchmarking/#1-basic-pipeline-performance","title":"1. Basic Pipeline Performance","text":"Operation Type Data Size Batch Size Throughput (items/s) Latency (ms) Memory (MB) Simple Transform 10K 100 5,000 0.2 45 Simple Transform 10K 1000 8,000 0.125 52 Simple Transform 100K 1000 12,000 0.083 124"},{"location":"user-guide/benchmarking/#2-complex-pipeline-performance","title":"2. Complex Pipeline Performance","text":"Operation Type Data Size Batch Size Throughput (items/s) Latency (ms) Memory (MB) JSON Processing 10K 100 2,000 0.5 78 JSON Processing 10K 1000 3,500 0.286 86 JSON Processing 100K 1000 4,000 0.25 156"},{"location":"user-guide/benchmarking/#3-io-bound-operations","title":"3. I/O Bound Operations","text":"Operation Type Concurrent Requests Batch Size Throughput (req/s) Latency (ms) Memory (MB) HTTP Requests 10 1 100 10 45 HTTP Requests 50 1 400 2.5 52 HTTP Requests 100 1 600 1.67 68"},{"location":"user-guide/benchmarking/#performance-characteristics","title":"Performance Characteristics","text":""},{"location":"user-guide/benchmarking/#1-memory-usage","title":"1. Memory Usage","text":"<pre><code>async def measure_memory_usage(pipeline: AsyncPipeline, data_sizes: List[int]):\n    results = []\n    for size in data_sizes:\n        data = generate_test_data(size)\n        start_mem = psutil.Process().memory_info().rss\n\n        async with pipeline:\n            await pipeline.process(data)\n\n        end_mem = psutil.Process().memory_info().rss\n        results.append({\n            \"size\": size,\n            \"memory_used\": end_mem - start_mem\n        })\n    return results\n</code></pre>"},{"location":"user-guide/benchmarking/#2-cpu-usage","title":"2. CPU Usage","text":"<pre><code>async def measure_cpu_usage(pipeline: AsyncPipeline, duration: int):\n    start_time = time.time()\n    cpu_samples = []\n\n    async def monitor_cpu():\n        while time.time() - start_time &lt; duration:\n            cpu_samples.append(psutil.Process().cpu_percent())\n            await asyncio.sleep(0.1)\n\n    async def run_pipeline():\n        async with pipeline:\n            while time.time() - start_time &lt; duration:\n                await pipeline.process(generate_test_data(1000))\n\n    await asyncio.gather(monitor_cpu(), run_pipeline())\n    return {\n        \"average_cpu\": sum(cpu_samples) / len(cpu_samples),\n        \"max_cpu\": max(cpu_samples),\n        \"min_cpu\": min(cpu_samples)\n    }\n</code></pre>"},{"location":"user-guide/benchmarking/#scaling-characteristics","title":"Scaling Characteristics","text":""},{"location":"user-guide/benchmarking/#1-vertical-scaling","title":"1. Vertical Scaling","text":"CPU Cores Memory (GB) Throughput Improvement 2 4 1x (baseline) 4 8 1.8x 8 16 3.2x 16 32 5.5x"},{"location":"user-guide/benchmarking/#2-batch-size-impact","title":"2. Batch Size Impact","text":"Batch Size Throughput (items/s) Memory Overhead Latency Impact 10 1,000 Low Minimal 100 5,000 Medium Low 1000 12,000 High Medium 10000 20,000 Very High High"},{"location":"user-guide/benchmarking/#best-practices","title":"Best Practices","text":""},{"location":"user-guide/benchmarking/#1-choosing-batch-sizes","title":"1. Choosing Batch Sizes","text":"<pre><code>def recommend_batch_size(\n    data_size: int,\n    memory_limit: int,\n    target_latency: float\n) -&gt; int:\n    \"\"\"\n    Recommend optimal batch size based on constraints.\n\n    Args:\n        data_size: Total number of records\n        memory_limit: Maximum memory in MB\n        target_latency: Target processing latency in ms\n\n    Returns:\n        Recommended batch size\n    \"\"\"\n    # Implementation depends on your specific use case\n    base_size = 1000\n    memory_factor = memory_limit / 1000\n    latency_factor = target_latency / 10\n\n    return min(\n        base_size * memory_factor,\n        base_size * latency_factor,\n        data_size\n    )\n</code></pre>"},{"location":"user-guide/benchmarking/#2-resource-allocation","title":"2. Resource Allocation","text":"<pre><code>def calculate_resources(\n    throughput_target: int,\n    avg_record_size: int\n) -&gt; Dict[str, int]:\n    \"\"\"\n    Calculate required resources for target throughput.\n\n    Args:\n        throughput_target: Target records per second\n        avg_record_size: Average record size in bytes\n\n    Returns:\n        Dictionary with recommended resources\n    \"\"\"\n    return {\n        \"cpu_cores\": max(2, throughput_target // 1000),\n        \"memory_mb\": max(\n            1024,\n            (throughput_target * avg_record_size * 2) // (1024 * 1024)\n        ),\n        \"batch_size\": max(100, throughput_target // 100)\n    }\n</code></pre>"},{"location":"user-guide/benchmarking/#running-benchmarks","title":"Running Benchmarks","text":""},{"location":"user-guide/benchmarking/#1-quick-benchmark","title":"1. Quick Benchmark","text":"<pre><code>async def quick_benchmark(pipeline: AsyncPipeline):\n    \"\"\"Run a quick benchmark to get baseline performance.\"\"\"\n    monitor = PipelineMonitor()\n    pipeline.set_monitor(monitor)\n\n    data = generate_test_data(10000)\n\n    async with pipeline:\n        start = time.time()\n        await pipeline.process(data)\n        duration = time.time() - start\n\n    metrics = monitor.get_metrics()\n    return {\n        \"throughput\": len(data) / duration,\n        \"latency\": metrics.average_latency,\n        \"memory\": psutil.Process().memory_info().rss / 1024 / 1024\n    }\n</code></pre>"},{"location":"user-guide/benchmarking/#2-comprehensive-benchmark","title":"2. Comprehensive Benchmark","text":"<pre><code>async def comprehensive_benchmark(\n    pipeline: AsyncPipeline,\n    configs: List[Dict[str, Any]]\n):\n    \"\"\"Run comprehensive benchmarks with different configurations.\"\"\"\n    results = []\n\n    for config in configs:\n        pipeline.configure(config)\n        result = await benchmark_pipeline(\n            pipeline,\n            generate_test_data(config[\"data_size\"]),\n            warmup_iterations=3\n        )\n        results.append({\n            \"config\": config,\n            \"results\": result\n        })\n\n    return results\n</code></pre>"},{"location":"user-guide/benchmarking/#interpreting-results","title":"Interpreting Results","text":"<ol> <li>Throughput Analysis</li> <li>Linear scaling with CPU cores up to 8 cores</li> <li>Diminishing returns beyond 8 cores</li> <li> <p>Batch size has significant impact</p> </li> <li> <p>Memory Usage</p> </li> <li>Grows linearly with batch size</li> <li>Spikes during data transformation</li> <li> <p>GC impact on large datasets</p> </li> <li> <p>Latency Patterns</p> </li> <li>Increases with batch size</li> <li>Network I/O dominates for small batches</li> <li>CPU processing dominates for large batches</li> </ol>"},{"location":"user-guide/configuration/","title":"Configuration Management","text":""},{"location":"user-guide/configuration/#using-pipeconfig","title":"Using PipeConfig","text":"<p>Define type-safe configurations:</p> <pre><code>from rivusio import PipeConfig, ConfigurablePipe\nfrom pydantic import Field, validator\n\nclass BatchConfig(PipeConfig):\n    size: int = Field(ge=1, le=1000)\n    timeout: int = Field(ge=0)\n\n    @validator(\"timeout\")\n    def validate_timeout(cls, v, values):\n        if v == 0 and values[\"size\"] &gt; 100:\n            raise ValueError(\"Timeout required for large batch sizes\")\n        return v\n\nclass BatchProcessor(ConfigurablePipe[List[Dict], Dict, BatchConfig]):\n    async def process(self, data: List[Dict]) -&gt; Dict:\n        if len(data) &gt; self.config.size:\n            raise ValueError(\"Batch size exceeded\")\n        return await self.process_batch(data)\n</code></pre>"},{"location":"user-guide/configuration/#environment-variables","title":"Environment Variables","text":"<pre><code>class ApiConfig(PipeConfig):\n    api_key: str = Field(..., env='API_KEY')\n    endpoint: str = Field(..., env='API_ENDPOINT')\n    timeout: int = Field(default=30, env='API_TIMEOUT')\n</code></pre>"},{"location":"user-guide/configuration/#nested-configuration","title":"Nested Configuration","text":"<p>Create complex configurations:</p> <pre><code>class KafkaConfig(PipeConfig):\n    bootstrap_servers: List[str]\n    topic: str\n    group_id: str\n\nclass ProcessingConfig(PipeConfig):\n    batch_size: int = 100\n    timeout: int = 30\n    kafka: KafkaConfig\n\nconfig = ProcessingConfig(\n    batch_size=200,\n    kafka=KafkaConfig(\n        bootstrap_servers=[\"localhost:9092\"],\n        topic=\"my-topic\",\n        group_id=\"my-group\"\n    )\n)\n</code></pre>"},{"location":"user-guide/configuration/#validation","title":"Validation","text":"<p>Add validation rules:</p> <pre><code>from pydantic import validator\n\nclass BatchConfig(PipeConfig):\n    size: int = Field(ge=1, le=1000)\n    timeout: int = Field(ge=0)\n\n    @validator(\"timeout\")\n    def validate_timeout(cls, v, values):\n        if v == 0 and values[\"size\"] &gt; 100:\n            raise ValueError(\"Timeout required for large batch sizes\")\n        return v\n</code></pre>"},{"location":"user-guide/configuration/#best-practices","title":"Best Practices","text":"<ol> <li>Use type hints for all fields</li> <li>Provide sensible defaults</li> <li>Add field descriptions</li> <li>Use environment variables for sensitive data</li> <li>Validate configuration values</li> </ol>"},{"location":"user-guide/error-handling/","title":"Error Handling","text":""},{"location":"user-guide/error-handling/#overview","title":"Overview","text":"<p>Rivusio provides comprehensive error handling capabilities to help you build robust data processing pipelines.</p>"},{"location":"user-guide/error-handling/#error-types","title":"Error Types","text":""},{"location":"user-guide/error-handling/#pipeerror","title":"PipeError","text":"<p>Base error type for pipe-specific errors:</p> <pre><code>from rivusio.exceptions import PipeError\n\ntry:\n    result = await pipe.process(data)\nexcept PipeError as e:\n    print(f\"Error in pipe {e.pipe}: {e.error}\")\n</code></pre>"},{"location":"user-guide/error-handling/#pipelineerror","title":"PipelineError","text":"<p>Error type for pipeline-level errors:</p> <pre><code>from rivusio.exceptions import PipelineError\n\ntry:\n    result = await pipeline.process(data)\nexcept PipelineError as e:\n    print(f\"Pipeline error: {e}\")\n    print(f\"Failed pipe: {e.pipe}\")\n    print(f\"Original error: {e.original_error}\")\n</code></pre>"},{"location":"user-guide/error-handling/#retry-mechanism","title":"Retry Mechanism","text":"<p>Configure retries for transient errors:</p> <pre><code>from rivusio import RetryConfig\n\nconfig = RetryConfig(\n    max_retries=3,\n    retry_delay=5,\n    backoff_factor=2,\n    exceptions=[TransientError]\n)\n\npipe = MyPipe(config=config)\n</code></pre>"},{"location":"user-guide/error-handling/#error-recovery","title":"Error Recovery","text":"<p>Implement custom error recovery:</p> <pre><code>class MyPipe(ConfigurablePipe[Dict, Dict]):\n    async def process(self, data: Dict) -&gt; Dict:\n        try:\n            return await self._process_data(data)\n        except ValueError as e:\n            # Handle value error\n            return self._handle_value_error(data, e)\n        except KeyError as e:\n            # Handle key error\n            return self._handle_key_error(data, e)\n</code></pre>"},{"location":"user-guide/error-handling/#dead-letter-queues","title":"Dead Letter Queues","text":"<p>Handle unprocessable messages:</p> <pre><code>class DeadLetterConfig(PipeConfig):\n    queue_url: str\n\nclass ProcessingPipe(ConfigurablePipe[Dict, Dict]):\n    def __init__(self, config: PipeConfig, dlq_config: DeadLetterConfig):\n        super().__init__(config)\n        self.dlq = DeadLetterQueue(dlq_config)\n\n    async def process(self, data: Dict) -&gt; Dict:\n        try:\n            return await self._process_data(data)\n        except UnprocessableError as e:\n            await self.dlq.send(data, error=e)\n            return None\n</code></pre>"},{"location":"user-guide/error-handling/#error-monitoring","title":"Error Monitoring","text":"<p>Monitor errors in your pipeline:</p> <pre><code>from rivusio.monitoring import PipelineMonitor\n\nmonitor = PipelineMonitor()\npipeline.set_error_monitor(monitor)\n\n# Process data\nawait pipeline.process(data)\n\n# Get error metrics\nerror_metrics = monitor.get_metrics()\nprint(f\"Total errors: {error_metrics['error_count']}\")\nprint(f\"Error rate: {error_metrics['error_rate']}\")\n</code></pre>"},{"location":"user-guide/error-handling/#best-practices","title":"Best Practices","text":"<ol> <li>Use appropriate error types</li> <li>Implement retries for transient errors</li> <li>Add proper error logging</li> <li>Monitor error rates</li> <li>Handle unprocessable messages with DLQ</li> <li>Document error handling behavior</li> </ol>"},{"location":"user-guide/monitoring/","title":"Monitoring","text":""},{"location":"user-guide/monitoring/#overview","title":"Overview","text":"<p>Rivusio provides built-in monitoring capabilities for tracking pipeline performance metrics. The monitoring system supports:</p> <ul> <li>Processing time tracking</li> <li>Error rate monitoring</li> <li>Throughput measurement</li> <li>Time-windowed metrics collection</li> </ul>"},{"location":"user-guide/monitoring/#basic-metrics-collection","title":"Basic Metrics Collection","text":"<p>Use <code>MetricsCollector</code> for basic metrics collection:</p> <pre><code>from datetime import timedelta\nfrom rivusio.monitoring import MetricsCollector\n\n# Create collector with 5-minute window\ncollector = MetricsCollector(window_size=timedelta(minutes=5))\n\n# Record metrics\ncollector.record_processing_time(0.123)  # 123ms processing time\ncollector.record_success()  # Record successful operation\ncollector.record_throughput(100)  # Processed 100 items\n\n# Get metrics\nmetrics = collector.get_metrics()\nprint(f\"Avg processing time: {metrics['processing_time']['avg']:.3f}\")\nprint(f\"Error rate: {metrics['error_rate']['avg']:.1f}\")\nprint(f\"Throughput: {metrics['throughput']['avg']:.0f}\")\n</code></pre>"},{"location":"user-guide/monitoring/#pipeline-monitoring","title":"Pipeline Monitoring","text":"<p>Use <code>PipelineMonitor</code> for pipeline-specific monitoring:</p> <pre><code>from rivusio.monitoring import PipelineMonitor\n\n# Create and attach monitor\nmonitor = PipelineMonitor(window_size=timedelta(minutes=10))\npipeline.set_monitor(monitor)\n\n# Start monitoring\nmonitor.start()\n\ntry:\n    # Process data\n    await pipeline.process(data)\n    monitor.record_success()\nexcept Exception as e:\n    monitor.record_error()\n    raise\nfinally:\n    monitor.stop()\n\n# Get metrics\nmetrics = monitor.get_metrics()\nprint(f\"Total time: {metrics['total_time']:.2f}s\")\nprint(f\"Error rate: {metrics['error_rate']['avg']:.2%}\")\n</code></pre>"},{"location":"user-guide/monitoring/#metric-types","title":"Metric Types","text":""},{"location":"user-guide/monitoring/#processing-time","title":"Processing Time","text":"<p>Tracks how long operations take:</p> <pre><code># Record processing duration\ncollector.record_processing_time(duration_seconds)\n\n# Get processing time metrics\nmetrics = collector.get_metrics()\nprocessing_stats = metrics['processing_time']\nprint(f\"Average: {processing_stats['avg']:.3f}s\")\nprint(f\"Maximum: {processing_stats['max']:.3f}s\")\nprint(f\"Latest: {processing_stats['latest']:.3f}s\")\n</code></pre>"},{"location":"user-guide/monitoring/#error-rate","title":"Error Rate","text":"<p>Tracks success/failure ratio:</p> <pre><code># Record outcomes\ntry:\n    # Process data\n    collector.record_success()\nexcept Exception:\n    collector.record_error()\n    raise\n\n# Get error rate metrics\nmetrics = collector.get_metrics()\nerror_stats = metrics['error_rate']\nprint(f\"Error rate: {error_stats['avg']:.2%}\")\n</code></pre>"},{"location":"user-guide/monitoring/#throughput","title":"Throughput","text":"<p>Tracks items processed per time window:</p> <pre><code># Record batch processing\ncollector.record_throughput(items_processed)\n\n# Get throughput metrics\nmetrics = collector.get_metrics()\nthroughput_stats = metrics['throughput']\nprint(f\"Average throughput: {throughput_stats['avg']:.0f} items\")\nprint(f\"Peak throughput: {throughput_stats['max']:.0f} items\")\n</code></pre>"},{"location":"user-guide/monitoring/#time-windows","title":"Time Windows","text":"<p>Metrics are collected in sliding time windows:</p> <pre><code># Create collector with custom window\ncollector = MetricsCollector(\n    window_size=timedelta(minutes=15)  # 15-minute window\n)\n\n# Values older than window_size are automatically discarded\n# Each metric type (processing_time, error_rate, throughput)\n# maintains its own window\n</code></pre>"},{"location":"user-guide/monitoring/#best-practices","title":"Best Practices","text":"<ol> <li>Choose appropriate window sizes:</li> <li>Shorter windows (1-5 minutes) for real-time monitoring</li> <li> <p>Longer windows (15-60 minutes) for trend analysis</p> </li> <li> <p>Monitor multiple aspects:</p> </li> <li>Processing time for performance</li> <li>Error rates for reliability</li> <li> <p>Throughput for capacity planning</p> </li> <li> <p>Use pipeline monitors for end-to-end visibility:</p> </li> <li>Start/stop timing around pipeline execution</li> <li>Track success/failure at pipeline level</li> <li> <p>Monitor overall throughput</p> </li> <li> <p>Clean up resources:</p> </li> <li>Stop monitors when done</li> <li>Clear old metrics if needed</li> <li>Use monitors in context managers when possible</li> </ol>"},{"location":"user-guide/performance-tuning/","title":"Performance Tuning Guide","text":"<p>This guide provides comprehensive information about optimizing Rivusio for maximum performance.</p>"},{"location":"user-guide/performance-tuning/#key-performance-metrics","title":"Key Performance Metrics","text":"<ol> <li>Throughput: Records processed per second</li> <li>Latency: Time to process individual records</li> <li>Resource Usage: CPU, memory, and I/O utilization</li> <li>Backpressure: Buffer utilization and blocking time</li> </ol>"},{"location":"user-guide/performance-tuning/#configuration-optimization","title":"Configuration Optimization","text":""},{"location":"user-guide/performance-tuning/#1-stream-configuration","title":"1. Stream Configuration","text":"<pre><code>from datetime import timedelta\nfrom rivusio.streams import StreamConfig\n\nconfig = StreamConfig(\n    batch_size=1000,              # Adjust based on data size\n    window_size=timedelta(seconds=30),\n    buffer_size=5000,             # Control memory usage\n    max_retries=3,                # Balance reliability vs performance\n)\n</code></pre>"},{"location":"user-guide/performance-tuning/#batch-size-optimization","title":"Batch Size Optimization","text":"<ul> <li>Small batches (100-1000): Lower latency, good for real-time</li> <li>Large batches (1000+): Higher throughput, good for bulk processing</li> <li>Monitor memory usage to find optimal size</li> </ul>"},{"location":"user-guide/performance-tuning/#2-pipeline-configuration","title":"2. Pipeline Configuration","text":"<pre><code>from rivusio import AsyncPipeline\nfrom rivusio.monitoring import PipelineMonitor\n\npipeline = AsyncPipeline()\nmonitor = PipelineMonitor(window_size=timedelta(minutes=5))\npipeline.set_monitor(monitor)\n\n# Enable parallel processing where possible\npipeline.parallel_workers = 4  # Adjust based on CPU cores\n</code></pre>"},{"location":"user-guide/performance-tuning/#memory-management","title":"Memory Management","text":""},{"location":"user-guide/performance-tuning/#1-memory-profiling","title":"1. Memory Profiling","text":"<pre><code>from memory_profiler import profile\n\n@profile\nasync def process_data():\n    async with pipeline:\n        await pipeline.process(data)\n</code></pre>"},{"location":"user-guide/performance-tuning/#2-memory-optimization-techniques","title":"2. Memory Optimization Techniques","text":"<pre><code># Use generators for large datasets\nasync def data_generator():\n    async for chunk in large_dataset:\n        yield chunk\n\n# Stream processing with windowing\nstream = AsyncStream(\n    data_generator(),\n    config=StreamConfig(window_size=timedelta(seconds=30))\n)\n</code></pre>"},{"location":"user-guide/performance-tuning/#cpu-optimization","title":"CPU Optimization","text":""},{"location":"user-guide/performance-tuning/#1-parallel-processing","title":"1. Parallel Processing","text":"<pre><code># Configure parallel execution\npipeline.parallel_execution = True\npipeline.max_workers = min(32, (os.cpu_count() or 1) * 2)\n\n# Process data in parallel\nresults = await pipeline.execute_parallel(data_batch)\n</code></pre>"},{"location":"user-guide/performance-tuning/#2-cpu-profiling","title":"2. CPU Profiling","text":"<pre><code>import cProfile\nimport pstats\n\nprofiler = cProfile.Profile()\nprofiler.enable()\n# Your code here\nprofiler.disable()\nstats = pstats.Stats(profiler).sort_stats('cumulative')\nstats.print_stats()\n</code></pre>"},{"location":"user-guide/performance-tuning/#io-optimization","title":"I/O Optimization","text":""},{"location":"user-guide/performance-tuning/#1-async-io","title":"1. Async I/O","text":"<pre><code>class AsyncIOPipe(AsyncBasePipe[Data, Result]):\n    async def process(self, data: Data) -&gt; Result:\n        async with aiohttp.ClientSession() as session:\n            async with session.get(url) as response:\n                return await response.json()\n</code></pre>"},{"location":"user-guide/performance-tuning/#2-buffering","title":"2. Buffering","text":"<pre><code>from rivusio.streams import BufferedStreamConfig\n\nconfig = BufferedStreamConfig(\n    buffer_size=1000,\n    flush_interval=timedelta(seconds=5)\n)\n</code></pre>"},{"location":"user-guide/performance-tuning/#monitoring-and-metrics","title":"Monitoring and Metrics","text":""},{"location":"user-guide/performance-tuning/#1-performance-metrics-collection","title":"1. Performance Metrics Collection","text":"<pre><code>from rivusio.monitoring import MetricsCollector\n\nmetrics = MetricsCollector()\nmetrics.record_timing(\"processing_time\", 1.5)\nmetrics.record_count(\"records_processed\", 1000)\n</code></pre>"},{"location":"user-guide/performance-tuning/#2-real-time-monitoring","title":"2. Real-time Monitoring","text":"<pre><code>async def monitor_performance():\n    while True:\n        metrics = pipeline.monitor.get_metrics()\n        print(f\"Throughput: {metrics.throughput}/s\")\n        print(f\"Latency: {metrics.latency}ms\")\n        await asyncio.sleep(1)\n</code></pre>"},{"location":"user-guide/performance-tuning/#performance-testing","title":"Performance Testing","text":""},{"location":"user-guide/performance-tuning/#1-load-testing","title":"1. Load Testing","text":"<pre><code>async def load_test(pipeline: AsyncPipeline, data_size: int):\n    start_time = time.time()\n    data = generate_test_data(data_size)\n\n    async with pipeline:\n        result = await pipeline.process(data)\n\n    duration = time.time() - start_time\n    print(f\"Processed {data_size} records in {duration:.2f}s\")\n    print(f\"Throughput: {data_size/duration:.2f} records/s\")\n</code></pre>"},{"location":"user-guide/performance-tuning/#2-stress-testing","title":"2. Stress Testing","text":"<pre><code>async def stress_test(pipeline: AsyncPipeline, duration: int):\n    end_time = time.time() + duration\n    records_processed = 0\n\n    while time.time() &lt; end_time:\n        data = generate_continuous_data()\n        await pipeline.process(data)\n        records_processed += len(data)\n</code></pre>"},{"location":"user-guide/performance-tuning/#best-practices","title":"Best Practices","text":"<ol> <li>Data Size Management</li> <li>Use streaming for large datasets</li> <li>Implement windowing for time-series data</li> <li> <p>Buffer data appropriately</p> </li> <li> <p>Resource Management</p> </li> <li>Use context managers for cleanup</li> <li>Implement proper error handling</li> <li> <p>Monitor system resources</p> </li> <li> <p>Optimization Strategy</p> </li> <li>Profile before optimizing</li> <li>Test with production-like data</li> <li>Monitor real-world performance</li> </ol>"},{"location":"user-guide/performance-tuning/#performance-troubleshooting","title":"Performance Troubleshooting","text":"<ol> <li>High Latency</li> <li>Reduce batch size</li> <li>Check for I/O bottlenecks</li> <li> <p>Monitor system resources</p> </li> <li> <p>Memory Issues</p> </li> <li>Implement proper cleanup</li> <li>Use generators for large datasets</li> <li> <p>Monitor memory usage</p> </li> <li> <p>CPU Bottlenecks</p> </li> <li>Enable parallel processing</li> <li>Optimize compute-heavy operations</li> <li>Profile CPU usage</li> </ol>"},{"location":"user-guide/performance-tuning/#performance-tuning","title":"Performance Tuning","text":""},{"location":"user-guide/performance-tuning/#overview","title":"Overview","text":"<p>Rivusio provides several ways to optimize pipeline performance: 1. Parallel execution strategies 2. Pipeline composition 3. Resource management 4. Memory optimization</p>"},{"location":"user-guide/performance-tuning/#parallel-execution","title":"Parallel Execution","text":""},{"location":"user-guide/performance-tuning/#execution-strategies","title":"Execution Strategies","text":"<p>Choose the right execution strategy based on your workload:</p> <pre><code>from rivusio.core.parallel import ExecutionStrategy\n\n# I/O-bound tasks (default)\npipeline.configure_parallel(ExecutionStrategy.GATHER)\n\n# Mixed I/O and light CPU tasks\npipeline.configure_parallel(\n    ExecutionStrategy.THREAD_POOL,\n    max_workers=4  # Defaults to min(32, cpu_count + 4)\n)\n\n# CPU-intensive tasks (coming soon)\npipeline.configure_parallel(\n    ExecutionStrategy.PROCESS_POOL,\n    max_workers=8,  # Defaults to cpu_count\n    chunk_size=1000  # For batching items\n)\n</code></pre>"},{"location":"user-guide/performance-tuning/#when-to-use-each-strategy","title":"When to Use Each Strategy","text":"<ol> <li>GATHER</li> <li>Best for: I/O-bound tasks (network requests, file operations)</li> <li>Pros: Low overhead, efficient for async operations</li> <li> <p>Cons: No true parallelism, blocked by CPU-bound tasks</p> </li> <li> <p>THREAD_POOL</p> </li> <li>Best for: Mixed I/O and light CPU tasks</li> <li>Pros: True parallelism for I/O, handles blocking calls</li> <li> <p>Cons: Limited by GIL for CPU-intensive tasks</p> </li> <li> <p>PROCESS_POOL (coming soon)</p> </li> <li>Best for: CPU-intensive tasks</li> <li>Pros: True parallelism, bypasses GIL</li> <li>Cons: Higher overhead, data serialization costs</li> </ol>"},{"location":"user-guide/performance-tuning/#pipeline-composition","title":"Pipeline Composition","text":"<p>Optimize pipeline structure:</p> <ol> <li> <p>Minimize Pipeline Length <pre><code># Bad: Many small pipes\npipeline = pipe1 &gt;&gt; pipe2 &gt;&gt; pipe3 &gt;&gt; pipe4 &gt;&gt; pipe5\n\n# Better: Combine related operations\npipeline = preprocess &gt;&gt; transform &gt;&gt; postprocess\n</code></pre></p> </li> <li> <p>Balance Async/Sync Operations <pre><code># Bad: Mixing async/sync unnecessarily\npipeline = AsyncPipeline([\n    async_io_pipe,\n    sync_transform,  # Blocks event loop\n    async_io_pipe\n])\n\n# Better: Group sync operations\npipeline = AsyncPipeline([\n    async_io_pipe,\n    combined_sync_transform,  # Single sync block\n    async_io_pipe\n])\n</code></pre></p> </li> </ol>"},{"location":"user-guide/performance-tuning/#resource-management","title":"Resource Management","text":"<ol> <li> <p>Use Context Managers <pre><code>async with pipeline:\n    results = await pipeline.execute_parallel(items)\n# Resources automatically cleaned up\n</code></pre></p> </li> <li> <p>Configure Worker Pools <pre><code># Adjust based on system resources\npipeline.configure_parallel(\n    strategy=ExecutionStrategy.THREAD_POOL,\n    max_workers=min(32, cpu_count + 4)\n)\n</code></pre></p> </li> </ol>"},{"location":"user-guide/performance-tuning/#memory-optimization","title":"Memory Optimization","text":"<ol> <li> <p>Chunk Processing <pre><code># Process large datasets in chunks\npipeline.configure_parallel(\n    strategy=ExecutionStrategy.THREAD_POOL,\n    chunk_size=1000  # Adjust based on memory constraints\n)\n</code></pre></p> </li> <li> <p>Clean Up Intermediate Results <pre><code># Get outputs if needed, then clear\noutputs = pipeline.get_pipe_outputs(pipe)\npipeline._pipe_outputs[pipe].clear()  # Free memory\n</code></pre></p> </li> </ol>"},{"location":"user-guide/performance-tuning/#monitoring-performance","title":"Monitoring Performance","text":"<p>Use built-in metrics to identify bottlenecks:</p> <pre><code># Get performance metrics\nmetrics = pipeline.get_metrics()\n\n# Analyze pipe-specific metrics\nfor pipe_name, pipe_metrics in metrics.items():\n    print(f\"Pipe: {pipe_name}\")\n    print(f\"Processing time: {pipe_metrics['processing_time']}\")\n    print(f\"Items processed: {pipe_metrics['item_count']}\")\n</code></pre>"},{"location":"user-guide/performance-tuning/#best-practices_1","title":"Best Practices","text":"<ol> <li>Profile your workload to choose the right execution strategy</li> <li>Monitor memory usage with large datasets</li> <li>Use appropriate chunk sizes for parallel processing</li> <li>Clean up resources properly</li> <li>Keep pipelines focused and efficient</li> <li>Consider data locality and serialization costs</li> </ol>"},{"location":"user-guide/pipelines/","title":"Working with Pipelines","text":""},{"location":"user-guide/pipelines/#overview","title":"Overview","text":"<p>Pipelines in Rivusio are sequences of pipes that process data in a specific order. They can be linear, branching, or merging, allowing you to build complex data processing workflows.</p>"},{"location":"user-guide/pipelines/#creating-pipelines","title":"Creating Pipelines","text":"<p>There are several ways to create pipelines:</p>"},{"location":"user-guide/pipelines/#using-pipeline-classes","title":"Using Pipeline Classes","text":"<pre><code>from rivusio import AsyncPipeline, SyncPipeline, MixedPipeline\n\n# Create async pipeline\nasync_pipeline = AsyncPipeline([async_pipe1, async_pipe2])\n\n# Create sync pipeline \nsync_pipeline = SyncPipeline([sync_pipe1, sync_pipe2])\n\n# Create mixed pipeline\nmixed_pipeline = MixedPipeline([sync_pipe1, async_pipe2])\n</code></pre>"},{"location":"user-guide/pipelines/#using-the-operator","title":"Using the &gt;&gt; Operator","text":"<pre><code># Create sync pipeline\nsync_pipeline = sync_pipe1 &gt;&gt; sync_pipe2\n\n# Create async pipeline  \nasync_pipeline = async_pipe1 &gt;&gt; async_pipe2\n\n# Create mixed pipeline\nmixed_pipeline = sync_pipe1 &gt;&gt; async_pipe2\n</code></pre>"},{"location":"user-guide/pipelines/#pipeline-configuration","title":"Pipeline Configuration","text":"<p>Configure pipeline-wide settings:</p> <pre><code>from rivusio import PipelineConfig\n\nconfig = PipelineConfig(\n    max_retries=3,\n    retry_delay=5,\n    timeout=30\n)\n\npipeline = AsyncPipeline([pipe1, pipe2], name=\"MyPipeline\")\n</code></pre>"},{"location":"user-guide/pipelines/#parallel-execution","title":"Parallel Execution","text":"<p>Rivusio supports parallel execution of pipelines using different strategies:</p> <pre><code>from rivusio.core.parallel import ExecutionStrategy\n\n# Configure parallel execution\npipeline.configure_parallel(\n    strategy=ExecutionStrategy.THREAD_POOL,  # or GATHER for I/O-bound tasks\n    max_workers=4,  # Optional, defaults based on strategy\n    chunk_size=1000  # For process pool chunking\n)\n\n# Process items in parallel\nasync with pipeline:\n    results = await pipeline.execute_parallel(batch_of_items)\n</code></pre> <p>The available execution strategies are: - <code>GATHER</code>: Uses asyncio.gather for I/O-bound tasks (default) - <code>THREAD_POOL</code>: Uses ThreadPoolExecutor for I/O and light CPU tasks - <code>PROCESS_POOL</code>: Uses ProcessPoolExecutor for CPU-intensive tasks (currently using thread pool as temporary solution)</p>"},{"location":"user-guide/pipelines/#error-handling","title":"Error Handling","text":"<p>Handle errors at the pipeline level:</p> <pre><code>from rivusio.core.exceptions import PipelineError\n\ntry:\n    result = await pipeline.process(data)\nexcept PipelineError as e:\n    print(f\"Error in pipe {e.pipe}: {e.error}\")\n</code></pre>"},{"location":"user-guide/pipelines/#pipeline-metrics","title":"Pipeline Metrics","text":"<p>Monitor pipeline performance:</p> <pre><code># Get metrics for all pipes\nmetrics = pipeline.get_metrics()\n\n# Get outputs from a specific pipe\npipe_outputs = pipeline.get_pipe_outputs(specific_pipe)\n</code></pre>"},{"location":"user-guide/pipelines/#best-practices","title":"Best Practices","text":"<ol> <li>Use type hints to ensure type safety throughout the pipeline</li> <li>Choose the appropriate pipeline type (Sync/Async/Mixed) based on your pipes</li> <li>Use parallel execution for batch processing of independent items</li> <li>Configure error handling and monitoring for production use</li> <li>Clean up resources by using pipelines as async context managers</li> <li>Choose the right execution strategy based on your workload:</li> <li>GATHER for I/O-bound tasks</li> <li>THREAD_POOL for mixed I/O and light CPU tasks</li> <li>PROCESS_POOL for CPU-intensive tasks (coming soon)</li> </ol>"},{"location":"user-guide/pipes/","title":"Working with Pipes","text":""},{"location":"user-guide/pipes/#overview","title":"Overview","text":"<p>Pipes are the basic building blocks in Rivusio. Each pipe represents a single data transformation operation that can be composed into larger pipelines.</p>"},{"location":"user-guide/pipes/#types-of-pipes","title":"Types of Pipes","text":"<p>Rivusio provides three base pipe types:</p> <ol> <li><code>SyncBasePipe</code>: For synchronous operations</li> <li><code>AsyncBasePipe</code>: For asynchronous operations</li> <li><code>BasePipe</code>: Abstract base class for both sync and async pipes</li> </ol>"},{"location":"user-guide/pipes/#creating-a-pipe","title":"Creating a Pipe","text":"<p>Here's how to create a basic pipe:</p> <pre><code>from rivusio import AsyncBasePipe, PipeConfig\nfrom typing import Dict\n\nclass TransformConfig(PipeConfig):\n    \"\"\"Configuration for the transform pipe.\"\"\"\n    uppercase_fields: List[str] = [\"name\", \"title\"]\n\nclass TransformPipe(AsyncBasePipe[Dict, Dict]):\n    \"\"\"Transform specific fields to uppercase.\"\"\"\n\n    def __init__(self, config: TransformConfig) -&gt; None:\n        \"\"\"Initialize the pipe.\"\"\"\n        super().__init__()\n        self.config = config\n        self.name = config.name or self.__class__.__name__\n\n    async def process(self, data: Dict) -&gt; Dict:\n        \"\"\"Process the input data.\"\"\"\n        result = data.copy()\n        for field in self.config.uppercase_fields:\n            if field in result:\n                result[field] = result[field].upper()\n        return result\n\n# Create and use the pipe\nconfig = TransformConfig(name=\"my_transformer\")\npipe = TransformPipe(config)\nresult = await pipe.process({\"name\": \"john\", \"title\": \"developer\"})\n</code></pre>"},{"location":"user-guide/pipes/#configuration","title":"Configuration","text":"<p>Pipes can be configured using Pydantic models:</p> <pre><code>from pydantic import BaseModel\nfrom rivusio import PipeConfig\n\nclass MyPipeConfig(PipeConfig):\n    max_retries: int = 3\n    timeout: float = 5.0\n    batch_size: int = 100\n</code></pre>"},{"location":"user-guide/pipes/#error-handling","title":"Error Handling","text":"<p>Implement error handling in your pipe's process method:</p> <pre><code>from rivusio import AsyncBasePipe, PipeError\n\nclass ProcessingPipe(AsyncBasePipe[Dict, Dict]):\n    async def process(self, data: Dict) -&gt; Dict:\n        try:\n            # Process data\n            result = await self._process_data(data)\n            return result\n        except Exception as e:\n            raise PipeError(\n                pipe=self,\n                message=\"Failed to process data\",\n                error=e\n            ) from e\n</code></pre>"},{"location":"user-guide/pipes/#best-practices","title":"Best Practices","text":"<ol> <li>Always use type hints for input and output types</li> <li>Keep pipes focused on a single responsibility</li> <li>Handle errors appropriately using PipeError</li> <li>Use meaningful names and docstrings</li> <li>Make pipes configurable via PipeConfig</li> <li>Implement proper cleanup in aenter and aexit if needed</li> </ol>"},{"location":"user-guide/pipes/#composing-pipes","title":"Composing Pipes","text":"<p>Pipes can be composed using the &gt;&gt; operator:</p> <pre><code># Create pipeline\npipeline = filter_pipe &gt;&gt; transform_pipe &gt;&gt; output_pipe\n\n# Process data\nresult = await pipeline(data)\n</code></pre>"},{"location":"user-guide/pipes/#monitoring","title":"Monitoring","text":"<p>Monitor pipe performance using built-in metrics:</p> <p>```python</p>"},{"location":"user-guide/pipes/#get-pipe-metrics","title":"Get pipe metrics","text":"<p>metrics = pipe.get_metrics() print(f\"Processing time: {metrics['processing_time']}\") print(f\"Items processed: {metrics['item_count']}\")</p>"},{"location":"user-guide/plugins/","title":"Working with Plugins","text":""},{"location":"user-guide/plugins/#overview","title":"Overview","text":"<p>The plugin system in Rivusio allows you to extend the framework with custom sources and sinks. Sources are pipes that generate data (like reading from a database or message queue), while sinks are pipes that output data (like writing to a file or sending to an API).</p>"},{"location":"user-guide/plugins/#creating-plugins","title":"Creating Plugins","text":""},{"location":"user-guide/plugins/#async-source-example","title":"Async Source Example","text":"<pre><code>from rivusio.plugins import registry\nfrom rivusio.core.pipe import AsyncBasePipe\n\n@registry.register_async_source(\"kafka\")\nclass KafkaSource(AsyncBasePipe[None, dict]):\n    \"\"\"Source that reads messages from Kafka.\"\"\"\n\n    async def process(self, _: None) -&gt; dict:\n        # Read from Kafka\n        message = await self.consumer.get_message()\n        return message\n\n# Use the source\nsource = KafkaSource()\nasync for message in stream.process(source):\n    print(message)\n</code></pre>"},{"location":"user-guide/plugins/#sync-sink-example","title":"Sync Sink Example","text":"<pre><code>from rivusio.plugins import registry\nfrom rivusio.core.pipe import SyncBasePipe\n\n@registry.register_sync_sink(\"csv\")\nclass CsvSink(SyncBasePipe[dict, None]):\n    \"\"\"Sink that writes data to CSV files.\"\"\"\n\n    def process(self, data: dict) -&gt; None:\n        # Write to CSV\n        with open(self.filename, 'a') as f:\n            writer = csv.DictWriter(f, fieldnames=data.keys())\n            writer.writerow(data)\n\n# Use the sink\nsink = CsvSink(filename=\"output.csv\")\npipeline = transform_pipe &gt;&gt; sink\n</code></pre>"},{"location":"user-guide/plugins/#plugin-types","title":"Plugin Types","text":"<p>Rivusio supports four types of plugins:</p> <ol> <li>Async Sources (<code>@registry.register_async_source</code>)</li> <li>Generate data asynchronously</li> <li> <p>Good for I/O-bound sources like databases or APIs</p> </li> <li> <p>Async Sinks (<code>@registry.register_async_sink</code>)</p> </li> <li>Write data asynchronously</li> <li> <p>Useful for async APIs or message queues</p> </li> <li> <p>Sync Sources (<code>@registry.register_sync_source</code>)</p> </li> <li>Generate data synchronously</li> <li> <p>Good for file-based or in-memory sources</p> </li> <li> <p>Sync Sinks (<code>@registry.register_sync_sink</code>)</p> </li> <li>Write data synchronously</li> <li>Useful for files or simple outputs</li> </ol>"},{"location":"user-guide/plugins/#best-practices","title":"Best Practices","text":"<ol> <li>Choose the Right Base</li> <li>Use AsyncBasePipe for I/O-bound operations</li> <li> <p>Use SyncBasePipe for CPU-bound or simple operations</p> </li> <li> <p>Error Handling</p> </li> <li>Catch and handle source-specific errors</li> <li> <p>Convert to PipeError for consistent error handling</p> </li> <li> <p>Resource Management</p> </li> <li>Implement <code>__aenter__</code> and <code>__aexit__</code> for async resources</li> <li> <p>Implement <code>__enter__</code> and <code>__exit__</code> for sync resources</p> </li> <li> <p>Configuration</p> </li> <li>Use PipeConfig for plugin configuration</li> <li>Document configuration options clearly</li> </ol>"},{"location":"user-guide/plugins/#plugin-discovery","title":"Plugin Discovery","text":"<p>Plugins are discovered and registered at runtime when their module is imported. Make sure to import your plugin modules before using them:</p> <pre><code># Import plugins\nimport myapp.plugins.kafka  # Registers KafkaSource\nimport myapp.plugins.csv    # Registers CsvSink\n\n# Now you can use them\nsource = KafkaSource()\nsink = CsvSink()\n</code></pre>"},{"location":"user-guide/stream-processing/","title":"Stream Processing","text":""},{"location":"user-guide/stream-processing/#basic-streaming","title":"Basic Streaming","text":"<pre><code>from rivusio import StreamPipe, StreamConfig\nfrom typing import AsyncIterator, Dict\n\nclass DataStreamPipe(StreamPipe[Dict, Dict]):\n    async def process_stream(self, stream: AsyncIterator[Dict]) -&gt; AsyncIterator[Dict]:\n        async for item in stream:\n            if self.should_process(item):\n                yield await self.transform(item)\n\nasync def main():\n    stream_pipe = DataStreamPipe()\n    async for result in stream_pipe.process_stream(data_source()):\n        await store_result(result)\n</code></pre>"},{"location":"user-guide/stream-processing/#batch-streaming","title":"Batch Streaming","text":"<pre><code>class BatchStreamPipe(StreamPipe[Dict, List[Dict]]):\n    def __init__(self, batch_size: int):\n        self.batch_size = batch_size\n        self.current_batch = []\n\n    async def process_stream(self, stream: AsyncIterator[Dict]) -&gt; AsyncIterator[List[Dict]]:\n        async for item in stream:\n            self.current_batch.append(item)\n            if len(self.current_batch) &gt;= self.batch_size:\n                yield self.current_batch\n                self.current_batch = []\n</code></pre>"},{"location":"user-guide/streams/","title":"Stream Processing","text":""},{"location":"user-guide/streams/#overview","title":"Overview","text":"<p>Rivusio provides native support for stream processing, allowing you to build efficient data streaming pipelines.</p>"},{"location":"user-guide/streams/#basic-streaming","title":"Basic Streaming","text":"<p>Create a streaming pipeline:</p> <pre><code>from rivusio.streams import StreamPipe\nfrom typing import AsyncIterator, Dict\n\nclass MyStreamPipe(StreamPipe[Dict, Dict]):\n    async def process_stream(self, stream: AsyncIterator[Dict]) -&gt; AsyncIterator[Dict]:\n        async for item in stream:\n            yield await self.process_item(item)\n\n    async def process_item(self, item: Dict) -&gt; Dict:\n        return {\"processed\": item}\n\n# Use the stream pipe\npipe = MyStreamPipe()\nasync for result in pipe.stream(source_data):\n    print(result)\n</code></pre>"},{"location":"user-guide/streams/#windowing","title":"Windowing","text":"<p>Process data in windows:</p> <pre><code>from rivusio.streams import WindowedStreamPipe\nfrom datetime import timedelta\n\nclass WindowProcessor(WindowedStreamPipe[Dict, List]):\n    def __init__(self, window_size: timedelta):\n        super().__init__(window_size)\n\n    async def process_window(self, window: List[Dict]) -&gt; List:\n        return self.aggregate_window(window)\n</code></pre>"},{"location":"user-guide/streams/#batching","title":"Batching","text":"<p>Process data in batches:</p> <pre><code>from rivusio.streams import BatchStreamPipe\n\nclass BatchProcessor(BatchStreamPipe[Dict, List]):\n    def __init__(self, batch_size: int):\n        super().__init__(batch_size)\n\n    async def process_batch(self, batch: List[Dict]) -&gt; List:\n        return await self.process_items(batch)\n</code></pre>"},{"location":"user-guide/streams/#backpressure","title":"Backpressure","text":"<p>Handle backpressure:</p> <pre><code>from rivusio.streams import BackpressureConfig\n\nconfig = BackpressureConfig(\n    max_buffer_size=1000,\n    throttle_threshold=0.8,\n    min_processing_time=0.1\n)\n\npipe = StreamPipe(config=config)\n</code></pre>"},{"location":"user-guide/streams/#error-handling","title":"Error Handling","text":"<p>Handle stream errors:</p> <pre><code>from rivusio.streams import StreamError\n\ntry:\n    async for result in pipe.stream(data):\n        try:\n            process_result(result)\n        except StreamError as e:\n            handle_stream_error(e)\nexcept Exception as e:\n    handle_fatal_error(e)\n</code></pre>"},{"location":"user-guide/streams/#monitoring","title":"Monitoring","text":"<p>Monitor stream processing:</p> <pre><code>from rivusio.monitoring import PipelineMonitor\n\nmonitor = PipelineMonitor()\npipe.set_monitor(monitor)\n\n# Get stream metrics\nmetrics = monitor.get_metrics()\nprint(f\"Throughput: {metrics['throughput']}\")\nprint(f\"Processing time: {metrics['processing_time']}\")\n</code></pre>"},{"location":"user-guide/streams/#best-practices","title":"Best Practices","text":"<ol> <li>Use appropriate window/batch sizes</li> <li>Handle backpressure</li> <li>Implement proper error handling</li> <li>Monitor stream performance</li> <li>Clean up resources properly</li> <li>Test with realistic data volumes</li> </ol>"},{"location":"user-guide/streams/#async-streams","title":"Async Streams","text":"<pre><code>from rivusio import AsyncBasePipe, AsyncStream\nfrom typing import AsyncIterator, Dict, Optional\nimport asyncio\n\nclass AsyncNumberFilterPipe(AsyncBasePipe[Dict, Optional[Dict]]):\n    async def process(self, data: Dict) -&gt; Optional[Dict]:\n        filtered = {k: v for k, v in data.items() if isinstance(v, (int, float))}\n        return filtered if filtered else None\n\nasync def data_generator() -&gt; AsyncIterator[Dict]:\n    data = [\n        {\"a\": 1, \"b\": \"text\", \"c\": 2.5},\n        {\"x\": \"skip\", \"y\": \"ignore\"},\n        {\"d\": 10, \"e\": 20.0, \"f\": 30},\n    ]\n    for item in data:\n        yield item\n        await asyncio.sleep(0.1)  # Simulate async data source\n\nasync def main():\n    # Create stream and pipe\n    stream = AsyncStream(data_generator())\n    pipe = AsyncNumberFilterPipe()\n\n    # Process stream\n    async for result in stream.through(pipe):\n        if result:  # Handle None results from filter\n            print(f\"Processed: {result}\")\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n</code></pre>"},{"location":"user-guide/streams/#sync-streams","title":"Sync Streams","text":"<pre><code>from rivusio import SyncBasePipe, SyncStream\nfrom typing import Iterator, List\n\nclass BatchSumPipe(SyncBasePipe[List[float], float]):\n    def process(self, batch: List[float]) -&gt; float:\n        return sum(batch)\n\ndef number_generator() -&gt; Iterator[List[float]]:\n    batches = [\n        [1.0, 2.0, 3.0],\n        [4.0, 5.0, 6.0],\n        [7.0, 8.0, 9.0]\n    ]\n    for batch in batches:\n        yield batch\n\ndef main():\n    # Create stream and pipe\n    stream = SyncStream(number_generator())\n    pipe = BatchSumPipe()\n\n    # Process stream\n    for batch_sum in stream.through(pipe):\n        print(f\"Batch sum: {batch_sum}\")\n\nif __name__ == \"__main__\":\n    main()\n</code></pre>"},{"location":"user-guide/streams/#stream-with-error-handling","title":"Stream with Error Handling","text":"<pre><code>from rivusio import AsyncBasePipe, AsyncStream, PipeError\nfrom typing import AsyncIterator, Dict\nimport asyncio\n\nclass SafeDivisionPipe(AsyncBasePipe[Dict, Dict]):\n    async def process(self, data: Dict) -&gt; Dict:\n        try:\n            return {k: 100 / v for k, v in data.items()}\n        except ZeroDivisionError as e:\n            raise PipeError(f\"Division by zero: {str(e)}\")\n\n    async def handle_error(self, error: Exception, data: Dict) -&gt; Dict:\n        print(f\"Error processing {data}: {error}\")\n        return {\"error\": str(error)}\n\nasync def data_with_errors() -&gt; AsyncIterator[Dict]:\n    data = [\n        {\"a\": 2, \"b\": 4},\n        {\"a\": 0, \"b\": 1},  # Will cause error\n        {\"a\": 5, \"b\": 10}\n    ]\n    for item in data:\n        yield item\n        await asyncio.sleep(0.1)\n\nasync def main():\n    stream = AsyncStream(data_with_errors())\n    pipe = SafeDivisionPipe()\n\n    async for result in stream.through(pipe):\n        print(f\"Result: {result}\")\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n</code></pre>"},{"location":"user-guide/troubleshooting/","title":"Troubleshooting Guide","text":"<p>This guide helps you diagnose and resolve common issues you might encounter while using Rivusio.</p>"},{"location":"user-guide/troubleshooting/#common-issues","title":"Common Issues","text":""},{"location":"user-guide/troubleshooting/#1-pipeline-performance-issues","title":"1. Pipeline Performance Issues","text":""},{"location":"user-guide/troubleshooting/#symptoms","title":"Symptoms","text":"<ul> <li>Slow processing speed</li> <li>High memory usage</li> <li>Increasing latency</li> </ul>"},{"location":"user-guide/troubleshooting/#solutions","title":"Solutions","text":"<ol> <li> <p>Check Batch Size <pre><code># Optimize batch size for your use case\nconfig = StreamConfig(batch_size=1000)  # Adjust based on your data\nstream = AsyncStream(source, config=config)\n</code></pre></p> </li> <li> <p>Monitor Memory Usage <pre><code>from rivusio.monitoring import PipelineMonitor\n\nmonitor = PipelineMonitor()\npipeline.set_monitor(monitor)\n</code></pre></p> </li> <li> <p>Use Windowing for Large Datasets <pre><code>from datetime import timedelta\nconfig = StreamConfig(window_size=timedelta(seconds=30))\n</code></pre></p> </li> </ol>"},{"location":"user-guide/troubleshooting/#2-memory-leaks","title":"2. Memory Leaks","text":""},{"location":"user-guide/troubleshooting/#symptoms_1","title":"Symptoms","text":"<ul> <li>Increasing memory usage over time</li> <li>Out of memory errors</li> </ul>"},{"location":"user-guide/troubleshooting/#solutions_1","title":"Solutions","text":"<ol> <li> <p>Proper Resource Cleanup <pre><code>async with pipeline:\n    await pipeline.process(data)\n</code></pre></p> </li> <li> <p>Manual Cleanup <pre><code>try:\n    await pipeline.process(data)\nfinally:\n    await pipeline.cleanup()\n</code></pre></p> </li> </ol>"},{"location":"user-guide/troubleshooting/#3-data-loss-or-corruption","title":"3. Data Loss or Corruption","text":""},{"location":"user-guide/troubleshooting/#symptoms_2","title":"Symptoms","text":"<ul> <li>Missing data in output</li> <li>Unexpected data transformations</li> </ul>"},{"location":"user-guide/troubleshooting/#solutions_2","title":"Solutions","text":"<ol> <li> <p>Enable Detailed Logging <pre><code>import logging\nlogging.basicConfig(level=logging.DEBUG)\n</code></pre></p> </li> <li> <p>Add Validation Pipes <pre><code>class ValidationPipe(AsyncBasePipe[Data, Data]):\n    async def process(self, data: Data) -&gt; Data:\n        # Add your validation logic\n        assert data.required_field is not None\n        return data\n</code></pre></p> </li> </ol>"},{"location":"user-guide/troubleshooting/#4-pipeline-configuration-issues","title":"4. Pipeline Configuration Issues","text":""},{"location":"user-guide/troubleshooting/#symptoms_3","title":"Symptoms","text":"<ul> <li>Pipeline fails to start</li> <li>Configuration not applied</li> </ul>"},{"location":"user-guide/troubleshooting/#solutions_3","title":"Solutions","text":"<ol> <li> <p>Verify Configuration <pre><code>from pydantic import ValidationError\n\ntry:\n    config = PipeConfig(param1=\"value1\")\nexcept ValidationError as e:\n    print(f\"Invalid configuration: {e}\")\n</code></pre></p> </li> <li> <p>Check Plugin Registration <pre><code>from rivusio.plugins import PluginRegistry\n\nregistry = PluginRegistry()\nprint(registry.list_plugins())  # Verify your plugins are registered\n</code></pre></p> </li> </ol>"},{"location":"user-guide/troubleshooting/#debugging-techniques","title":"Debugging Techniques","text":""},{"location":"user-guide/troubleshooting/#1-enable-debug-logging","title":"1. Enable Debug Logging","text":"<pre><code>import logging\nlogging.basicConfig(\n    level=logging.DEBUG,\n    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'\n)\n</code></pre>"},{"location":"user-guide/troubleshooting/#2-use-pipeline-monitoring","title":"2. Use Pipeline Monitoring","text":"<pre><code>from rivusio.monitoring import PipelineMonitor\n\nmonitor = PipelineMonitor()\npipeline.set_monitor(monitor)\n\n# Check metrics after processing\nprint(monitor.get_metrics())\n</code></pre>"},{"location":"user-guide/troubleshooting/#3-add-debug-pipes","title":"3. Add Debug Pipes","text":"<pre><code>class DebugPipe(AsyncBasePipe[Any, Any]):\n    async def process(self, data: Any) -&gt; Any:\n        print(f\"Debug: Processing data: {data}\")\n        return data\n\npipeline.add_pipe(DebugPipe())\n</code></pre>"},{"location":"user-guide/troubleshooting/#performance-optimization","title":"Performance Optimization","text":""},{"location":"user-guide/troubleshooting/#1-batch-size-tuning","title":"1. Batch Size Tuning","text":"<ul> <li>Start with small batches (100-1000)</li> <li>Gradually increase while monitoring performance</li> <li>Monitor memory usage</li> </ul>"},{"location":"user-guide/troubleshooting/#2-window-size-optimization","title":"2. Window Size Optimization","text":"<ul> <li>Consider data arrival rate</li> <li>Balance between latency and throughput</li> <li>Monitor processing time</li> </ul>"},{"location":"user-guide/troubleshooting/#3-resource-management","title":"3. Resource Management","text":"<ul> <li>Use async context managers</li> <li>Implement proper cleanup</li> <li>Monitor system resources</li> </ul>"},{"location":"user-guide/troubleshooting/#getting-help","title":"Getting Help","text":"<p>If you're still experiencing issues:</p> <ol> <li>Check the GitHub Issues</li> <li>Review the API Documentation</li> <li>Join our Community Discussion</li> </ol>"},{"location":"user-guide/troubleshooting/#contributing-bug-reports","title":"Contributing Bug Reports","text":"<p>When reporting bugs:</p> <ol> <li>Provide minimal reproducible example</li> <li>Include full error traceback</li> <li>Share system information:</li> <li>Python version</li> <li>Rivusio version</li> <li>OS details</li> <li>Describe expected vs actual behavior</li> </ol>"}]}